"""This module handles all aspects of the world model, including state representation, environment dynamics, and prediction."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/04a_optimizers.utils.ipynb.

# %% auto 0
__all__ = ['get_cls', 'init_opt', 'EarlyStopper', 'init_opt_dis']

# %% ../../nbs/04a_optimizers.utils.ipynb 3
from fastcore import *
from fastcore.utils import *

# %% ../../nbs/04a_optimizers.utils.ipynb 4
from functools import partial
from torch.optim import Optimizer

# %% ../../nbs/04a_optimizers.utils.ipynb 5
import importlib
def get_cls(module_name, class_name):
    module = importlib.import_module(module_name)
    return getattr(module, class_name)

# %% ../../nbs/04a_optimizers.utils.ipynb 6
def init_opt(cfg, model):
    optimizer_cls = get_cls("torch.optim", cfg.optimizer.name)
    optimizer = optimizer_cls(model.parameters(), lr=cfg.optimizer.lr)
    return optimizer

# %% ../../nbs/04a_optimizers.utils.ipynb 7
# Source - https://stackoverflow.com/a
# Posted by isle_of_gods, modified by community. See post 'Timeline' for change history
# Retrieved 2025-11-15, License - CC BY-SA 4.0

class EarlyStopper:
    def __init__(self, patience=1, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.min_validation_loss = float('inf')

    def early_stop(self, validation_loss):
        if validation_loss < self.min_validation_loss:
            self.min_validation_loss = validation_loss
            self.counter = 0
        elif validation_loss > (self.min_validation_loss + self.min_delta):
            self.counter += 1
            if self.counter >= self.patience:
                return True
        return False


# %% ../../nbs/04a_optimizers.utils.ipynb 9
# from mawm.optimizers.schedulers import WSDSchedule, CosineWDSchedule
import torch
def init_opt_dis(
    cfg,
    jepa,
    msg_encoder,
    msg_pred,
    obs_pred,
    betas=(0.9, 0.999),
    eps=1e-8,
):
    all_params = (
        list(jepa.parameters()) + 
        list(msg_encoder.parameters()) + 
        list(msg_pred.parameters()) +
        list(obs_pred.parameters())
    )

    optimizer = torch.optim.AdamW(all_params, betas=betas, eps=eps)
    # scheduler = WSDSchedule(
    #     optimizer,
    #     warmup_steps=int(warmup * iterations_per_epoch),
    #     anneal_steps=int(anneal * iterations_per_epoch),
    #     start_lr=start_lr,
    #     ref_lr=ref_lr,
    #     final_lr=final_lr,
    #     T_max=int(num_epochs * iterations_per_epoch),
    # )
    # wd_scheduler = CosineWDSchedule(
    #     optimizer,
    #     ref_wd=wd,
    #     final_wd=final_wd,
    #     T_max=int(num_epochs * iterations_per_epoch),
    # )
    # scaler = torch.cuda.amp.GradScaler() if mixed_precision else None
    return optimizer#, scaler, scheduler, wd_scheduler

