"""This module handles all aspects of the world model, including state representation, environment dynamics, and prediction."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/04a_optimizers.utils.ipynb.

# %% auto 0
__all__ = ['get_cls', 'get_opt', 'EarlyStopper', 'init_opt']

# %% ../../nbs/04a_optimizers.utils.ipynb 3
from fastcore import *
from fastcore.utils import *

# %% ../../nbs/04a_optimizers.utils.ipynb 4
from functools import partial
from torch.optim import Optimizer

# %% ../../nbs/04a_optimizers.utils.ipynb 5
import importlib
def get_cls(module_name, class_name):
    module = importlib.import_module(module_name)
    return getattr(module, class_name)

# %% ../../nbs/04a_optimizers.utils.ipynb 6
def get_opt(cfg, model):
    optimizer_cls = get_cls("torch.optim", cfg.optimizer.name)
    optimizer = optimizer_cls(model.parameters(), lr=cfg.optimizer.lr)
    return optimizer

# %% ../../nbs/04a_optimizers.utils.ipynb 7
# Source - https://stackoverflow.com/a
# Posted by isle_of_gods, modified by community. See post 'Timeline' for change history
# Retrieved 2025-11-15, License - CC BY-SA 4.0

class EarlyStopper:
    def __init__(self, patience=1, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.min_validation_loss = float('inf')

    def early_stop(self, validation_loss):
        if validation_loss < self.min_validation_loss:
            self.min_validation_loss = validation_loss
            self.counter = 0
        elif validation_loss > (self.min_validation_loss + self.min_delta):
            self.counter += 1
            if self.counter >= self.patience:
                return True
        return False


# %% ../../nbs/04a_optimizers.utils.ipynb 8
import torch
def init_opt(
    cfg,
    models,
):
    # all_params = []
    # for k in models.keys():
    #     for model in models[k].values():
    #         all_params += list(model.parameters())
    # optimizer = torch.optim.AdamW(all_params, lr= cfg.optimizer.lr, betas=betas, eps=eps)

    base_lr = cfg.optimizer.lr
    jepa_params = list(models['rec']['jepa'].parameters())
    encoder_params = list(models["send"]["obs_enc"].parameters()) + list(models['send']['msg_enc'].parameters())
    comm_params = list(models['send']['comm_module'].parameters())
    proj_params = list(models['send']['proj'].parameters())
    
    param_groups = [
        {'params': jepa_params, 'lr': 0.5 * base_lr, 'name': 'jepa'},
        {'params': encoder_params, 'lr': base_lr, 'name': 'encoders'},
        {'params': comm_params, 'lr': base_lr * 2.0, 'name': 'comm_module'},
        {'params': proj_params, 'lr': base_lr * 1.5, 'name': 'proj'}
    ]
    
    optimizer = torch.optim.AdamW(param_groups, weight_decay=1e-4)
    

    return optimizer

