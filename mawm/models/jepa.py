"""Contains an encoder and a predictor."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/02f_models.jepa.ipynb.

# %% auto 0
__all__ = ['get_model', 'JEPA']

# %% ../../nbs/02f_models.jepa.ipynb 3
from fastcore import *
from fastcore.utils import *

# %% ../../nbs/02f_models.jepa.ipynb 4
from dataclasses import dataclass
from typing import Optional, NamedTuple

import torch

from functools import reduce
import operator



# %% ../../nbs/02f_models.jepa.ipynb 5
from ..core import get_cls
def get_model(model_name: str, type = "vision", params: dict = {}):
    model_cls = get_cls(f"mawm.models.{type}", f"{model_name}")
    model = model_cls(**params)
    return model

# %% ../../nbs/02f_models.jepa.ipynb 6
import torch
import torch.nn as nn
import torch.nn.functional as F
from ..core import get_cls

class JEPA(nn.Module):
    def __init__(
        self,
        cfg,
        input_dim = (42, 42, 3),
        repr_dim = None,
        action_dim = 5,
    ):
        super().__init__()

        self.cfg = cfg
        self.input_dim = input_dim
        self.repr_dim = repr_dim
        self.action_dim = action_dim
        self.backbone, self.dynamics = self.get_models()

    def get_models(self):
        model_cls = get_cls("mawm.models.vision", f"{self.cfg.backbone.arch}")
        backbone = model_cls(config=self.cfg.backbone, input_dim=self.input_dim)
        repr_dim = backbone.repr_dim

        model_cls = get_cls("mawm.models.dynamics", f"{self.cfg.predictor.arch}")
        dynamics = model_cls(config=self.cfg.predictor, repr_dim=repr_dim, action_dim=self.action_dim)
        return backbone, dynamics
        

# %% ../../nbs/02f_models.jepa.ipynb 10
@patch
def forward(
        self: JEPA,
        x: torch.Tensor, # [B, T, C, H, W]
        pos: torch.Tensor = None,  # [B, T, 2]
        repr_input: bool = False,
        actions: torch.Tensor = None,# [B, T, 1]
        msgs: torch.Tensor = None,# [B, T, 32]
        T: int = None,
        goal: torch.Tensor = None
    ):

    z0 = self.backbone(x, position=pos) if not repr_input else x
    z0 = torch.einsum('b t c h w->t b c h w', z0)# [T, B, C, H, W]
    actions = torch.einsum('b t d->t b d', actions) # [T, B, D]
    msgs = torch.einsum('b t m->t b m', msgs) # [T, B, M]
    
    Z = self.dynamics.forward_multiple(z0, actions[:-1], msgs[:-1], T) # TODO: check if it should be msgs[1:]

    return z0, Z

# %% ../../nbs/02f_models.jepa.ipynb 21
@patch
def update_ema(self: JEPA):
    if self.config.momentum > 0:
        for param, ema_param in zip(
            self.backbone.parameters(), self.backbone_ema.parameters()
        ):
            ema_param.data.mul_(self.config.momentum).add_(
                param.data, alpha=1 - self.config.momentum
            )
