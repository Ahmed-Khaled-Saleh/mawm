"""CNN for image feature extraction."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/02b_models.comm.ipynb.

# %% auto 0
__all__ = ['MSGEnc', 'CommModule']

# %% ../../nbs/02b_models.comm.ipynb 3
import numpy as np
import torch
import torch.distributions as td
import torch.nn as nn
from fastcore.utils import *

from torch import nn
from torch.nn import functional as F

# %% ../../nbs/02b_models.comm.ipynb 5
import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange

class MSGEnc(nn.Module):
    def __init__(self, num_primitives=5, latent_dim=32):
        self.latent_dim = latent_dim
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(num_primitives, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 16, kernel_size=3, stride=2),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(16 * 3 * 3, 64),
            nn.ReLU(),
            nn.Linear(64, latent_dim)
        )
        
    def forward(self, x):
        if x.dim() == 4:
            x = x.unsqueeze(1)  # Add time dimension if missing
        B, T, C, H, W = x.shape
        x = rearrange(x, 'b t c h w -> (b t) c h w')
        x = self.net(x) # [B*T, latent_dim]
        x = rearrange(x, '(b t) d -> b t d', b= B)
        return x


# %% ../../nbs/02b_models.comm.ipynb 8
import torch
import torch.nn as nn
from einops import rearrange
class CommModule(nn.Module):
    def __init__(self, input_channel= 32):
        super().__init__()
        
        # input shape: (batch, input_channel, 15, 15)
        self.network = nn.Sequential(
            # First layer: Refine latent features without changing spatial size
            nn.Conv2d(input_channel, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            
            # Second layer: Downsample from 15x15 to 7x7
            # Formula: floor((15 + 2*0 - 3) / 2) + 1 = 7
            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=0),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            
            # Final layer: Map to the 5 element channels
            # We use a 1x1 convolution to reach the target channel count
            nn.Conv2d(64, 5, kernel_size=1)
        )

    def forward(self, x):
        if x.dim() == 5:
            # print("Reshaping input from 5D to 4D for processing.")
            # Reshape from (batch, time, channels, height, width) to (batch * time, channels, height, width)
            b, t, c, h, w = x.shape
            x = x.view(b * t, c, h, w)
        elif x.dim() == 4:
            # print("Input shape is already 4D, proceeding without reshaping.")
            b, c, h, w = x.shape
        # Output shape: (batch, 5, 7, 7)
        x =  self.network(x)
        x = rearrange(x, '(b t) c h w -> b t c h w', b= b, t= t) if 't' in locals() else x
        return x


