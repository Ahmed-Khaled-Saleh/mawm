"""CNN for image feature extraction."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/02a_models.vision.ipynb.

# %% auto 0
__all__ = ['ENCODER_LAYERS_CONFIG', 'PassThrough', 'GridMLP', 'MeNet6']

# %% ../../nbs/02a_models.vision.ipynb 3
import numpy as np
import torch
import torch.distributions as td
import torch.nn as nn
from fastcore.utils import *

from torch import nn
from torch.nn import functional as F

# %% ../../nbs/02a_models.vision.ipynb 4
ENCODER_LAYERS_CONFIG = {
    # L1
    "a": [(2, 32, 5, 1, 0), (32, 32, 4, 2, 0), (32, 32, 3, 1, 0), (32, 16, 1, 1, 0)],
    "b": [(2, 16, 5, 1, 0), (16, 32, 4, 2, 0), (32, 32, 3, 1, 0), (32, 16, 1, 1, 0)],
    "c": [(2, 16, 5, 1, 0), (16, 16, 4, 2, 0), (16, 16, 3, 1, 0)],
    "f": [(2, 16, 5, 1, 0), (16, 16, 5, 2, 0), (16, 16, 5, 1, 2)],
    "g": [(2, 32, 5, 1, 0), (32, 32, 5, 2, 0), (32, 32, 5, 1, 2), (32, 16, 1, 1, 0)],
    "h": [(2, 16, 5, 1, 0), (16, 16, 5, 2, 0), (16, 16, 3, 1, 0)],
    "i": [(2, 16, 5, 1, 0), (16, 16, 5, 2, 0), (16, 16, 3, 1, 1)],
    "i_fc": [
        (2, 16, 5, 1, 0),
        (16, 16, 5, 2, 0),
        (16, 16, 3, 1, 1),
        ("fc", 13456, 512),
    ],
    "i_b": [(6, 16, 5, 1, 0), (16, 16, 5, 2, 0), (16, 16, 3, 1, 0)],
    "d4rl_a": [
        (6, 16, 5, 1, 0),
        (16, 32, 5, 2, 0),
        (32, 32, 3, 1, 0),
        (32, 32, 3, 1, 1),
        (32, 16, 1, 1, 0),
    ],
    "d4rl_b": [
        (6, 16, 5, 1, 0),
        (16, 32, 5, 2, 0),
        (32, 32, 3, 1, 0),
        (32, 32, 3, 1, 1),
        (32, 32, 3, 1, 1),
        (32, 16, 1, 1, 0),
    ],
    "d4rl_c": [
        (6, 16, 5, 1, 0),
        (16, 32, 5, 2, 0),
        (32, 32, 3, 1, 0),
        (32, 32, 3, 1, 1),
        (32, 32, 3, 1, 1),
    ],
    "j": [(2, 32, 5, 1, 0), (32, 32, 5, 2, 0), (32, 32, 3, 1, 1), (32, 16, 1, 1, 0)],
    "k": [(2, 16, 5, 1, 0), (16, 32, 5, 2, 0), (32, 32, 3, 1, 1), (32, 16, 1, 1, 0)],
    # L2
    "d": [(16, 16, 3, 1, 0), (16, 16, 3, 1, 0)],
    "e": [
        ("pad", (0, 1, 0, 1)),
        (16, 16, 3, 1, 0),
        ("avg_pool", 2, 2, 0),
        (16, 16, 3, 1, 0),
    ],
    "l2a": [(16, 16, 5, 1, 2), (16, 16, 5, 2, 2), (16, 16, 3, 1, 1)],  # (8, 16, 15, 15)
    "l2b": [(16, 16, 3, 1, 1), (16, 16, 3, 2, 1), (16, 16, 3, 1, 1)],  # (8, 16, 15, 15)
    "l2c": [(16, 32, 5, 1, 2), (32, 32, 5, 2, 2), (32, 32, 3, 1, 1)],  # (8, 32, 15, 15)
    "l2d": [(16, 32, 3, 1, 1), (32, 32, 3, 2, 1), (32, 32, 3, 1, 1)],  # (8, 32, 15, 15)
    "l2e": [(16, 16, 3, 2, 1), (16, 16, 3, 1, 1)],
}

# %% ../../nbs/02a_models.vision.ipynb 7
class PassThrough(nn.Module):
    def forward(self, x):
        return x


# %% ../../nbs/02a_models.vision.ipynb 17
import torch
import torch.nn as nn
from .utils import Expander2D, build_conv

class GridMLP(nn.Module):
    def __init__(self, grid_width= 15, grid_height= 15, embed_dim= 8, w= 15, h= 15):
        super().__init__()
        
        self.x_embeddings = nn.Embedding(grid_width, embed_dim)
        self.y_embeddings = nn.Embedding(grid_height, embed_dim)
        
        self.mlp = nn.Sequential(
            # nn.Linear(embed_dim * 2, 16),
            # nn.ReLU(),
            # nn.Linear(32, 32),
            # nn.ReLU(),
            # nn.Linear(32, 16),
            Expander2D(w, h)
        )
        
    def forward(self, x):
        """
        Input Shape:  [T, B, C, H, W]
        Output Shape: [T, B, C`, H`, W`]
        """

        if x.dim() == 3:
            x_coords, y_coords = x[:,:, 0], x[:,:, 1]
        else:
            x_coords, y_coords = x[:, 0], x[:, 1]
            
        x_vec = self.x_embeddings(x_coords)
        y_vec = self.y_embeddings(y_coords)

        combined = torch.cat([x_vec, y_vec], dim=-1)
        return self.mlp(combined)



# %% ../../nbs/02a_models.vision.ipynb 19
import torch
import torch.nn as nn
from .utils import Expander2D, build_conv

class MeNet6(nn.Module):
    def __init__(
        self,
        config,
        input_dim: int,
    ):
        super().__init__()

        self.config = config
        self.input_dim = input_dim
        subclass = config.backbone_subclass
        layers_config = ENCODER_LAYERS_CONFIG[subclass]

        if "l2" in subclass:
            # add prenormalization and relu layers?
            pre_conv = nn.Sequential(nn.GroupNorm(4, layers_config[0][0]), nn.ReLU())
        else:
            pre_conv = nn.Identity()
        conv_layers = build_conv(layers_config, (input_dim[0],))

        self.layers = nn.Sequential(pre_conv, conv_layers)

        if config.position_dim:
            # infer output dim of encoder
            sample_input = torch.randn(input_dim).unsqueeze(0) # [1, C, H, W]
            sample_output = self.layers(sample_input)
            encoder_output_dim = tuple(sample_output.shape[1:])

            if (
                self.config.position_encoder_arch
                and self.config.position_encoder_arch != "id"
            ):
                self.position_encoder = GridMLP(grid_width= 15, grid_height= 15, embed_dim= 8,
                                               w=encoder_output_dim[-2], h=encoder_output_dim[-1])
            else:
                self.position_encoder = Expander2D(
                    w=encoder_output_dim[-2], h=encoder_output_dim[-1]
                )
        
    @property
    def repr_dim(self):
        with torch.no_grad():
            sample_inp = torch.randn(self.input_dim).unsqueeze(0)  # [1, 1, C, H, W]
            sample_out = self.layers(sample_inp).unsqueeze(0)
            encoder_output_dim = tuple(sample_out.shape[1:])

            if self.config.position_dim:
                sample_pos = torch.randint(0, 15, (1, self.config.position_dim)).unsqueeze(0)
                sample_out_pos = self.position_encoder(sample_pos)
                pos_enc_output_dim = tuple(sample_out_pos.shape[1:])
                t = ([ax1 + ax2 for ax1, ax2 in zip(encoder_output_dim[1:2], pos_enc_output_dim[1:2])][0],
                     *encoder_output_dim[2:])
                return t
                
            return encoder_output_dim
    
    def forward(self, x, position=None):
        """
        input:
            x: [T, BS, *] or [BS, T, *]
        output:
            x: [T, BS, *] or [T, BS, *]
        """
        time= False
        if x.dim() == 2 or x.dim() == 4:
            encoded_obs = self.layers(x) # # [BS, C, H, W]
        else:
            time= True
            obs = x.flatten(0, 1) # [T*BS, C, H, W]
            encoded_obs = self.layers(obs) # [T*BS, 16, 10, 10]

        if position is not None:
            if position.dim() == 3:
                position = position.flatten(0, 1)  # [T*BS, pos_dim]
            encoded_pos = self.position_encoder(position)  # [T*BS, pos_dim, H, W]
            z = torch.cat([encoded_obs, encoded_pos], dim=1)
        else:
            encoded_pos = None
            z = encoded_obs
            
        if time:
            new_shape = x.shape[:2] + z.shape[1:]
            z = z.reshape(new_shape)
            return z
        return z
    
