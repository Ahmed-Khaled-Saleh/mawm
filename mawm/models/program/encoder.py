"""Get program rcontinuous representation."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../nbs/02f_models.program.encoder.ipynb.

# %% auto 0
__all__ = ['ProgramEncoder', 'ProgramPredictor']

# %% ../../../nbs/02f_models.program.encoder.ipynb 3
from fastcore import *
from fastcore.utils import *

# %% ../../../nbs/02f_models.program.encoder.ipynb 5
import torch
import torch.nn as nn
import torch.nn.functional as F
from ..base.dense import DenseModel
from .embedder import ProgramEmbedder
from ...core import PRIMITIVE_TEMPLATES

class ProgramEncoder(nn.Module):
    def __init__(
        self,
        num_primitives,
        param_cardinalities,   # list: for each slot, how many discrete values possible
        max_params_per_primitive,
        seq_len=4,
        d_name=32,
        d_param=32,
        output_dim=256,
        model_info={ 'layers': 3,'node_size': 128,'activation': nn.ReLU,'dist': None}
    ):
        super().__init__()
        
        self.program_embedder = ProgramEmbedder(
            num_primitives= num_primitives,
            param_cardinalities= param_cardinalities,
            max_params_per_primitive= max_params_per_primitive,
            d_name= d_name,
            d_param= d_param,
        )

        self.fuse = nn.Linear(seq_len * (d_name + d_param + d_param), model_info['node_size'])
        self.program_mlp = DenseModel(output_shape= (output_dim,), input_size=model_info['node_size'], info= model_info)

    def forward(self, primitive_ids, param_ids):
        """
        primitive_ids: LongTensor of shape [B, L]
        param_ids: LongTensor of shape [B, L, max_params] with -1 for missing parameters
        """
        combined_B_L_D = self.program_embedder(primitive_ids, param_ids) # shape: [B, L, D]
        B, L, D = combined_B_L_D.shape
        combined_B_LD = combined_B_L_D.view(B, L * D)  # Flatten to [B, D]
        combined_B_LD = self.fuse(combined_B_LD)  # shape: [B, node_size]
        primitive_vec = self.program_mlp(combined_B_LD)# shape: [B, output_dim]
        
        return primitive_vec

# %% ../../../nbs/02f_models.program.encoder.ipynb 11
import torch
import torch.nn as nn
import torch.nn.functional as F
from ..base.dense import DenseModel
from .embedder import ProgramEmbedder
from ...core import PRIMITIVE_TEMPLATES

class ProgramPredictor(nn.Module):
    def __init__(
        self,
        output_dim=32,
        model_info={ 'layers': 3,'node_size': 256,'activation': nn.ReLU,'dist': None}
    ):
        super().__init__()
        
        self.predictor = DenseModel(output_shape= (output_dim,), input_size=model_info['node_size'], info= model_info)

    def forward(self, x):
        """
        primitive_ids: LongTensor of shape [B, L]
        param_ids: LongTensor of shape [B, L, max_params] with -1 for missing parameters
        """
        z_hat = self.predictor(x)# shape: [B, output_dim]
        
        return z_hat
