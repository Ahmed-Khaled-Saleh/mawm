"""Embedding works by representing programs as fixed-size tensors of dimension dim, such that each program element is one-third of the values of this tensor."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../nbs/02kk_models.program.embedder.ipynb.

# %% auto 0
__all__ = ['MAX_PARAMS', 'get_indices', 'batchify_programs', 'ProgramEmbedder']

# %% ../../../nbs/02kk_models.program.embedder.ipynb 3
from fastcore import *
from fastcore.utils import *

# %% ../../../nbs/02kk_models.program.embedder.ipynb 7
from ...core import Program, PRIMITIVE_TEMPLATES
import torch
import torch.nn as nn
import torch.nn.functional as F

MAX_PARAMS = 2

def get_indices(program, max_params=MAX_PARAMS, device="cpu", padding_vals=[-1, -1]):
    
    L = len(program.tokens)
    if L == 0:
        # Handle empty program as requested
        prim_ids = torch.zeros((1, 0), dtype=torch.long, device=device)
        param_tensor = torch.zeros((1, 0, max_params), dtype=torch.long, device=device)
        prim_ids.add_(padding_vals[0])
        param_tensor.add_(padding_vals[1])
        return prim_ids, param_tensor

    prim_ids_list = []
    params_list = []
    for (prim_idx, params) in program.tokens:
        prim_ids_list.append(int(prim_idx))
        p = list(params)[:max_params]
        if len(p) < max_params:
            # Padding parameters with -1
            p = p + [padding_vals[1]] * (max_params - len(p))
        params_list.append(p)
        
    prim_ids = torch.tensor([prim_ids_list], dtype=torch.long, device=device)          # (1, L)
    param_tensor = torch.tensor([params_list], dtype=torch.long, device=device)   # (1, L, max_params)
    return prim_ids, param_tensor



# final_batch_prim_ids shape: (2, 3)
# final_batch_param_tensor shape: (2, 3, 2)

# %% ../../../nbs/02kk_models.program.embedder.ipynb 8
def batchify_programs(batch_programs, padding_vals=[-1, -1]):
    all_prim_tensors = []
    all_param_tensors = []
    max_len = 0

    for program in batch_programs:
        prim_ids, param_tensor = get_indices(program, padding_vals=padding_vals)
        all_prim_tensors.append(prim_ids.squeeze(0))     
        all_param_tensors.append(param_tensor.squeeze(0))
        max_len = max(max_len, prim_ids.size(1))

    # USE EOS AS PAD
    PAD_PRIM = padding_vals[0]   # <---- IMPORTANT CHANGE
    PAD_PARAM = padding_vals[1]       # parameters can remain -1

    padded_prim_ids = []
    padded_param_tensors = []

    for prim_t, param_t in zip(all_prim_tensors, all_param_tensors):
        L = prim_t.size(0)
        pad_len = max_len - L
        
        padded_prim_ids.append(F.pad(prim_t, (0, pad_len), value=PAD_PRIM))   # <-- EOS PAD

        padded_param_tensors.append(F.pad(param_t, (0, 0, 0, pad_len), value=PAD_PARAM))

    batch_prim_ids = torch.stack(padded_prim_ids, dim=0)
    batch_param_tensor = torch.stack(padded_param_tensors, dim=0)
    return batch_prim_ids, batch_param_tensor


# %% ../../../nbs/02kk_models.program.embedder.ipynb 18
import torch
import torch.nn as nn

class ProgramEmbedder(nn.Module):
    "Embeds a program into a fixed-size vector."
    def __init__(
        self,
        num_primitives,
        param_cardinalities,   # list: for each slot, how many discrete values possible
        max_params_per_primitive,
        d_name=32,
        d_param=32,
    ):
        super().__init__()

        self.num_primitives = num_primitives
        self.max_params = max_params_per_primitive
        self.empty_name = nn.Parameter(torch.zeros(d_name))
        self.empty_param = nn.Parameter(torch.zeros(d_param))
        
        self.name_embed = nn.Embedding(num_primitives + 1, d_name)
        self.param_embeds = nn.ModuleList([
            nn.Embedding(card, d_param) for card in param_cardinalities
        ])

    def forward(self, prim_ids, params_ids):
        """
        prim_ids: (B, L) LongTensor of primitive IDs
        params_ids: (B, L, max_params_per_primitive) LongTensor of parameter IDs
        returns: (B, L, D) Tensor of embedded programs
        """
        B, L = prim_ids.shape
        d_name = self.name_embed.embedding_dim
        d_param = self.param_embeds[0].embedding_dim
        D = d_name + self.max_params * d_param

        mask = (prim_ids == -1)
        name_embeds_B_L_D = self.name_embed(torch.clamp(prim_ids, min=0))  # (B, L, d_name)
        name_embeds_B_L_D[mask] = self.empty_name
        
        param_vecs = []
        for slot, embed in enumerate(self.param_embeds):
            slot_vals = params_ids[:, :, slot]  # [N]
            mask = (slot_vals == -1)

            slot_embed = embed(torch.clamp(slot_vals, min=0))
            slot_embed[mask] = self.empty_param

            param_vecs.append(slot_embed)

        params_emb_stacked = torch.stack(param_vecs, dim=2) # (B, L, num_params, d)
        params_vect_embeds = params_emb_stacked.view(B, L, -1) # (B, L, num_params * d)

        program_embed_B_L_D = torch.cat([name_embeds_B_L_D, params_vect_embeds], dim=-1) # (B, L, D)

        return program_embed_B_L_D
    
