"""Definitions of various data sets used in learning with offline rollouts."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/01c_data.loaders.ipynb.

# %% auto 0
__all__ = ['MarlGridDataset', 'RolloutObservationDataset', 'RolloutSequenceDataset', 'LejepaVisionDataset', 'D4RL']

# %% ../../nbs/01c_data.loaders.ipynb 3
from fastcore import *
from fastcore.utils import *

# %% ../../nbs/01c_data.loaders.ipynb 5
import torch
import os, sys
import numpy as np
from torchvision import datasets, transforms

class MarlGridDataset(torch.utils.data.Dataset):
    def __init__(self, data_path, num_agents= 2, train= True, seq_len= 64, msg_tf= None, transform=None):
        self.data = data_path
        self.rollouts = [os.path.join(self.data, f) 
                         for f in os.listdir(data_path) 
                         if os.path.isfile(os.path.join(data_path, f))]
        
        self.train_rollouts = self.rollouts[:int(0.8*len(self.rollouts))]
        self.test_rollouts = self.rollouts[int(0.8*len(self.rollouts)):]
        self.train = train
        if train:
            self.rollouts = self.train_rollouts
        else:
            self.rollouts = self.test_rollouts
            
        void_tf = transforms.Compose([
            transforms.ToTensor(),
        ])
        self.transform = transform if transform else void_tf
        self.msg_tf = msg_tf if msg_tf else lambda x: x
        self.seq_len = seq_len
        self.agents = [f"agent_{i}" for i in range(num_agents)]

    def __len__(self):
        if self.train:
            return len(self.train_rollouts)
        else:
            return len(self.test_rollouts)
    
    def sample_idxs(self, episode_len):
        idxs = np.arange(episode_len)
        start = np.random.randint(0, len(idxs) - self.seq_len)
        seq = idxs[start:start + self.seq_len]
        return seq
    
    def __getitem__(self, idx):
        path = self.rollouts[idx]
        data = np.load(path, allow_pickle=True)
        episode_len = data['episode_len'].item()
        loaded_rollout = False
        try:
            seq = self.sample_idxs(episode_len)
            loaded_rollout = True

        except Exception as e:
            loaded_rollout = False
            idx = np.random.randint(0, len(self))
            return self.__getitem__(idx)
        
        return {
            f"{ag}": 
                    {
                    "obs": torch.stack([self.transform(frame) for frame in data[f"{ag}_obs"][seq]]),
                    "pos": torch.from_numpy(data[f"{ag}_selfpos"][seq]),
                    "msg": torch.stack([self.msg_tf(frame) for frame in data[f"{ag}_obs"][seq]]),
                    "act": torch.from_numpy(data[f"{ag}_act"][seq]),
                    "next_obs": torch.stack([self.transform(frame) for frame in data[f"{ag}_next_obs"][seq]]),
                    "done": torch.from_numpy(data[f"{ag}_done"][seq]),
                    } 
            for ag in (self.agents)}
        
        
    

# %% ../../nbs/01c_data.loaders.ipynb 26
from bisect import bisect
from os import listdir
from os.path import join, isdir
from tqdm import tqdm
import torch
import torch.utils.data
import numpy as np


# %% ../../nbs/01c_data.loaders.ipynb 27
class _RolloutDataset(torch.utils.data.Dataset): 
    def __init__(self, agent, root, transform, buffer_size=200, seq_len= 50, train=True, obs_key = 'pov'): # pylint: disable=too-many-arguments
        
        self.agent = agent
        self._transform = transform
        self.obs_key = obs_key
        self.seq_len = seq_len
        self._files = [join(root, sd) for sd in listdir(root)]

        def train_test_split(files, train):
            if train:
                return files[:-600]
            else:
                return files[-600:]

        self._files = train_test_split(self._files, train)
        self._cum_size = None
        self._buffer = None
        self._buffer_fnames = None
        self._buffer_index = 0
        self._buffer_size = buffer_size

    def sample_sequence(self, rollout, agent, seq_len):

        episode_len = rollout["episode_len"].item()
        start = np.random.randint(0, episode_len)
        start = min(start, episode_len - seq_len)
        print(f"Sampling sequence for {agent} from index {start} to {start + seq_len}")

        end = start + seq_len

        obs = rollout[f"{agent}_obs"][start:end]
        rew = rollout[f"{agent}_rew"][start:end]
        act = rollout[f"{agent}_act"][start:end]
        info = rollout[f"{agent}_info"][start:end]

        data = {
            f"{agent}_obs": obs,
            f"{agent}_rew": rew,
            f"{agent}_act": act,
            f"{agent}_info": info
        }
        return data

    def load_next_buffer(self):
        """ Loads next buffer """
        self._buffer_fnames = self._files[self._buffer_index:self._buffer_index + self._buffer_size]
        self._buffer_index += self._buffer_size
        self._buffer_index = self._buffer_index % len(self._files)
        self._buffer = []
        self._cum_size = [0]

        # progress bar
        pbar = tqdm(total=len(self._buffer_fnames),
                    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} {postfix}')
        pbar.set_description("Loading file buffer ...")

        for f in self._buffer_fnames:
            with np.load(f, allow_pickle= True) as data:
                rollout = self.sample_sequence(data, self.agent, self.seq_len)
                self._buffer += [rollout] # list of dicts,each dict is an episode data
                self._cum_size += [self._cum_size[-1] + self.seq_len]
            pbar.update(1)
        pbar.close()

    def __len__(self):
        if not self._cum_size:
            self.load_next_buffer()
        return self._cum_size[-1]

    def __getitem__(self, i):
        # binary search through cum_size
        file_index = bisect(self._cum_size, i) - 1
        seq_index = i - self._cum_size[file_index]
        data = self._buffer[file_index]
        return self._get_data(data, seq_index)
    
    def reset_buffer(self):
        self._buffer = None
        self._cum_size = None
        self._buffer_fnames = None
        self._buffer_index = 0  

    def _get_data(self, data, seq_index):
        raise NotImplementedError

    def _data_per_sequence(self, data_length):
        raise NotImplementedError

# %% ../../nbs/01c_data.loaders.ipynb 29
class RolloutObservationDataset(_RolloutDataset): # pylint: disable=too-few-public-methods

    def _data_per_sequence(self, data_length):
        return data_length

    def _get_data(self, data, seq_index):
        done = data[f'{self.agent}_info'][seq_index]['done']
        obs = data[f'{self.agent}_obs'][seq_index][self.obs_key].astype(np.uint8)
        act = data[f'{self.agent}_act'][seq_index]
        return self._transform(obs), act, done, self.agent


# %% ../../nbs/01c_data.loaders.ipynb 54
class RolloutSequenceDataset(_RolloutDataset): # pylint: disable=too-few-public-methods
    """ Encapsulates rollouts.

    Rollouts should be stored in subdirs of the root directory, in the form of npz files,
    each containing a dictionary with the keys:
        - observations: (rollout_len, *obs_shape)
        - actions: (rovllout_len, action_size)
        - rewards: (rollout_len,)
        - terminals: (rollout_len,), boolean

     As the dataset is too big to be entirely stored in rams, only chunks of it
     are stored, consisting of a constant number of files (determined by the
     buffer_size parameter).  Once built, buffers must be loaded with the
     load_next_buffer method.

    Data are then provided in the form of tuples (obs, action, reward, terminal, next_obs):
    - obs: (seq_len, *obs_shape)
    - actions: (seq_len, action_size)
    - reward: (seq_len,)
    - terminal: (seq_len,) boolean
    - next_obs: (seq_len, *obs_shape)

    NOTE: seq_len < rollout_len in moste use cases

    :args root: root directory of data sequences
    :args seq_len: number of timesteps extracted from each rollout
    :args transform: transformation of the observations
    :args train: if True, train data, else test
    """
    def __init__(self, agent, root, seq_len, transform, buffer_size=200, train=True, obs_key='pov'): # pylint: disable=too-many-arguments
        super().__init__(agent, root, transform, buffer_size, train, obs_key)
        self._seq_len = seq_len
        self.agent = agent
         
    def __getitem__(self, i):
        file_index = bisect(self._cum_size, i) - 1
        seq_index = i - self._cum_size[file_index]
        data = self._buffer[file_index]
        
        # Ensure we don't go past the episode boundary
        episode_len = data['episode_len'].item()
        max_start = episode_len - self._seq_len - 1
        
        if seq_index > max_start:
            # This shouldn't happen if __len__ is correct
            # But as a safety, clamp it
            seq_index = max(0, max_start)
        
        return {**self._get_data(data, seq_index), 'index': i}
    
    def _get_agent_data(self, data, seq_index):
        data_dict = {}
        
        # Get obs[t] and obs[t+1] for t in [seq_index, seq_index+seq_len)
        obs_data = data[f'{self.agent}_obs'][seq_index:seq_index + self._seq_len + 1]
        
        # Transform observations
        obs_transformed = [self._transform(obs_data[i][self.obs_key].astype(np.uint8)) 
                        for i in range(len(obs_data))]
        
        # obs[t] and next_obs[t] = obs[t+1]
        obs = obs_transformed[:-1]        # length: seq_len
        next_obs = obs_transformed[1:]    # length: seq_len
        
        # Get actions, rewards, info that correspond to transitions
        # action[t] is the action taken at obs[t] leading to next_obs[t]
        action = data[f'{self.agent}_act'][seq_index:seq_index + self._seq_len]
        reward = data[f'{self.agent}_rew'][seq_index:seq_index + self._seq_len]
        info = data[f'{self.agent}_info'][seq_index:seq_index + self._seq_len]
        
        # Stack obs into tensors for easier collation
        data_dict["obs"] = torch.stack(obs)           # (seq_len, C, H, W)
        data_dict["next_obs"] = torch.stack(next_obs) # (seq_len, C, H, W)
        data_dict["act"] = torch.from_numpy(action.astype(np.float32))  # (seq_len, action_dim)
        data_dict["rew"] = torch.from_numpy(reward.astype(np.float32))  # (seq_len,)
        data_dict["info"] = info  # Keep as is or remove if not needed
        
        return data_dict

    def _get_data(self, data, seq_index):
        return self._get_agent_data(data, seq_index)

    def _data_per_sequence(self, data_length):
        return data_length - self._seq_len


# %% ../../nbs/01c_data.loaders.ipynb 64
import torch
class LejepaVisionDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, transform, V=1):
        self.V = V
        self.ds = dataset
        self.tf = transform
        self.load_next_buffer = dataset.load_next_buffer
        
    def __getitem__(self, i):
        img, done, agent = self.ds[i]
        return torch.stack([self.tf(img) for _ in range(self.V)]), done, agent

    def __len__(self):
        return len(self.ds)

# %% ../../nbs/01c_data.loaders.ipynb 68
from datasets import load_from_disk, concatenate_datasets
import torch
class D4RL(torch.utils.data.Dataset): 
    def __init__(self, data_path, transform, buffer_size=1, seq_len= 100, train=True, obs_key = 'pov'):
        
        self._transform = transform
        self.obs_key = obs_key
        self.seq_len = seq_len
        self._files = [join(data_path, sd) for sd in listdir(data_path)]

        def train_test_split(files, train):
            if train:
                return files[:-600]
            else:
                return files[-600:]

        self._files = train_test_split(self._files, train)
        self._cum_size = None
        self._buffer = None
        self._buffer_fnames = None
        self._buffer_index = 0
        self._buffer_size = buffer_size
        
    def __len__(self):
        if not self._cum_size:
            self.load_next_buffer()
        return len(self.ds)

    def load_next_buffer(self):
        """ Loads next buffer """
        self._buffer_fnames = self._files[self._buffer_index:self._buffer_index + self._buffer_size]
        self._buffer_index += self._buffer_size
        self._buffer_index = self._buffer_index % len(self._files)
        self._buffer = []
        self._cum_size = [0]

        for p in self._buffer_fnames:
            self._buffer.append(load_from_disk(p))
            self._cum_size += [self._cum_size[-1] + self.seq_len]
        self.ds = concatenate_datasets(self._buffer)


    def __getitem__(self, i):
        return self.ds[i]
    
    def reset_buffer(self):
        self._buffer = None
        self._cum_size = None
        self._buffer_fnames = None
        self._buffer_index = 0  

