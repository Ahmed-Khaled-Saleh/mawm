"""Planner using the Cross-Entropy Method (CEM) for optimization of discrete action sequences."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/10a_planners.cem_planner.ipynb.

# %% auto 0
__all__ = ['DiscreteCEMPlanner', 'CEMPlanner']

# %% ../../nbs/10a_planners.cem_planner.ipynb 3
from fastcore.utils import *
import pandas as pd
import wandb
import os
from ..data.utils import base_tf, msg_tf
import numpy as np

# %% ../../nbs/10a_planners.cem_planner.ipynb 4
import torch
import torch.nn.functional as F
import numpy as np
from einops import rearrange, repeat

class DiscreteCEMPlanner():
    def __init__(
        self,
        horizon,
        topk,
        num_samples,
        opt_steps,
        eval_every,
        wm,
        action_dim, # This is now the number of discrete actions
        objective_fn,
        preprocessor,
        evaluator,
        wandb_run,
        smoothing=0.01, # Laplace smoothing constant
        logging_prefix="plan_discrete",
        log_filename="logs.json",
        **kwargs,
    ):
        super().__init__(
            wm, action_dim, objective_fn, preprocessor, 
            evaluator, wandb_run, log_filename
        )
        self.horizon = horizon
        self.topk = topk
        self.num_samples = num_samples
        self.opt_steps = opt_steps
        self.eval_every = eval_every
        self.smoothing = smoothing
        self.logging_prefix = logging_prefix

    def init_probs(self, obs_0):
        """Initializes a uniform distribution over actions for each timestep."""
        n_evals = obs_0["visual"].shape[0]
        # (Batch, Horizon, Num_Actions) - start with uniform probabilities
        probs = torch.full((n_evals, self.horizon, self.action_dim), 1.0 / self.action_dim)
        return probs

    

# %% ../../nbs/10a_planners.cem_planner.ipynb 5
@patch
def plan(self: DiscreteCEMPlanner, obs_0, obs_g, actions=None):
    trans_obs_0 = self.preprocessor(obs_0).to(self.device)
    trans_obs_g = self.preprocessor(obs_g).to(self.device)
    
    z_obs_g = self.wm.backbone(trans_obs_g)
    z_obs_g = {f"goal_agent{i}": v for i, v in enumerate(z_obs_g)}
    
    trans_obs_0 = {f"agent{i}": v for i, v in enumerate(trans_obs_0)}

    msgs = {agent: msg_tf((obs_0[agent]['pov'], agent, False)) for agent in self.agents}

    # Initialize probabilities
    probs = self.init_probs(obs_0).to(self.device)
    n_evals = probs.shape[0]

    for i in range(self.opt_steps):
        losses = []
        for traj_idx in range(n_evals):
            # 1. Prepare observations for the current trajectory batch
            cur_trans_obs_0 = {
                key: repeat(arr[traj_idx].unsqueeze(0), "1 ... -> n ...", n=self.num_samples)
                for key, arr in trans_obs_0.items()
            }
            cur_z_obs_g = {
                key: repeat(arr[traj_idx].unsqueeze(0), "1 ... -> n ...", n=self.num_samples)
                for key, arr in z_obs_g.items()
            }

            # 2. Sample discrete actions from the current distribution
            # We flatten to sample, then reshape back
            p = probs[traj_idx] # (H, action_dim)
            # Sample (num_samples) for each step in the horizon
            sampled_indices = torch.multinomial(p, self.num_samples, replacement=True) 
            sampled_indices = sampled_indices.T # (num_samples, H)

            # 3. Rollout in World Model
            # Note: If your WM expects one-hot, convert sampled_indices here
            with torch.no_grad():
                # We pass sampled_indices (integers). Adjust if WM needs one-hot vectors.
                i_z_obses, i_zs = self.wm.forward_multiple(
                    obs_0=cur_trans_obs_0,
                    act=sampled_indices, 
                )

            # 4. Evaluate and find Elites
            loss = self.objective_fn(i_z_obses, cur_z_obs_g)
            topk_idx = torch.argsort(loss)[: self.topk]
            elite_actions = sampled_indices[topk_idx] # (topk, H)

            # 5. Update Distribution (Frequency Counting)
            # Convert elite actions to one-hot: (topk, H, action_dim)
            elite_one_hot = F.one_hot(elite_actions, num_classes=self.action_dim).float()
            
            # Average the one-hots to get new probabilities
            new_probs = elite_one_hot.mean(dim=0) # (H, action_dim)

            # 6. Apply Laplace Smoothing
            # Ensures we don't have 0% probability for any action
            new_probs = (new_probs + self.smoothing) / (1 + self.smoothing * self.action_dim)
            probs[traj_idx] = new_probs

            losses.append(loss[topk_idx[0]].item())

        # Logging logic (same as your original code)
        self.wandb_run.log({f"{self.logging_prefix}/loss": np.mean(losses), "step": i + 1})
        
        if self.evaluator is not None and i % self.eval_every == 0:
            # For evaluation, we pick the most likely action (argmax)
            best_actions = torch.argmax(probs, dim=-1)
            logs, successes, _, _ = self.evaluator.eval_actions(
                best_actions, filename=f"{self.logging_prefix}_output_{i+1}"
            )
            # ... (rest of logging)
            if np.all(successes): break

    # Return the most likely action sequence
    return torch.argmax(probs, dim=-1), np.full(n_evals, np.inf)

# %% ../../nbs/10a_planners.cem_planner.ipynb 25
class CEMPlanner:
    def __init__(self, model, msg_enc, msg_pred, obs_pred, 
                 action_dim= 5, horizon= 10, num_samples= 1000,
                 num_elites=100, opt_steps=100):
        
        self.probs = torch.full((horizon, action_dim), 1.0/action_dim) # Uniform initial distribution of shape [Horizon, Action_Dim]
        self.model = model
        self.msg_enc = msg_enc
        self.msg_pred = msg_pred
        self.obs_pred = obs_pred
        self.action_dim = action_dim
        self.horizon = horizon
        self.num_samples = num_samples
        self.num_elites = num_elites
        self.opt_steps = opt_steps
        self.device = 'cpu'
        

    def update_dist(self, probs, samples, costs, num_elites=50):

        values, elite_indices = torch.topk(-costs, num_elites)
        elites = samples[elite_indices] # [num_elites, Horizon]
        
        # Update probabilities for the next round
        new_probs = torch.full_like(probs, 0.01 / probs.shape[-1])
        for t in range(probs.shape[0]):
            new_probs[t].scatter_add_(0, elites[:, t], torch.ones(num_elites) * (0.99 / num_elites))
        
        best_plan = elites[0]
        
        return new_probs, best_plan


    @torch.no_grad()
    def plan(self, o_t, pos_t, o_g, m_other, other_actions):
        z_t  = self.model.backbone(o_t.unsqueeze(0)) # [B=1, C, H, W] => [1, 18, 15, 15] #, position= pos_t.unsqueeze(0)
        z_g = self.model.backbone(o_g.unsqueeze(0)) # [B=1, C, H, W] => [1, 16, 15, 15]

        a_self = torch.multinomial(self.probs, self.num_samples, replacement=True).T # [num_samples, horizon] 
        
        h0_other = self.msg_enc(m_other.unsqueeze(0).unsqueeze(1)) # [5, 7, 7] => [1, 1, 32]
        a_other = other_actions.repeat(self.num_samples, 1) # [num_samples, horizon]

        total_costs = self.evolve(z_t, z_g, h0_other, a_self, a_other)
        self.probs, best_plan = self.update_dist(self.probs, a_self, total_costs, self.num_elites)
        return best_plan

    def evolve(self, z_t, z_goal, h0_other, a_self, a_other):
        S = self.num_samples

        curr_z_self = repeat(z_t, 'b c h w -> (b s) t c h w', b= 1, s=S, t= 1) # [s, t= 1, c=16, h=15, w=15]
        curr_h_other = repeat(h0_other, 'b t d -> s t (b d)', s=S) # [s, 1, 32]
        
        curr_z_other = self.obs_pred(curr_h_other)#[s, 1, 16, 15, 15]  
        curr_h_self = self.msg_pred(curr_z_self) #[s, 1, 32]

        curr_z_self = rearrange(curr_z_self, "s t c h w -> (t s) c h w", t= 1)
        curr_z_other = rearrange(curr_z_other, "s t c h w -> (t s) c h w", t= 1)

        total_cost = torch.zeros(S, device=self.device)

        for t in range(self.horizon):
            
            curr_h_other = rearrange(curr_h_other, "s t d -> (t s) d", t= 1)
            curr_h_self = rearrange(curr_h_self, "s t d -> (t s) d", t= 1)

            a_self_t = a_self[:, t].unsqueeze(1) # [1 500]
            a_other_t = a_other[:, t].unsqueeze(1) # [1, 500]

            next_z_self = self.model.dynamics.forward(current_state = curr_z_self, curr_action= a_self_t, curr_msg= curr_h_other)
            next_z_other = self.model.dynamics.forward(current_state = curr_z_other, curr_action= a_other_t, curr_msg= curr_h_self)

            next_h_self = self.msg_pred(rearrange(next_z_self, '(s t) c h w -> s t c h w ', t= 1))
            next_h_other = self.msg_pred(rearrange(next_z_other, '(s t) c h w -> s t c h w ', t= 1))

            total_cost += (next_z_self[:, :-2] - z_goal.unsqueeze(1)).pow(2).mean(dim=(2, 3, 4)).squeeze()

            curr_z_self, curr_z_other = next_z_self, next_z_other
            curr_h_self, curr_h_other = next_h_self, next_h_other

        return total_cost
