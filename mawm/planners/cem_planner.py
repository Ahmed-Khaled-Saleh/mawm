"""Planner using the Cross-Entropy Method (CEM) for optimization of discrete action sequences."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/06_planners.cem_planner.ipynb.

# %% auto 0
__all__ = ['DiscreteCEMPlanner']

# %% ../../nbs/06_planners.cem_planner.ipynb 3
from fastcore.utils import *
import pandas as pd
import wandb
import os
from ..data.utils import base_tf, msg_tf
import numpy as np

# %% ../../nbs/06_planners.cem_planner.ipynb 4
import torch
import torch.nn.functional as F
import numpy as np
from einops import rearrange, repeat

class DiscreteCEMPlanner():
    def __init__(
        self,
        horizon,
        topk,
        num_samples,
        opt_steps,
        eval_every,
        wm,
        action_dim, # This is now the number of discrete actions
        objective_fn,
        preprocessor,
        evaluator,
        wandb_run,
        smoothing=0.01, # Laplace smoothing constant
        logging_prefix="plan_discrete",
        log_filename="logs.json",
        **kwargs,
    ):
        super().__init__(
            wm, action_dim, objective_fn, preprocessor, 
            evaluator, wandb_run, log_filename
        )
        self.horizon = horizon
        self.topk = topk
        self.num_samples = num_samples
        self.opt_steps = opt_steps
        self.eval_every = eval_every
        self.smoothing = smoothing
        self.logging_prefix = logging_prefix

    def init_probs(self, obs_0):
        """Initializes a uniform distribution over actions for each timestep."""
        n_evals = obs_0["visual"].shape[0]
        # (Batch, Horizon, Num_Actions) - start with uniform probabilities
        probs = torch.full((n_evals, self.horizon, self.action_dim), 1.0 / self.action_dim)
        return probs

    

# %% ../../nbs/06_planners.cem_planner.ipynb 5
@patch
def plan(self: DiscreteCEMPlanner, obs_0, obs_g, actions=None):
    trans_obs_0 = self.preprocessor(obs_0).to(self.device)
    trans_obs_g = self.preprocessor(obs_g).to(self.device)
    
    z_obs_g = self.wm.backbone(trans_obs_g)
    z_obs_g = {f"goal_agent{i}": v for i, v in enumerate(z_obs_g)}
    
    trans_obs_0 = {f"agent{i}": v for i, v in enumerate(trans_obs_0)}

    msgs = {agent: msg_tf((obs_0[agent]['pov'], agent, False)) for agent in self.agents}

    # Initialize probabilities
    probs = self.init_probs(obs_0).to(self.device)
    n_evals = probs.shape[0]

    for i in range(self.opt_steps):
        losses = []
        for traj_idx in range(n_evals):
            # 1. Prepare observations for the current trajectory batch
            cur_trans_obs_0 = {
                key: repeat(arr[traj_idx].unsqueeze(0), "1 ... -> n ...", n=self.num_samples)
                for key, arr in trans_obs_0.items()
            }
            cur_z_obs_g = {
                key: repeat(arr[traj_idx].unsqueeze(0), "1 ... -> n ...", n=self.num_samples)
                for key, arr in z_obs_g.items()
            }

            # 2. Sample discrete actions from the current distribution
            # We flatten to sample, then reshape back
            p = probs[traj_idx] # (H, action_dim)
            # Sample (num_samples) for each step in the horizon
            sampled_indices = torch.multinomial(p, self.num_samples, replacement=True) 
            sampled_indices = sampled_indices.T # (num_samples, H)

            # 3. Rollout in World Model
            # Note: If your WM expects one-hot, convert sampled_indices here
            with torch.no_grad():
                # We pass sampled_indices (integers). Adjust if WM needs one-hot vectors.
                i_z_obses, i_zs = self.wm.forward_multiple(
                    obs_0=cur_trans_obs_0,
                    act=sampled_indices, 
                )

            # 4. Evaluate and find Elites
            loss = self.objective_fn(i_z_obses, cur_z_obs_g)
            topk_idx = torch.argsort(loss)[: self.topk]
            elite_actions = sampled_indices[topk_idx] # (topk, H)

            # 5. Update Distribution (Frequency Counting)
            # Convert elite actions to one-hot: (topk, H, action_dim)
            elite_one_hot = F.one_hot(elite_actions, num_classes=self.action_dim).float()
            
            # Average the one-hots to get new probabilities
            new_probs = elite_one_hot.mean(dim=0) # (H, action_dim)

            # 6. Apply Laplace Smoothing
            # Ensures we don't have 0% probability for any action
            new_probs = (new_probs + self.smoothing) / (1 + self.smoothing * self.action_dim)
            probs[traj_idx] = new_probs

            losses.append(loss[topk_idx[0]].item())

        # Logging logic (same as your original code)
        self.wandb_run.log({f"{self.logging_prefix}/loss": np.mean(losses), "step": i + 1})
        
        if self.evaluator is not None and i % self.eval_every == 0:
            # For evaluation, we pick the most likely action (argmax)
            best_actions = torch.argmax(probs, dim=-1)
            logs, successes, _, _ = self.evaluator.eval_actions(
                best_actions, filename=f"{self.logging_prefix}_output_{i+1}"
            )
            # ... (rest of logging)
            if np.all(successes): break

    # Return the most likely action sequence
    return torch.argmax(probs, dim=-1), np.full(n_evals, np.inf)
