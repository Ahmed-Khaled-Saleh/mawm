"""Planner using the Cross-Entropy Method (CEM) for optimization of discrete action sequences."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/10a_planners.cem_planner.ipynb.

# %% auto 0
__all__ = ['DiscreteCEMPlanner', 'CEMPlanner']

# %% ../../nbs/10a_planners.cem_planner.ipynb 3
from fastcore.utils import *
import pandas as pd
import wandb
import os
from ..data.utils import base_tf, msg_tf
import numpy as np

# %% ../../nbs/10a_planners.cem_planner.ipynb 4
import torch
import torch.nn.functional as F
import numpy as np
from einops import rearrange, repeat

class DiscreteCEMPlanner():
    def __init__(
        self,
        horizon,
        topk,
        num_samples,
        opt_steps,
        eval_every,
        wm,
        action_dim, # This is now the number of discrete actions
        objective_fn,
        preprocessor,
        evaluator,
        wandb_run,
        smoothing=0.01, # Laplace smoothing constant
        logging_prefix="plan_discrete",
        log_filename="logs.json",
        **kwargs,
    ):
        super().__init__(
            wm, action_dim, objective_fn, preprocessor, 
            evaluator, wandb_run, log_filename
        )
        self.horizon = horizon
        self.topk = topk
        self.num_samples = num_samples
        self.opt_steps = opt_steps
        self.eval_every = eval_every
        self.smoothing = smoothing
        self.logging_prefix = logging_prefix

    def init_probs(self, obs_0):
        """Initializes a uniform distribution over actions for each timestep."""
        n_evals = obs_0["visual"].shape[0]
        # (Batch, Horizon, Num_Actions) - start with uniform probabilities
        probs = torch.full((n_evals, self.horizon, self.action_dim), 1.0 / self.action_dim)
        return probs

    

# %% ../../nbs/10a_planners.cem_planner.ipynb 5
@patch
def plan(self: DiscreteCEMPlanner, obs_0, obs_g, actions=None):
    self.probs = torch.full((self.horizon, self.action_dim), 1.0/self.action_dim).to(self.device)
    trans_obs_0 = self.preprocessor(obs_0).to(self.device)
    trans_obs_g = self.preprocessor(obs_g).to(self.device)
    
    z_obs_g = self.wm.backbone(trans_obs_g)
    z_obs_g = {f"goal_agent{i}": v for i, v in enumerate(z_obs_g)}
    
    trans_obs_0 = {f"agent{i}": v for i, v in enumerate(trans_obs_0)}

    msgs = {agent: msg_tf((obs_0[agent]['pov'], agent, False)) for agent in self.agents}

    # Initialize probabilities
    probs = self.init_probs(obs_0).to(self.device)
    n_evals = probs.shape[0]

    for i in range(self.opt_steps):
        losses = []
        for traj_idx in range(n_evals):
            # 1. Prepare observations for the current trajectory batch
            cur_trans_obs_0 = {
                key: repeat(arr[traj_idx].unsqueeze(0), "1 ... -> n ...", n=self.num_samples)
                for key, arr in trans_obs_0.items()
            }
            cur_z_obs_g = {
                key: repeat(arr[traj_idx].unsqueeze(0), "1 ... -> n ...", n=self.num_samples)
                for key, arr in z_obs_g.items()
            }

            # 2. Sample discrete actions from the current distribution
            # We flatten to sample, then reshape back
            p = probs[traj_idx] # (H, action_dim)
            # Sample (num_samples) for each step in the horizon
            sampled_indices = torch.multinomial(p, self.num_samples, replacement=True) 
            sampled_indices = sampled_indices.T # (num_samples, H)

            # 3. Rollout in World Model
            # Note: If your WM expects one-hot, convert sampled_indices here
            with torch.no_grad():
                # We pass sampled_indices (integers). Adjust if WM needs one-hot vectors.
                i_z_obses, i_zs = self.wm.forward_multiple(
                    obs_0=cur_trans_obs_0,
                    act=sampled_indices, 
                )

            # 4. Evaluate and find Elites
            loss = self.objective_fn(i_z_obses, cur_z_obs_g)
            topk_idx = torch.argsort(loss)[: self.topk]
            elite_actions = sampled_indices[topk_idx] # (topk, H)

            # 5. Update Distribution (Frequency Counting)
            # Convert elite actions to one-hot: (topk, H, action_dim)
            elite_one_hot = F.one_hot(elite_actions, num_classes=self.action_dim).float()
            
            # Average the one-hots to get new probabilities
            new_probs = elite_one_hot.mean(dim=0) # (H, action_dim)

            # 6. Apply Laplace Smoothing
            # Ensures we don't have 0% probability for any action
            new_probs = (new_probs + self.smoothing) / (1 + self.smoothing * self.action_dim)
            probs[traj_idx] = new_probs

            losses.append(loss[topk_idx[0]].item())

        # Logging logic (same as your original code)
        self.wandb_run.log({f"{self.logging_prefix}/loss": np.mean(losses), "step": i + 1})
        
        if self.evaluator is not None and i % self.eval_every == 0:
            # For evaluation, we pick the most likely action (argmax)
            best_actions = torch.argmax(probs, dim=-1)
            logs, successes, _, _ = self.evaluator.eval_actions(
                best_actions, filename=f"{self.logging_prefix}_output_{i+1}"
            )
            # ... (rest of logging)
            if np.all(successes): break

    # Return the most likely action sequence
    return torch.argmax(probs, dim=-1), np.full(n_evals, np.inf)

# %% ../../nbs/10a_planners.cem_planner.ipynb 28
import torch
from einops import repeat, rearrange
from ..models.utils import Expander2D

class CEMPlanner:
    def __init__(self, model, msg_enc, msg_pred, obs_pred, 
                 action_dim=5, horizon=10, num_samples=1000,
                 num_elites=100, opt_steps=10, device='cpu'):
        
        self.model = model
        self.msg_enc = msg_enc
        self.msg_pred = msg_pred
        self.obs_pred = obs_pred
        self.action_dim = action_dim
        self.horizon = horizon
        self.num_samples = num_samples
        self.num_elites = num_elites
        self.opt_steps = opt_steps
        self.device = device
        
        # Hyperparameters for improved stability
        self.alpha = 0.3  # Momentum for distribution update
        self.epsilon = 0.01  # Laplace smoothing
        self.temp_start = 1.0  # Initial temperature
        self.temp_end = 0.5  # Final temperature
        
    def update_dist(self, probs, samples, costs, num_elites, iteration):
        """Update probability distribution using elite samples with momentum."""
        # Select elites (minimize cost)
        _, elite_indices = torch.topk(-costs, num_elites)
        elites = samples[elite_indices]
        
        # Compute empirical distribution from elites
        empirical_probs = torch.zeros_like(probs)
        for t in range(self.horizon):
            counts = torch.bincount(elites[:, t], minlength=self.action_dim).float()
            empirical_probs[t] = counts / num_elites
        
        # Momentum update for stability
        new_probs = self.alpha * empirical_probs + (1 - self.alpha) * probs
        
        # Laplace smoothing
        new_probs = (1 - self.epsilon) * new_probs + (self.epsilon / self.action_dim)
        
        # Temperature annealing for sharpening
        temp = self.temp_start + (self.temp_end - self.temp_start) * (iteration / self.opt_steps)
        new_probs = new_probs ** (1 / temp)
        new_probs = new_probs / new_probs.sum(dim=1, keepdim=True)
        
        return new_probs, elites[0]

    @torch.no_grad()
    def plan(self, o_t, o_g, pos_t, m_other, other_actions):
        """Plan action sequence using CEM."""
        # Move inputs to device
        o_t = o_t.to(self.device)
        o_g = o_g.to(self.device)
        pos_t = pos_t.to(self.device)
        m_other = m_other.to(self.device)
        other_actions = other_actions.to(self.device)
        
        # Initialize uniform distribution
        current_probs = torch.full(
            (self.horizon, self.action_dim), 
            1.0 / self.action_dim, 
            device=self.device
        )

        # Encode current state and goal
        z_t = self.model.backbone(o_t.unsqueeze(0))
        pos_t_expanded = Expander2D(z_t.shape[-1], z_t.shape[-2])(pos_t.unsqueeze(0))
        z_t = torch.cat([z_t, pos_t_expanded], dim=1)
        
        z_g = self.model.backbone(o_g.unsqueeze(0))
        # Match dimensions with cost calculation (exclude position channels)
        z_goal_target = z_g#[:, :-2]
        
        # Encode other agent's message
        h0_other = self.msg_enc(m_other.unsqueeze(0).unsqueeze(1))
        a_other = other_actions.repeat(self.num_samples, 1)

        best_plan = None
        best_cost = float('inf')
        
        for i in range(self.opt_steps):
            # Sample action sequences
            a_self = torch.multinomial(
                current_probs, 
                self.num_samples, 
                replacement=True
            ).T.to(self.device)
            
            # Evaluate sampled sequences
            total_costs = self.evolve(z_t, z_goal_target, h0_other, a_self, a_other)
            
            # Track best solution
            min_cost = total_costs.min()
            if min_cost < best_cost:
                best_cost = min_cost
                best_idx = total_costs.argmin()
                best_plan = a_self[best_idx]
            
            # Debug info
            if i == 0 or i == self.opt_steps - 1:
                print(f"Iter {i}: Cost Î¼={total_costs.mean():.4f}, "
                      f"Std={total_costs.std():.4f}, min={min_cost:.4f}")

            # Update distribution
            current_probs, _ = self.update_dist(
                current_probs, a_self, total_costs, self.num_elites, i
            )

        return best_plan

    def evolve(self, z_t, z_goal, h0_other, a_self, a_other):
        """Roll out trajectories and compute costs."""
        S = self.num_samples
        
        # Initialize states
        curr_z_self = repeat(z_t, 'b c h w -> (b s) c h w', s=S)
        curr_h_other = repeat(h0_other, 'b t d -> (s b t) d', s=S)
        
        curr_z_other = rearrange(
            self.obs_pred(curr_h_other.unsqueeze(1)), 
            "s t c h w -> (s t) c h w"
        )
        curr_h_self = self.msg_pred(curr_z_self[:, :-2].unsqueeze(1)).squeeze(1)

        total_cost = torch.zeros(S, device=self.device)
        
        # Roll out trajectories
        for t in range(self.horizon):
            a_self_t = a_self[:, t].unsqueeze(1)
            a_other_t = a_other[:, t].unsqueeze(1)
            
            # Forward dynamics
            next_z_self = self.model.dynamics.forward(
                current_state=curr_z_self, 
                curr_action=a_self_t, 
                curr_msg=curr_h_other
            )
            next_z_other = self.model.dynamics.forward(
                current_state=curr_z_other, 
                curr_action=a_other_t, 
                curr_msg=curr_h_self
            )

            # Update messages
            curr_h_self = self.msg_pred(next_z_self[:, :-2].unsqueeze(1)).squeeze(1)
            curr_h_other = self.msg_pred(next_z_other[:, :-2].unsqueeze(1)).squeeze(1)

            # Compute step cost (MSE to goal)
            diff = next_z_self[:, :-2] - z_goal
            step_cost = diff.pow(2).mean(dim=(1, 2, 3))
            
            # Weight final timestep more heavily
            weight = 2.0 if t == self.horizon - 1 else 1.0
            total_cost += weight * step_cost

            curr_z_self, curr_z_other = next_z_self, next_z_other

        return total_cost
