"""Planner using the Cross-Entropy Method (CEM) for optimization of discrete action sequences."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/10a_planners.cem_planner.ipynb.

# %% auto 0
__all__ = ['DiscreteCEMPlanner', 'CEMPlanner']

# %% ../../nbs/10a_planners.cem_planner.ipynb 3
from fastcore.utils import *
import pandas as pd
import wandb
import os
from ..data.utils import base_tf, msg_tf
import numpy as np

# %% ../../nbs/10a_planners.cem_planner.ipynb 4
import torch
import torch.nn.functional as F
import numpy as np
from einops import rearrange, repeat

class DiscreteCEMPlanner():
    def __init__(
        self,
        horizon,
        topk,
        num_samples,
        opt_steps,
        eval_every,
        wm,
        action_dim, # This is now the number of discrete actions
        objective_fn,
        preprocessor,
        evaluator,
        wandb_run,
        smoothing=0.01, # Laplace smoothing constant
        logging_prefix="plan_discrete",
        log_filename="logs.json",
        **kwargs,
    ):
        super().__init__(
            wm, action_dim, objective_fn, preprocessor, 
            evaluator, wandb_run, log_filename
        )
        self.horizon = horizon
        self.topk = topk
        self.num_samples = num_samples
        self.opt_steps = opt_steps
        self.eval_every = eval_every
        self.smoothing = smoothing
        self.logging_prefix = logging_prefix

    def init_probs(self, obs_0):
        """Initializes a uniform distribution over actions for each timestep."""
        n_evals = obs_0["visual"].shape[0]
        # (Batch, Horizon, Num_Actions) - start with uniform probabilities
        probs = torch.full((n_evals, self.horizon, self.action_dim), 1.0 / self.action_dim)
        return probs

    

# %% ../../nbs/10a_planners.cem_planner.ipynb 5
@patch
def plan(self: DiscreteCEMPlanner, obs_0, obs_g, actions=None):
    self.probs = torch.full((self.horizon, self.action_dim), 1.0/self.action_dim).to(self.device)
    trans_obs_0 = self.preprocessor(obs_0).to(self.device)
    trans_obs_g = self.preprocessor(obs_g).to(self.device)
    
    z_obs_g = self.wm.backbone(trans_obs_g)
    z_obs_g = {f"goal_agent{i}": v for i, v in enumerate(z_obs_g)}
    
    trans_obs_0 = {f"agent{i}": v for i, v in enumerate(trans_obs_0)}

    msgs = {agent: msg_tf((obs_0[agent]['pov'], agent, False)) for agent in self.agents}

    # Initialize probabilities
    probs = self.init_probs(obs_0).to(self.device)
    n_evals = probs.shape[0]

    for i in range(self.opt_steps):
        losses = []
        for traj_idx in range(n_evals):
            # 1. Prepare observations for the current trajectory batch
            cur_trans_obs_0 = {
                key: repeat(arr[traj_idx].unsqueeze(0), "1 ... -> n ...", n=self.num_samples)
                for key, arr in trans_obs_0.items()
            }
            cur_z_obs_g = {
                key: repeat(arr[traj_idx].unsqueeze(0), "1 ... -> n ...", n=self.num_samples)
                for key, arr in z_obs_g.items()
            }

            # 2. Sample discrete actions from the current distribution
            # We flatten to sample, then reshape back
            p = probs[traj_idx] # (H, action_dim)
            # Sample (num_samples) for each step in the horizon
            sampled_indices = torch.multinomial(p, self.num_samples, replacement=True) 
            sampled_indices = sampled_indices.T # (num_samples, H)

            # 3. Rollout in World Model
            # Note: If your WM expects one-hot, convert sampled_indices here
            with torch.no_grad():
                # We pass sampled_indices (integers). Adjust if WM needs one-hot vectors.
                i_z_obses, i_zs = self.wm.forward_multiple(
                    obs_0=cur_trans_obs_0,
                    act=sampled_indices, 
                )

            # 4. Evaluate and find Elites
            loss = self.objective_fn(i_z_obses, cur_z_obs_g)
            topk_idx = torch.argsort(loss)[: self.topk]
            elite_actions = sampled_indices[topk_idx] # (topk, H)

            # 5. Update Distribution (Frequency Counting)
            # Convert elite actions to one-hot: (topk, H, action_dim)
            elite_one_hot = F.one_hot(elite_actions, num_classes=self.action_dim).float()
            
            # Average the one-hots to get new probabilities
            new_probs = elite_one_hot.mean(dim=0) # (H, action_dim)

            # 6. Apply Laplace Smoothing
            # Ensures we don't have 0% probability for any action
            new_probs = (new_probs + self.smoothing) / (1 + self.smoothing * self.action_dim)
            probs[traj_idx] = new_probs

            losses.append(loss[topk_idx[0]].item())

        # Logging logic (same as your original code)
        self.wandb_run.log({f"{self.logging_prefix}/loss": np.mean(losses), "step": i + 1})
        
        if self.evaluator is not None and i % self.eval_every == 0:
            # For evaluation, we pick the most likely action (argmax)
            best_actions = torch.argmax(probs, dim=-1)
            logs, successes, _, _ = self.evaluator.eval_actions(
                best_actions, filename=f"{self.logging_prefix}_output_{i+1}"
            )
            # ... (rest of logging)
            if np.all(successes): break

    # Return the most likely action sequence
    return torch.argmax(probs, dim=-1), np.full(n_evals, np.inf)

# %% ../../nbs/10a_planners.cem_planner.ipynb 27
from ..models.utils import Expander2D
import torch
from einops import repeat, rearrange

class CEMPlanner:
    def __init__(self, model, msg_enc, msg_pred, obs_pred, 
                 action_dim=5, horizon=10, num_samples=1000,
                 num_elites=100, opt_steps=10):
        
        self.model = model
        self.msg_enc = msg_enc
        self.msg_pred = msg_pred
        self.obs_pred = obs_pred
        self.action_dim = action_dim # Number of discrete choices (0-4)
        self.horizon = horizon
        self.num_samples = num_samples
        self.num_elites = num_elites
        self.opt_steps = opt_steps
        self.device = 'cpu'
        
    def update_dist(self, probs, samples, costs, num_elites):
        # We want to MINIMIZE cost, so we take the topk of negative costs
        _, elite_indices = torch.topk(-costs, num_elites)
        elites = samples[elite_indices] # [num_elites, horizon]
        
        new_probs = torch.zeros_like(probs)
        for t in range(self.horizon):
            counts = torch.bincount(elites[:, t], minlength=self.action_dim).float()
            new_probs[t] = counts / num_elites
        
        # Laplace smoothing to ensure we always have some exploration noise
        epsilon = 0.01
        new_probs = (1 - epsilon) * new_probs + (epsilon / self.action_dim)
        
        return new_probs, elites[0]

    @torch.no_grad()
    def plan(self, o_t, pos_t, o_g, m_other, other_actions):
        # RESET: Start fresh so we don't get stuck in a local minimum from the last step
        current_probs = torch.full((self.horizon, self.action_dim), 1.0/self.action_dim, device=self.device)

        z_t = self.model.backbone(o_t.unsqueeze(0)) 
        pos_t_expanded = Expander2D(z_t.shape[-1], z_t.shape[-2])(pos_t.unsqueeze(0))
        z_t = torch.cat([z_t, pos_t_expanded], dim=1) 
        z_g = self.model.backbone(o_g.unsqueeze(0)) 
        
        h0_other = self.msg_enc(m_other.unsqueeze(0).unsqueeze(1)) 
        a_other = other_actions.repeat(self.num_samples, 1) 

        best_plan = None
        for i in range(self.opt_steps):
            # Sample sequences: [num_samples, horizon]
            a_self = torch.multinomial(current_probs, self.num_samples, replacement=True).T 
            
            total_costs = self.evolve(z_t, z_g, h0_other, a_self, a_other)
            
            # DEBUG: Uncomment this to see if costs are actually changing
            if i == 0: print(f"Cost Std: {total_costs.std():.6f}, Min: {total_costs.min():.4f}")

            current_probs, best_plan = self.update_dist(current_probs, a_self, total_costs, self.num_elites)

        return best_plan

    def evolve(self, z_t, z_goal, h0_other, a_self, a_other):
        S = self.num_samples
        
        # Ensure z_goal is ready for broadcasting: [1, C, H, W]
        # We strip the last 2 channels if your goal doesn't include the pos_t expansion
        z_goal_target = z_goal 

        curr_z_self = repeat(z_t, 'b c h w -> (b s) c h w', s=S)
        curr_h_other = repeat(h0_other, 'b t d -> (s b t) d', s=S)
        
        # Initial imagined states for the other agent
        curr_z_other = rearrange(self.obs_pred(curr_h_other.unsqueeze(1)), "s t c h w -> (s t) c h w")
        curr_h_self = self.msg_pred(curr_z_self[:, :-2].unsqueeze(1)).squeeze(1)

        total_cost = torch.zeros(S, device=self.device)
        
        for t in range(self.horizon):
            a_self_t = a_self[:, t].unsqueeze(1) # [S, 1]
            a_other_t = a_other[:, t].unsqueeze(1) # [S, 1]
            
            # Forward dynamics
            next_z_self = self.model.dynamics.forward(current_state=curr_z_self, curr_action=a_self_t, curr_msg=curr_h_other)
            next_z_other = self.model.dynamics.forward(current_state=curr_z_other, curr_action=a_other_t, curr_msg=curr_h_self)

            # Predict next messages
            curr_h_self = self.msg_pred(next_z_self[:, :-2].unsqueeze(1)).squeeze(1)
            curr_h_other = self.msg_pred(next_z_other[:, :-2].unsqueeze(1)).squeeze(1)

            # --- CRITICAL: Cost calculation ---
            # Compare only the latent state (excluding position channels if necessary)
            # next_z_self[:, :-2] is [S, 16, 15, 15]
            # z_goal_target is [1, 16, 15, 15]
            diff = next_z_self[:, :-2] - z_goal_target
            
            # Step-wise cost (MSE)
            total_cost += diff.pow(2).mean(dim=(1, 2, 3))

            curr_z_self, curr_z_other = next_z_self, next_z_other

        return total_cost
