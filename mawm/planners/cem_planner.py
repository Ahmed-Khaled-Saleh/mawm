"""Planner using the Cross-Entropy Method (CEM) for optimization of discrete action sequences."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/06_planners.cem_planner.ipynb.

# %% auto 0
__all__ = ['DecentralizedActionExchangingCEM']

# %% ../../nbs/06_planners.cem_planner.ipynb 23
import torch
import torch.nn.functional as F
from einops import rearrange, repeat
class DecentralizedActionExchangingCEM:
    def __init__(self, model, msg_encoder, msg_predictor, obs_predictor, action_dim=5, horizon=12, num_samples=500):
        self.model = model
        self.msg_encoder = msg_encoder
        self.msg_predictor = msg_predictor
        self.obs_predictor = obs_predictor
        self.action_dim = action_dim
        self.horizon = horizon
        self.num_samples = num_samples
        self.device = next(model.parameters()).device

    @torch.no_grad()
    def plan_step(self, z0_self, m0_other, z_goal_self, other_planned_actions, self_probs):
        """
        z0_self: My current latent
        h0_other: The initial message I got from the other agent
        z_goal_self: My target latent
        other_planned_actions: The [Horizon] actions the other agent TOLD me it wants to take
        self_probs: My current CEM probability distribution [Horizon, Action_Dim]
        """
      
        h0_other = self.msg_encoder(m0_other.unsqueeze(0).unsqueeze(1)) # [1, 1, 32]
        
        a_samples_self = torch.multinomial(self_probs, self.num_samples, replacement=True).T # [num_samples=500, horizon=12]

        a_fixed_other = other_planned_actions.repeat(self.num_samples, 1) # [num_samples=500, horizon=12]

        costs = self._rollout(z0_self, h0_other, z_goal_self, a_samples_self, a_fixed_other)
        
        return a_samples_self, costs

    def _rollout(self, z0_self, h0_other, z_goal_self, a_self, a_other):
        S = self.num_samples

        curr_z_self = repeat(z0_self, 'c h w -> s t c h w', s=S, t= 1) # [s=500, t= 1, c=16, h=15, w=15]
        curr_h_other = repeat(h0_other, 'b t d -> s t (b d)', s=S) # [s=500, 1, 32]
        
        curr_z_other = self.obs_predictor(curr_h_other)#[500, 1, 16, 15, 15]  
        curr_h_self = self.msg_predictor(curr_z_self) #[500, 1, 32]

        curr_z_self = rearrange(curr_z_self, "s t c h w -> (t s) c h w", t= 1)
        curr_z_other = rearrange(curr_z_other, "s t c h w -> (t s) c h w", t= 1)

        total_cost = torch.zeros(S, device=self.device)

        for t in range(self.horizon):
            print("T = ", t)
            
            curr_h_other = rearrange(curr_h_other, "s t d -> (t s) d", t= 1)
            curr_h_self = rearrange(curr_h_self, "s t d -> (t s) d", t= 1)

            a_self_t = a_self[:, t].unsqueeze(1) # [1 500]
            a_other_t = a_other[:, t].unsqueeze(1) # [1, 500]

            next_z_self = self.model.dynamics.forward(current_state = curr_z_self, curr_action= a_self_t, curr_msg= curr_h_other)
            next_z_other = self.model.dynamics.forward(current_state = curr_z_other, curr_action= a_other_t, curr_msg= curr_h_self)

            next_h_self = self.msg_predictor(rearrange(next_z_self, '(s t) c h w -> s t c h w ', t= 1))
            next_h_other = self.msg_predictor(rearrange(next_z_other, '(s t) c h w -> s t c h w ', t= 1))

            total_cost += (next_z_self[:, :-2] - z_goal_self.unsqueeze(1)).pow(2).mean(dim=(2, 3, 4)).squeeze()

            curr_z_self, curr_z_other = next_z_self, next_z_other
            curr_h_self, curr_h_other = next_h_self, next_h_other

        return total_cost
