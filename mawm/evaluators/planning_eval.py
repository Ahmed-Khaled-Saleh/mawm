"""Evaluator for planning performance using the Cross-Entropy Method (CEM) for optimization of discrete action sequences."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/10b_evaluators.planning_eval.ipynb.

# %% auto 0
__all__ = ['preprocessor', 'FindGoalPlanner']

# %% ../../nbs/10b_evaluators.planning_eval.ipynb 3
from fastcore.utils import *
import pandas as pd
import wandb
import os
from ..data.utils import base_tf, msg_tf
import numpy as np
import torch
import torch.nn.functional as F
from einops import repeat
from ..planners.cem_planner import CEMPlanner

# %% ../../nbs/10b_evaluators.planning_eval.ipynb 4
def preprocessor(env, obs, pos=True, get_msg=True):
    obs_transformed = {}
    positions = {}
    goals = {}
    messages = {}
    goal_pos = obs["global"]["goal_pos"]

    agents = [f'agent_{i}' for i in range(env.num_agents)]
    for i, agent_id in enumerate(agents):
        obs_transformed[agent_id] = base_tf(obs[agent_id]['pov'].astype(np.uint8))  # Add batch dimension
        goal = env.get_goal(env.agents[i], goal_pos)[0]
        goals[agent_id] = base_tf(goal.astype(np.uint8))

        if pos:
            positions[agent_id] = torch.from_numpy(obs[agent_id]['selfpos'])
        if get_msg:
            m = msg_tf((obs[agent_id]['pov'], agent_id, False, True))
            messages[agent_id] = m

    
    if pos and get_msg:
        return obs_transformed, positions, goals, messages
    elif pos:
        return obs_transformed, positions, goals
    elif get_msg:
        return obs_transformed, messages
    else:
        return obs_transformed

# %% ../../nbs/10b_evaluators.planning_eval.ipynb 5
import torch
import torch.nn.functional as F
from einops import repeat
# class FindGoalPlanner:

#     def __init__(self, model, msg_enc, comm_module, action_dim = 5, horizon= 10, pop_size= 1000, topk= 100, opt_steps= 10, agents= ['agent_0', 'agent_1'], device='cpu'):
#         self.model = model
#         self.msg_enc = msg_enc
#         self.comm_module = comm_module
#         self.agents = agents
#         self.device = device
#         self.action_dim = action_dim
#         self.pop_size = pop_size
#         self.topk = topk
#         self.opt_steps = opt_steps
#         self.horizon = horizon
#         self.current_probs = {agent: torch.full((self.horizon, self.action_dim), 1.0/self.action_dim, device=self.device) \
#                               for agent in self.agents}
#         self.loss = torch.nn.MSELoss(reduction='none')


# %% ../../nbs/10b_evaluators.planning_eval.ipynb 31
class FindGoalPlanner:

    def __init__(self, model, msg_enc, comm_module, action_dim=5, horizon=10, 
                 pop_size=1000, topk=100, opt_steps=10, 
                 agents=['agent_0', 'agent_1'], device='cpu'):
        self.model = model
        self.msg_enc = msg_enc
        self.comm_module = comm_module
        self.agents = agents
        self.device = device    
        self.action_dim = action_dim
        self.pop_size = pop_size
        self.topk = topk
        self.opt_steps = opt_steps
        self.horizon = horizon
        self.current_probs = {agent: torch.full((self.horizon, self.action_dim), 
                                                 1.0/self.action_dim, device=self.device) 
                              for agent in self.agents}
        self.loss = torch.nn.MSELoss(reduction='none')


# %% ../../nbs/10b_evaluators.planning_eval.ipynb 32
@patch    
def update_dist(self: FindGoalPlanner, costs, samples):
    for agent in self.agents:
        _, elite_indices = torch.topk(-costs[agent], self.topk)
        elites = samples[agent][elite_indices]
        
        new_probs = torch.zeros_like(self.current_probs[agent])
        for t in range(self.horizon):
            counts = torch.bincount(elites[:, t].argmax(dim=1).int(), 
                                    minlength=self.action_dim).float()
            new_probs[t] = (counts + 1e-6) / (counts.sum() + 1e-6 * self.action_dim)

        self.current_probs[agent] = new_probs



# %% ../../nbs/10b_evaluators.planning_eval.ipynb 33
@patch
@torch.no_grad()
def Plan(self: FindGoalPlanner, env, preprocessor):
    obs = env.reset()
    step = 0
    plan = {agent: [] for agent in self.agents}
    
    _, _, goals, _ = preprocessor(env, obs, pos=True, get_msg=True)
    goal_pos = obs["global"]["goal_pos"]
    position = repeat(torch.from_numpy(goal_pos).unsqueeze(0), "b d -> g b d", b=1, g=2)
    z_goals = self.model.backbone(torch.stack([goals[agent] for agent in self.agents]).to(self.device),
                                    position=position.to(self.device))
    
    z_goals = repeat(z_goals, 'b c h w -> (b s) c h w', s=self.pop_size)
    z_goals = {agent: z_goals[z_goals.size(0) // 2 * i : z_goals.size(0) // 2 * (i+1)] 
                for i, agent in enumerate(self.agents)}
    
    while step < 100:
        prev_obs, prev_pos, _, msgs = preprocessor(env, obs, pos=True, get_msg=True)
        
        for agent in self.agents:
            self.current_probs[agent] = torch.full((self.horizon, self.action_dim), 
                                                    1.0/self.action_dim, device=self.device)

        for n in range(self.opt_steps):
            samples = {agent: torch.multinomial(self.current_probs[agent], 
                                                self.pop_size, replacement=True).T 
                        for agent in self.agents}
            
            samples = {agent: F.one_hot(samples[agent], num_classes=self.action_dim).float().to(self.device) 
                        for agent in self.agents}
            
            start_z = self.model.backbone(
                torch.stack([prev_obs[a] for a in self.agents]).to(self.device),
                position=torch.stack([prev_pos[agent] for agent in self.agents]).to(self.device)
            )
            
            states = {agent: repeat(start_z[i], 'c h w -> s c h w', s=self.pop_size) 
                        for i, agent in enumerate(self.agents)}
            
            total_costs = {agent: torch.zeros(self.pop_size, device=self.device) 
                            for agent in self.agents}

            for t in range(self.horizon):
                next_states = {}
                for rec in self.agents:
                    sender = [a for a in self.agents if a != rec][0]
                    
                    if t == 0:
                        m = msgs[sender].to(self.device).unsqueeze(0)
                        m = repeat(m, 'b c h w -> (s b) c h w', s=self.pop_size, b=1)
                    else:
                        m = self.comm_module(states[sender].to(self.device))
                    
                    h_rec = self.msg_enc(m.to(self.device))
                    z_next = self.model.dynamics(states[rec], samples[rec][:, t], h_rec.squeeze(1))
                    next_states[rec] = z_next
                    
                    step_loss = self.loss(z_next, z_goals[rec]).mean(dim=[1, 2, 3])
                    total_costs[rec] += step_loss
                
                states = next_states

            self.update_dist(total_costs, samples)
            # Inside opt_steps loop:
            if n == 0 or n == self.opt_steps - 1:
                for agent in self.agents:
                    print(f"Iter {n}, {agent}: cost range [{total_costs[agent].min():.4f}, {total_costs[agent].max():.4f}]")


        # After the opt_steps loop, before execution:
        for agent in self.agents:
            print(f"\n{agent} final distribution at t=0:")
            print(self.current_probs[agent][0])
            print(f"Best trajectory cost: {total_costs[agent].min().item():.4f}")
            print(f"Worst trajectory cost: {total_costs[agent].max().item():.4f}")
            print(f"Mean trajectory cost: {total_costs[agent].mean().item():.4f}")

        executed_actions = {}
        for agent in self.agents:
            act = torch.argmax(self.current_probs[agent][0]).item()
            executed_actions[agent] = act
            plan[agent].append(act)

        obs, rewards, done, infos = env.step(executed_actions)

        if done.get('__all__', False): break
        step += 1
    
    return plan

