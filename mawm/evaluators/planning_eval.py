"""Evaluator for planning performance using the Cross-Entropy Method (CEM) for optimization of discrete action sequences."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/09_evaluators.planning_eval.ipynb.

# %% auto 0
__all__ = ['preprocessor', 'PlanEvaluator']

# %% ../../nbs/09_evaluators.planning_eval.ipynb 3
from fastcore.utils import *
import pandas as pd
import wandb
import os
from ..data.utils import base_tf, msg_tf
import numpy as np
import torch
from ..planners.cem_planner import CEMPlanner

# %% ../../nbs/09_evaluators.planning_eval.ipynb 4
def preprocessor(env, obs, pos=True, get_msg=True):
    obs_transformed = {}
    positions = {}
    goals = {}
    messages = {}
    goal_pos = obs["global"]["goal_pos"]

    agents = [f'agent_{i}' for i in range(env.num_agents)]
    for i, agent_id in enumerate(agents):
        obs_transformed[agent_id] = base_tf(obs[agent_id]['pov'].astype(np.uint8))  # Add batch dimension
        goal = env.get_goal(env.agents[i], goal_pos)[0]
        goals[agent_id] = base_tf(goal.astype(np.uint8))

        if pos:
            positions[agent_id] = torch.from_numpy(obs[agent_id]['selfpos'])
        if get_msg:
            m = msg_tf((obs[agent_id]['pov'], agent_id, False))
            messages[agent_id] = m

    
    if pos and get_msg:
        return obs_transformed, positions, goals, messages
    elif pos:
        return obs_transformed, positions, goals
    elif get_msg:
        return obs_transformed, messages
    else:
        return obs_transformed

# %% ../../nbs/09_evaluators.planning_eval.ipynb 5
class PlanEvaluator:
    "Evaluator for planning performance using the Cross-Entropy Method (CEM) for optimization of discrete action sequences."
    def __init__(self, model, msg_enc, msg_pred, planner, agents= ['agent_0', 'agent_1'], device='cpu'):
        self.model = model
        self.msg_enc = msg_enc
        self.msg_pred = msg_pred
        self.agents = agents
        self.device = device
        self.planners = {agent: planner for agent in agents}

# %% ../../nbs/09_evaluators.planning_eval.ipynb 6
@patch
def eval_all_agents(self: PlanEvaluator, env, preprocessor=preprocessor, negotiation_rounds=3):
    obs = env.reset()
    step = 0
    agents = self.agents
    horizon = self.planners[agents[0]].horizon

    # 1. Initialize "Intents" (The Draft Plans)
    # At t=0, we start with zeros (staying still)
    intents = {agent: torch.zeros(horizon, dtype=torch.long) for agent in agents}

    while step < 100:

        obs_transformed, pos, goals, msgs = preprocessor(env, obs, pos=True, get_msg=True)

        for r in range(negotiation_rounds):
            new_intents = {}
            print("Negotiation Round:", r+1)
            for agent in self.agents:
                # Find the other agent
                other_agent = [a for a in agents if a != agent][0]
                
                # Plan based on the OTHER agent's intent from the previous negotiation round
                # This grounds the "imagination" in reality
                new_intents[agent] = self.planners[agent].plan(
                    o_t=obs_transformed[agent], 
                    pos_t=pos[agent], 
                    o_g=goals[agent], 
                    m_other=msgs[other_agent],
                    other_actions=intents[other_agent] # This is the "Anchor"
                )
            
            # Update intents for the next negotiation round
            intents = new_intents

        
        # After negotiation rounds, take the FIRST action of the final best plan
        actions = {agent: intents[agent][0] for agent in agents}
        actions = {agent: np.int64(actions[agent].item()) for agent in agents}
        obs, rewards, done, infos = env.step(actions)
        print(f"Step: {step}, Actions taken: {actions}, Rewards: {rewards}, Done: {done}")
        # Shift the remaining plan forward by 1 and pad with a 0 (Stay)
        for agent in agents:
            shifted_plan = torch.cat([intents[agent][1:], torch.zeros(1, dtype=torch.long)])
            intents[agent] = shifted_plan

        if done['__all__']:
            break

        step += 1
        
    env.close()
    return intents # Returning the final sequences
