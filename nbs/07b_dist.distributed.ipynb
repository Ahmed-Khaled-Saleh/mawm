{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training init\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp distributed.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "import subprocess\n",
    "\n",
    "import logging\n",
    "\n",
    "from mawm.logger.base import get_logger\n",
    "\n",
    "logger = get_logger()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def init_distributed(port=29500, rank_and_world_size=(None, None)):\n",
    "    # tmpdir trick for some clusters\n",
    "    if \"SLURM_JOB_ID\" in os.environ:\n",
    "        tmpdir = Path(f\"/scratch/slurm_tmpdir/{os.environ['SLURM_JOB_ID']}\")\n",
    "        if tmpdir.exists():\n",
    "            os.environ[\"TMPDIR\"] = str(tmpdir)\n",
    "\n",
    "    if dist.is_available() and dist.is_initialized():\n",
    "        return dist.get_world_size(), dist.get_rank(), int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "\n",
    "    rank, world_size = rank_and_world_size\n",
    "    master_addr = os.environ.get(\"MASTER_ADDR\", None)\n",
    "    master_port = os.environ.get(\"MASTER_PORT\", str(port))\n",
    "\n",
    "    # Prefer torchrun env vars if present\n",
    "    if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n",
    "        rank = int(os.environ[\"RANK\"])\n",
    "        world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "        local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "\n",
    "    # Else SLURM\n",
    "    elif \"SLURM_PROCID\" in os.environ:\n",
    "        rank = int(os.environ[\"SLURM_PROCID\"])\n",
    "        world_size = int(os.environ[\"SLURM_NTASKS\"])\n",
    "        local_rank = int(os.environ.get(\"SLURM_LOCALID\", 0))\n",
    "        # pick first node in allocation for rendezvous if MASTER_ADDR not set\n",
    "        if master_addr is None and \"SLURM_NODELIST\" in os.environ:\n",
    "            try:\n",
    "                hostnames = subprocess.check_output(\n",
    "                    [\"scontrol\", \"show\", \"hostnames\", os.environ[\"SLURM_NODELIST\"]]\n",
    "                ).decode().split()\n",
    "                master_addr = hostnames[0]\n",
    "            except Exception:\n",
    "                master_addr = os.environ.get(\"HOSTNAME\", \"127.0.0.1\")\n",
    "    # Else manual mode (you passed rank_and_world_size)\n",
    "    elif (rank is not None) and (world_size is not None):\n",
    "        local_rank = 0\n",
    "        master_addr = master_addr or \"127.0.0.1\"\n",
    "    else:\n",
    "        # single-process fallback\n",
    "        logger.info(\"No distributed env detected — running single-process\")\n",
    "        return 1, 0, 0\n",
    "\n",
    "    # Ensure MASTER_ADDR/PORT are set and use IPv4 to avoid errno 97\n",
    "    os.environ.setdefault(\"MASTER_ADDR\", master_addr or \"127.0.0.1\")\n",
    "    os.environ.setdefault(\"MASTER_PORT\", master_port)\n",
    "\n",
    "    # Set CUDA device according to local_rank (ordinal in visible devices)\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            torch.cuda.set_device(local_rank)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to set CUDA device {local_rank}. CUDA_VISIBLE_DEVICES={os.environ.get('CUDA_VISIBLE_DEVICES')}. Error: {e}\")\n",
    "            raise\n",
    "\n",
    "    # Initialize process group if needed\n",
    "    if world_size > 1:\n",
    "        dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "        logger.info(f\"Initialized DDP: rank={rank} world_size={world_size} local_rank={local_rank}\")\n",
    "    else:\n",
    "        logger.info(\"Single process (world_size <= 1) — skipping init_process_group\")\n",
    "\n",
    "    return world_size, rank, local_rank\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class AllGather(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        if dist.is_available() and dist.is_initialized() and (dist.get_world_size() > 1):\n",
    "            x = x.contiguous()\n",
    "            outputs = [torch.zeros_like(x) for _ in range(dist.get_world_size())]\n",
    "            dist.all_gather(outputs, x)\n",
    "            return torch.cat(outputs, 0)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grads):\n",
    "        if dist.is_available() and dist.is_initialized() and (dist.get_world_size() > 1):\n",
    "            s = (grads.shape[0] // dist.get_world_size()) * dist.get_rank()\n",
    "            e = (grads.shape[0] // dist.get_world_size()) * (dist.get_rank() + 1)\n",
    "            grads = grads.contiguous()\n",
    "            dist.all_reduce(grads)\n",
    "            return grads[s:e]\n",
    "        return grads\n",
    "\n",
    "\n",
    "class AllReduceSum(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        if dist.is_available() and dist.is_initialized() and (dist.get_world_size() > 1):\n",
    "            x = x.contiguous()\n",
    "            dist.all_reduce(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grads):\n",
    "        return grads\n",
    "\n",
    "\n",
    "class AllReduce(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        if dist.is_available() and dist.is_initialized() and (dist.get_world_size() > 1):\n",
    "            x = x.contiguous() / dist.get_world_size()\n",
    "            dist.all_reduce(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grads):\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
