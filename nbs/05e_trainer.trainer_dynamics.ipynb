{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb656fc",
   "metadata": {},
   "source": [
    "# Dynamics trainer\n",
    "\n",
    "> This module implements LeJepa training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bbb56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp trainers.trainer_dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd36cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import *\n",
    "from fastcore.utils import *\n",
    "from torchvision.utils import save_image\n",
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb85e38",
   "metadata": {},
   "source": [
    "## Dynamics Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from mawm.trainers.trainer import Trainer\n",
    "from mawm.models.utils import save_checkpoint\n",
    "from mawm.losses.sigreg import SIGReg\n",
    "from mawm.losses.vicreg import VICReg\n",
    "\n",
    "class DynamicsTrainer(Trainer):\n",
    "    def __init__(self, cfg, model, train_loader, val_loader=None,\n",
    "                 optimizer=None, device=None,earlystopping=None, \n",
    "                 scheduler=None, writer= None):\n",
    "        \n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "        self.model = model['jepa']\n",
    "        self.msg_encoder = model['msg_enc']\n",
    "        self.projector = model['projector']\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.earlystopping = earlystopping\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "        self.writer = writer\n",
    "\n",
    "        self.sigreg = SIGReg().to(self.device)\n",
    "        self.vicreg = VICReg().to(self.device) # TODO: ensure .cude comment for projector is returned back\n",
    "        self.lambda_ = self.cfg.loss.lambda_\n",
    "\n",
    "        self.device = device if device else torch.device('cpu')\n",
    "        self.agents = [f\"agent_{i}\" for i in range(len(self.cfg.env.agents))]\n",
    "\n",
    "        self.dmpc_dir = os.path.join(self.cfg.log_dir, 'dmpc_marlrid')\n",
    "        if not os.path.exists(self.dmpc_dir):\n",
    "            os.mkdir(self.dmpc_dir)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed8ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from mawm.models.utils import flatten_conv_output\n",
    "@patch\n",
    "def train_epoch(self: DynamicsTrainer, epoch):\n",
    "    self.model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    actual_len = 0\n",
    "\n",
    "    for batch_idx, data in enumerate(self.train_loader):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        for agent_id in range(self.agents):\n",
    "            obs, pos, _, act, _, dones = data[agent_id].values()\n",
    "            batch_samples = obs.size(0)\n",
    "            mask = ~dones.bool()     # keep only where done is False\n",
    "\n",
    "            if mask.sum() == 0: # CHECK: mask is determined per the reciever agent\n",
    "                continue  # entire batch is terminals\n",
    "\n",
    "            obs = obs[mask]          # filter observations\n",
    "            pos = pos[mask]\n",
    "            msg = msg[mask]\n",
    "            act = act[mask]\n",
    "\n",
    "            obs = obs.to(self.device)\n",
    "            pos = pos.to(self.device)\n",
    "            msg = msg.to(self.device)\n",
    "            act = act.to(self.device)\n",
    "\n",
    "            for other_agent in range(self.agents):\n",
    "                if other_agent != agent_id:\n",
    "                    obs_sender, pos_sender, msg, _, _, dones = data[other_agent].values()\n",
    "                    \n",
    "                    msg = msg[mask]\n",
    "                    obs_sender = obs_sender[mask]\n",
    "                    pos_sender = pos_sender[mask]\n",
    "\n",
    "                    obs_sender = obs_sender.to(self.device)\n",
    "                    pos_sender = pos_sender.to(self.device)\n",
    "                    msg = msg.to(self.device)\n",
    "\n",
    "            \n",
    "            C = self.msg_encoder(msg[:-1]) # [B, T-1, C, H, W] => [T-1, B, dim=32]\n",
    "\n",
    "            Z0, Z = self.model(x= obs, pos= pos, actions= act[:-1], msgs= C, T= act.size(0)-1)  # Z0, Z: [T-1, B, c, h, w]\n",
    "            vicreg_loss = self.vicreg(Z0, Z)\n",
    "            \n",
    "            z_sender = self.model.backbone(obs_sender[:-1], position= pos_sender[:-1])  # [T-1, B, c, h, w]\n",
    "            proj_z, proj_c = self.projector(z_sender, C)\n",
    "\n",
    "            inv_loss = (proj_z - proj_c).square().mean()\n",
    "\n",
    "            sigreg_img = self.sigreg(proj_z)\n",
    "            sigreg_msg = self.sigreg(proj_c)\n",
    "\n",
    "            sigreg_loss = (sigreg_img + sigreg_msg ) / 2.0\n",
    "            lejepa_loss = (1- self.lambda_) * inv_loss + self.lambda_ * sigreg_loss \n",
    "\n",
    "            loss = (lejepa_loss + vicreg_loss['total_loss']) / len(self.agents)\n",
    "            loss.backward()\n",
    "\n",
    "            train_loss += loss.item() * batch_samples            \n",
    "        \n",
    "        actual_len += batch_samples\n",
    "        self.optimizer.step()\n",
    "               \n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(obs), len(self.train_loader.dataset),\n",
    "                100. * batch_idx / len(self.train_loader),\n",
    "                loss.item() / len(obs)))\n",
    "        \n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "            epoch, train_loss / actual_len))\n",
    "\n",
    "    return train_loss / actual_len\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72ef3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from einops import rearrange\n",
    "@patch\n",
    "def eval_epoch(self: DynamicsTrainer):\n",
    "    self.model.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    actual_len = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch_idx, data in enumerate(self.val_loader):\n",
    "            for agent_id in range(self.agents):\n",
    "\n",
    "                obs, pos, msg, act, _, dones = data[agent_id].values()\n",
    "                mask = ~dones.bool()     # keep only where done is False\n",
    "\n",
    "                if mask.sum() == 0:\n",
    "                    continue  # entire batch is terminals\n",
    "\n",
    "                obs = obs[mask]          # filter observations\n",
    "                pos = pos[mask]\n",
    "                msg = msg[mask]\n",
    "                act = act[mask]\n",
    "\n",
    "                obs = obs.to(self.device)\n",
    "                pos = pos.to(self.device)\n",
    "                msg = msg.to(self.device)\n",
    "                act = act.to(self.device)\n",
    "\n",
    "                for other_agent in range(self.agents):\n",
    "                    if other_agent != agent_id:\n",
    "                        obs_sender, pos_sender, msg, _, _, dones = data[other_agent].values()\n",
    "                        \n",
    "                        msg = msg[mask]\n",
    "                        obs_sender = obs_sender[mask]\n",
    "                        pos_sender = pos_sender[mask]\n",
    "\n",
    "                        obs_sender = obs_sender.to(self.device)\n",
    "                        pos_sender = pos_sender.to(self.device)\n",
    "                        msg = msg.to(self.device)\n",
    "\n",
    "                C = self.msg_encoder(msg[:-1]) # [B, T-1, dim=32]\n",
    "                Z0, Z = self.model(x= obs, pos= pos, actions= act[:-1], msgs= C, T= self.cfg.data.seq_len - 1)\n",
    "                vicreg_loss = self.vicreg(Z0, Z)\n",
    "                \n",
    "                z_sender = self.model.backbone(obs_sender[:-1], position= pos_sender)\n",
    "                z_sender = rearrange(z_sender, 't b c h w -> (b t) (c h w)')\n",
    "\n",
    "                proj_z = self.z_projector(z_sender)\n",
    "                proj_c = self.msg_projector(C)\n",
    "\n",
    "                inv_loss = (proj_z - proj_c).square().mean()\n",
    "\n",
    "                sigreg_img = self.sigreg(proj_z)\n",
    "                sigreg_msg = self.sigreg(proj_c)\n",
    "\n",
    "                sigreg_loss = (sigreg_img + sigreg_msg ) / 2.0\n",
    "                lejepa_loss = (1- self.lambda_) * inv_loss + self.lambda_ * sigreg_loss \n",
    "\n",
    "                loss = lejepa_loss + vicreg_loss['total_loss']\n",
    "                train_loss += loss.item() * obs.size(0)\n",
    "                val_loss += loss.item() * obs.size(0)\n",
    "                actual_len += obs.size(0)\n",
    "\n",
    "    val_loss /= actual_len\n",
    "    print('====> Test set loss: {:.4f}'.format(val_loss))\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0041e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import wandb\n",
    "\n",
    "@patch\n",
    "def fit(self: DynamicsTrainer):\n",
    "    self.model.to(self.device)\n",
    "    self.msg_encoder.to(self.device)\n",
    "    self.z_projector.to(self.device)\n",
    "    self.msg_projector.to(self.device)\n",
    "    \n",
    "    cur_best = None\n",
    "    lst_dfs = []\n",
    "\n",
    "    for epoch in range(1, self.cfg.epochs + 1):\n",
    "        lr = self.scheduler.adjust_learning_rate(epoch)\n",
    "        train_loss = self.train_epoch(epoch)\n",
    "        val_loss = self.eval_epoch()\n",
    "\n",
    "        # checkpointing\n",
    "        best_filename = os.path.join(self.lejepa_dir, 'best.pth')\n",
    "        filename = os.path.join(self.lejepa_dir, 'checkpoint.pth')\n",
    "\n",
    "        is_best = not cur_best or val_loss < cur_best\n",
    "        if is_best:\n",
    "            cur_best = val_loss\n",
    "\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "        }\n",
    "        save_checkpoint(state= state, is_best= is_best, filename= filename, best_filename= best_filename)\n",
    "\n",
    "        to_log = {\n",
    "            \"train_loss\": train_loss, \n",
    "            \"val_loss\": val_loss,\n",
    "        }\n",
    "\n",
    "        self.writer.write(to_log)\n",
    "        df = pd.DataFrame.from_records([{\"epoch\": epoch ,\"train_loss\": train_loss, \"val_loss\":val_loss}], index= \"epoch\")\n",
    "        lst_dfs.append(df)\n",
    "\n",
    "\n",
    "    df_res = pd.concat(lst_dfs)\n",
    "    df_reset = df_res.reset_index()\n",
    "    self.writer.write({'Train-Val Loss Table': wandb.Table(dataframe= df_reset)})\n",
    "\n",
    "    self.writer.finish()\n",
    "    return df_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69feff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
