{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2864b51",
   "metadata": {},
   "source": [
    "# VAE encoder\n",
    "\n",
    "> a VAE module for percpetion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ae85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87425da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcab1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import *\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463122dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\"\"\"\n",
    "Variational encoder model, used as a visual model\n",
    "for our model of the world.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"VAE encoder for 32×32 RGB images\"\"\"\n",
    "    def __init__(self, img_channels, latent_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(img_channels,  32, 4, stride=2, padding=1)   # 32→16\n",
    "        self.conv2 = nn.Conv2d(32,  64, 4, stride=2, padding=1)             # 16→8\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4, stride=2, padding=1)             # 8→4\n",
    "        self.conv4 = nn.Conv2d(128, 256, 4, stride=2, padding=1)            # 4→2\n",
    "\n",
    "        self.fc_mu        = nn.Linear(256 * 2 * 2, latent_size)\n",
    "        self.fc_logsigma  = nn.Linear(256 * 2 * 2, latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  \n",
    "        x = F.relu(self.conv2(x))  \n",
    "        x = F.relu(self.conv3(x))  \n",
    "        x = F.relu(self.conv4(x))  \n",
    "        print(x.shape)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "\n",
    "        mu = self.fc_mu(x)\n",
    "        logsigma = self.fc_logsigma(x)\n",
    "        return mu, logsigma\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230fbed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 32]), torch.Size([16, 32]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "enc = Encoder(3, 32)\n",
    "x = torch.randn(16, 3, 32, 32)\n",
    "mu, logsigma = enc(x)\n",
    "mu.shape, logsigma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c969c874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten = torch.randn(4, 32)\n",
    "ten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d700f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5277e-02, -8.6355e-01, -2.4682e+00, -1.8421e+00,  1.3121e-01,\n",
       "          2.3854e+00, -8.8746e-01, -4.6337e-01,  4.9301e-01,  6.7903e-01,\n",
       "          5.4017e-01, -1.8188e+00,  9.8022e-01,  1.0151e+00, -2.5583e-01,\n",
       "         -3.9285e-01, -8.1354e-01,  1.0092e+00,  2.0673e-01,  6.0336e-01,\n",
       "          5.1131e-01,  5.8135e-01,  1.8519e+00,  1.4168e+00, -4.1078e-01,\n",
       "         -2.5738e-01, -1.1851e+00, -3.6064e-01,  1.1961e-01, -2.6908e-01,\n",
       "          9.0220e-02,  5.6336e-01],\n",
       "        [ 1.5776e-03, -4.2801e-01, -2.1140e-01,  4.2696e-01, -5.6730e-01,\n",
       "          1.4228e+00,  1.4029e+00, -2.1389e+00, -1.2331e+00, -1.1081e-01,\n",
       "         -3.6752e-01,  2.0156e+00, -2.2204e+00, -2.0780e-01,  1.0776e+00,\n",
       "         -7.0024e-01, -3.1071e-01,  1.2378e-01,  1.6334e-01, -3.1914e-01,\n",
       "         -1.7702e-01, -3.9492e-02,  1.3265e-01, -1.0066e-01,  6.1435e-01,\n",
       "         -2.1366e-01, -4.9868e-01,  2.3917e-01, -1.1005e+00,  5.3529e-01,\n",
       "          1.3505e+00, -3.2555e-01],\n",
       "        [ 1.3088e+00, -8.0877e-01, -4.4868e-01,  3.6858e-01, -1.5153e+00,\n",
       "          7.1025e-02,  5.6137e-01, -7.4987e-01, -1.7124e-01,  2.9927e-02,\n",
       "          1.0483e-01,  1.7187e+00, -2.5132e-01, -7.9441e-01,  7.4304e-01,\n",
       "         -3.3271e-01, -1.1518e-01,  2.8902e+00, -7.7834e-01,  5.9838e-01,\n",
       "          2.1240e+00,  3.4864e-01, -2.1857e+00,  2.0833e-02, -6.6406e-01,\n",
       "         -1.8563e-01, -7.0466e-01, -2.4911e-02,  1.4266e+00, -1.1773e+00,\n",
       "          4.9909e-01,  1.3571e+00],\n",
       "        [-1.7755e-02, -1.4424e+00, -7.3357e-01,  1.2393e+00,  1.1981e+00,\n",
       "          8.4002e-01, -4.4515e-01,  5.7663e-01, -1.5994e+00, -7.9235e-01,\n",
       "         -5.8861e-01,  2.0852e+00,  2.5807e-01, -1.6358e+00,  1.3108e+00,\n",
       "          7.3649e-01, -9.5916e-01, -4.6577e-01, -1.2199e+00,  1.1018e+00,\n",
       "         -5.9666e-01,  2.7340e+00, -3.3996e+00, -1.0321e+00, -9.2745e-02,\n",
       "          4.7782e-01,  2.8663e+00,  1.6737e+00,  7.9715e-01,  7.3061e-01,\n",
       "         -6.0402e-01,  3.2260e-01]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b2a381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0179, 0.0077, 0.0016, 0.0029, 0.0210, 0.1996, 0.0076, 0.0116, 0.0301,\n",
       "         0.0362, 0.0315, 0.0030, 0.0490, 0.0507, 0.0142, 0.0124, 0.0081, 0.0504,\n",
       "         0.0226, 0.0336, 0.0306, 0.0329, 0.1171, 0.0758, 0.0122, 0.0142, 0.0056,\n",
       "         0.0128, 0.0207, 0.0140, 0.0201, 0.0323],\n",
       "        [0.0221, 0.0144, 0.0178, 0.0338, 0.0125, 0.0914, 0.0896, 0.0026, 0.0064,\n",
       "         0.0197, 0.0153, 0.1653, 0.0024, 0.0179, 0.0647, 0.0109, 0.0161, 0.0249,\n",
       "         0.0259, 0.0160, 0.0185, 0.0212, 0.0251, 0.0199, 0.0407, 0.0178, 0.0134,\n",
       "         0.0280, 0.0073, 0.0376, 0.0850, 0.0159],\n",
       "        [0.0550, 0.0066, 0.0095, 0.0215, 0.0033, 0.0160, 0.0261, 0.0070, 0.0125,\n",
       "         0.0153, 0.0165, 0.0829, 0.0116, 0.0067, 0.0313, 0.0107, 0.0132, 0.2676,\n",
       "         0.0068, 0.0270, 0.1244, 0.0211, 0.0017, 0.0152, 0.0077, 0.0123, 0.0073,\n",
       "         0.0145, 0.0619, 0.0046, 0.0245, 0.0578],\n",
       "        [0.0120, 0.0029, 0.0059, 0.0421, 0.0404, 0.0282, 0.0078, 0.0217, 0.0025,\n",
       "         0.0055, 0.0068, 0.0981, 0.0158, 0.0024, 0.0452, 0.0255, 0.0047, 0.0076,\n",
       "         0.0036, 0.0367, 0.0067, 0.1876, 0.0004, 0.0043, 0.0111, 0.0197, 0.2142,\n",
       "         0.0650, 0.0270, 0.0253, 0.0067, 0.0168]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(ten, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6842d77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(ten, dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b79ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 11, 17, 26])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from the softmax distribution\n",
    "torch.argmax(torch.softmax(ten, dim=-1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b71557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"VAE decoder for 32×32 RGB images\"\"\"\n",
    "    def __init__(self, img_channels, latent_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(latent_size, 256 * 2 * 2)\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1)  # 2→4\n",
    "        self.deconv2 = nn.ConvTranspose2d(128,  64, 4, stride=2, padding=1)  # 4→8\n",
    "        self.deconv3 = nn.ConvTranspose2d(64,   32, 4, stride=2, padding=1)  # 8→16\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, img_channels, 4, stride=2, padding=1) # 16→32\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(-1, 256, 2, 2)\n",
    "\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        x = F.relu(self.deconv3(x))\n",
    "\n",
    "        reconstruction = torch.sigmoid(self.deconv4(x))\n",
    "        return reconstruction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e720a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 32, 32])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "dec = Decoder(3, 32)\n",
    "z = torch.randn(16, 32)\n",
    "recon = dec(z)\n",
    "recon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ce3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAE(nn.Module):\n",
    "    \"\"\" Variational Autoencoder \"\"\"\n",
    "    def __init__(self, img_channels, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(img_channels, latent_size)\n",
    "        self.decoder = Decoder(img_channels, latent_size)\n",
    "\n",
    "    def forward(self, x): # pylint: disable=arguments-differ\n",
    "        mu, logsigma = self.encoder(x)\n",
    "        sigma = logsigma.exp()\n",
    "        eps = torch.randn_like(sigma)\n",
    "        z = eps.mul(sigma).add_(mu)\n",
    "\n",
    "        recon_x = self.decoder(z)\n",
    "        return recon_x, mu, logsigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2de076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 32])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "model = VAE(3, 32)\n",
    "x = torch.randn(16, 3, 32, 32)\n",
    "mu, logsigma = model.encoder(x)\n",
    "mu.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d645d903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a568f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
