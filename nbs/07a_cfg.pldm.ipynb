{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cfg\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from collections import deque\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import wandb\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "\n",
    "\n",
    "class Logger:\n",
    "    _instance = None\n",
    "\n",
    "    @classmethod\n",
    "    def run(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = cls()\n",
    "        return cls._instance\n",
    "\n",
    "    def __init__(self):\n",
    "        self.file_path = None\n",
    "        self.log_step = None\n",
    "        self.current_log = None\n",
    "        self.current_summary = None\n",
    "        self.wandb_enabled = None\n",
    "        self.output_path = None\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(\n",
    "        self,\n",
    "        output_path: Optional[str] = None,\n",
    "        *,\n",
    "        wandb_enabled: bool = False,\n",
    "        project: Optional[str] = None,\n",
    "        name: Optional[str] = None,\n",
    "        group: Optional[str] = None,\n",
    "        config: Optional[Dict[str, Any]] = None,\n",
    "    ):\n",
    "        if self.initialized:\n",
    "            print(\"Logger already initialized, skipping\")\n",
    "            return\n",
    "        if output_path is not None:\n",
    "            self.output_path: Path = Path(output_path)\n",
    "            os.makedirs(self.output_path, exist_ok=True)\n",
    "        else:\n",
    "            self.output_path = None\n",
    "\n",
    "        self.wandb_enabled: bool = wandb_enabled\n",
    "        if self.wandb_enabled:\n",
    "            assert project is not None, \"wandb requires project name\"\n",
    "            wandb.init(\n",
    "                project=project,\n",
    "                name=name,\n",
    "                group=group,\n",
    "                config=config,\n",
    "                settings=wandb.Settings(start_method=\"fork\"),\n",
    "                dir=self.output_path,\n",
    "            )\n",
    "        self.config: Optional[Dict[str, Any]] = config\n",
    "        self.save_config()\n",
    "        self.log_step = 0\n",
    "        self.current_log = {}\n",
    "        self.current_summary = {}\n",
    "        self.initialized = True\n",
    "\n",
    "    def log(\n",
    "        self,\n",
    "        log_dict: Dict[str, Any],\n",
    "        *,\n",
    "        commit: bool = True,\n",
    "    ):\n",
    "        self.current_log.update(log_dict)\n",
    "        self.log_step += 1\n",
    "        if commit:\n",
    "            self.commit()\n",
    "\n",
    "    def log_across_t(\n",
    "        self,\n",
    "        data: torch.Tensor,  # shape (T,)\n",
    "        name: str,\n",
    "    ):\n",
    "        for val in data:\n",
    "            self.log({name: val.item()})\n",
    "\n",
    "    def commit(self):\n",
    "        if self.wandb_enabled:\n",
    "            wandb.log(self.current_log)\n",
    "\n",
    "        if self.output_path is not None:\n",
    "            with (self.output_path / \"log.json\").open(\"a\") as f:\n",
    "                f.write(json.dumps(self.clean_dict(self.current_log)) + \"\\n\")\n",
    "\n",
    "        self.current_log = {}\n",
    "\n",
    "    def log_summary(self, log_dict: Dict[str, Any], commit: bool = True):\n",
    "        self.current_summary.update(log_dict)\n",
    "        if commit:\n",
    "            self.commit_summary()\n",
    "\n",
    "    def commit_summary(self):\n",
    "        if self.wandb_enabled:\n",
    "            wandb.summary.update(self.current_summary)\n",
    "\n",
    "        if self.output_path is not None:\n",
    "            with (self.output_path / \"summary.json\").open(\"w\") as f:\n",
    "                f.write(json.dumps(self.clean_dict(self.current_summary), indent=4))\n",
    "\n",
    "    def save_config(self):\n",
    "        if self.config is not None and self.output_path is not None:\n",
    "            p = self.output_path / \"config.yaml\"\n",
    "            with p.open(\"w\") as f:\n",
    "                OmegaConf.save(config=self.config, f=f)\n",
    "\n",
    "    def save_summary(self, filename):\n",
    "        if self.output_path is not None:\n",
    "            with (self.output_path / filename).open(\"w\") as f:\n",
    "                f.write(json.dumps(self.clean_dict(self.current_summary), indent=4))\n",
    "\n",
    "    def clean_dict(self, log_dict: Dict[str, Any]):\n",
    "        for k in log_dict.keys():\n",
    "            if isinstance(log_dict[k], dict):\n",
    "                self.clean_dict(log_dict[k])\n",
    "            elif isinstance(log_dict[k], torch.Tensor):\n",
    "                log_dict[k] = log_dict[k].item()\n",
    "        return log_dict\n",
    "\n",
    "    def log_line_plot(\n",
    "        self,\n",
    "        data: list,\n",
    "        plot_name: str,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Logs a single line plot with WandB.\n",
    "\n",
    "        :param data: list of tuples [[x1, y1], [x2, y2], ...]\n",
    "        :param plot_name: The name to assign to the plot in WandB.\n",
    "        \"\"\"\n",
    "        if self.wandb_enabled:\n",
    "            table = wandb.Table(data=data, columns=[\"x\", \"y\"])\n",
    "            wandb.log(\n",
    "                {\n",
    "                    f\"Custom Plots/{plot_name}\": wandb.plot.line(\n",
    "                        table, \"x\", \"y\", title=plot_name\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "\n",
    "    def log_multiline_plot(\n",
    "        self,\n",
    "        xs: list,\n",
    "        ys: list,\n",
    "        plot_name: str,\n",
    "    ):\n",
    "        if self.wandb_enabled:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    f\"Custom Plots/{plot_name}\": wandb.plot.line_series(\n",
    "                        xs=xs,\n",
    "                        ys=ys,\n",
    "                        keys=[\"Pos Traj\", \"Neg Traj\"],\n",
    "                        title=plot_name,\n",
    "                        xname=\"t\",\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "\n",
    "    def log_figure(self, figure: Any, name: str):\n",
    "        if self.output_path is not None:\n",
    "            # This assumes that the image is already saved to disk\n",
    "            filename = Path(self.output_path) / \"media\" / f\"{name}\"\n",
    "            filename.parent.mkdir(parents=True, exist_ok=True)\n",
    "        else:\n",
    "            filename = tempfile.NamedTemporaryFile(suffix=\".png\").name[\n",
    "                :-4\n",
    "            ]  # remove .png\n",
    "        figure.savefig(filename)\n",
    "\n",
    "        if self.wandb_enabled:\n",
    "            # matplotlib automatically adds the extension\n",
    "            wandb.log({name: wandb.Image(f\"{filename}.png\")})\n",
    "\n",
    "\n",
    "class MetricTracker:\n",
    "    def __init__(self, window_size: int):\n",
    "        self.window_size = window_size\n",
    "        self.data: Dict[str, Tuple[deque, float, float, float, float]] = {}\n",
    "\n",
    "    def update(self, key: str, value: float):\n",
    "        if key not in self.data:\n",
    "            self.data[key] = deque(maxlen=self.window_size), value, value, value, value\n",
    "        else:\n",
    "            values, mean, minimum, maximum, last = self.data[key]\n",
    "            values.append(value)\n",
    "            mean = sum(values) / len(values)\n",
    "            minimum = min(minimum, value)\n",
    "            maximum = max(maximum, value)\n",
    "            last = value\n",
    "            self.data[key] = values, mean, minimum, maximum, last\n",
    "\n",
    "    def build_log_dict(self) -> Dict[str, Dict[str, float]]:\n",
    "        result = {}\n",
    "        for key, (_values, mean, minimum, maximum, last) in self.data.items():\n",
    "            result.update(\n",
    "                {\n",
    "                    f\"{key}/mean\": mean,\n",
    "                    f\"{key}/minimum\": minimum,\n",
    "                    f\"{key}/maximum\": maximum,\n",
    "                    f\"{key}/last\": last,\n",
    "                }\n",
    "            )\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
