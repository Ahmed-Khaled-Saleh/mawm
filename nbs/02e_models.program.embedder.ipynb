{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Module\n",
    "\n",
    "> Embedding works by representing programs as fixed-size tensors of dimension dim, such that each program element is one-third of the values of this tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.program.embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import *\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AgentAt': 0, 'GoalAt': 1, 'ObstacleAt': 2, 'ItemAt': 3, 'Near': 4, 'CanMove': 5}\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "from mawm.core import Program, PRIMITIVE_TEMPLATES\n",
    "\n",
    "p = Program(tokens= [(0, [1.0, 2.0]), (4, []), (1, [3.0, 4.0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(PRIMITIVE_TEMPLATES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from mawm.core import Program, PRIMITIVE_TEMPLATES\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "MAX_PARAMS = 2\n",
    "\n",
    "def get_indices(program, max_params=MAX_PARAMS, device=\"cpu\", padding_vals=[-1, -1]):\n",
    "    \n",
    "    L = len(program.tokens)\n",
    "    if L == 0:\n",
    "        # Handle empty program as requested\n",
    "        prim_ids = torch.zeros((1, 0), dtype=torch.long, device=device)\n",
    "        param_tensor = torch.zeros((1, 0, max_params), dtype=torch.long, device=device)\n",
    "        prim_ids.add_(padding_vals[0])\n",
    "        param_tensor.add_(padding_vals[1])\n",
    "        return prim_ids, param_tensor\n",
    "\n",
    "    prim_ids_list = []\n",
    "    params_list = []\n",
    "    for (prim_idx, params) in program.tokens:\n",
    "        prim_ids_list.append(int(prim_idx))\n",
    "        p = list(params)[:max_params]\n",
    "        if len(p) < max_params:\n",
    "            # Padding parameters with -1\n",
    "            p = p + [padding_vals[1]] * (max_params - len(p))\n",
    "        params_list.append(p)\n",
    "        \n",
    "    prim_ids = torch.tensor([prim_ids_list], dtype=torch.long, device=device)          # (1, L)\n",
    "    param_tensor = torch.tensor([params_list], dtype=torch.long, device=device)   # (1, L, max_params)\n",
    "    return prim_ids, param_tensor\n",
    "\n",
    "\n",
    "\n",
    "# final_batch_prim_ids shape: (2, 3)\n",
    "# final_batch_param_tensor shape: (2, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def batchify_programs(batch_programs, padding_vals=[-1, -1]):\n",
    "    all_prim_tensors = []\n",
    "    all_param_tensors = []\n",
    "    max_len = 0\n",
    "\n",
    "    for program in batch_programs:\n",
    "        prim_ids, param_tensor = get_indices(program, padding_vals=padding_vals)\n",
    "        all_prim_tensors.append(prim_ids.squeeze(0))     \n",
    "        all_param_tensors.append(param_tensor.squeeze(0))\n",
    "        max_len = max(max_len, prim_ids.size(1))\n",
    "\n",
    "    # USE EOS AS PAD\n",
    "    PAD_PRIM = padding_vals[0]   # <---- IMPORTANT CHANGE\n",
    "    PAD_PARAM = padding_vals[1]       # parameters can remain -1\n",
    "\n",
    "    padded_prim_ids = []\n",
    "    padded_param_tensors = []\n",
    "\n",
    "    for prim_t, param_t in zip(all_prim_tensors, all_param_tensors):\n",
    "        L = prim_t.size(0)\n",
    "        pad_len = max_len - L\n",
    "        \n",
    "        padded_prim_ids.append(F.pad(prim_t, (0, pad_len), value=PAD_PRIM))   # <-- EOS PAD\n",
    "\n",
    "        padded_param_tensors.append(F.pad(param_t, (0, 0, 0, pad_len), value=PAD_PARAM))\n",
    "\n",
    "    batch_prim_ids = torch.stack(padded_prim_ids, dim=0)\n",
    "    batch_param_tensor = torch.stack(padded_param_tensors, dim=0)\n",
    "    return batch_prim_ids, batch_param_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# def batchify_programs(batch_programs):\n",
    "#     # --- Correct Batching Logic ---\n",
    "#     all_prim_tensors = []\n",
    "#     all_param_tensors = []\n",
    "#     max_len = 0\n",
    "\n",
    "#     # 1. Get individual tensors and find max_len\n",
    "#     for program in batch_programs:\n",
    "#         prim_ids, param_tensor = get_indices(program)\n",
    "#         all_prim_tensors.append(prim_ids.squeeze(0))     # Remove the (1) batch dim: (L)\n",
    "#         all_param_tensors.append(param_tensor.squeeze(0)) # Remove the (1) batch dim: (L, max_params)\n",
    "#         max_len = max(max_len, prim_ids.size(1))\n",
    "\n",
    "#     # 2. Pad and Stack\n",
    "#     padded_prim_ids = []\n",
    "#     padded_param_tensors = []\n",
    "#     PAD_VALUE = -1 # Use -1 for padding as in your get_indices function\n",
    "\n",
    "#     for prim_t, param_t in zip(all_prim_tensors, all_param_tensors):\n",
    "#         L = prim_t.size(0)\n",
    "        \n",
    "#         # Pad Prim IDs: (L) -> (L_max)\n",
    "#         pad_len = max_len - L\n",
    "#         padded_prim_ids.append(F.pad(prim_t, (0, pad_len), value=PAD_VALUE))\n",
    "        \n",
    "#         # Pad Param Tensor: (L, max_p) -> (L_max, max_p)\n",
    "#         # F.pad takes (padding_left, padding_right, padding_top, padding_bottom, ...)\n",
    "#         padded_param_tensors.append(F.pad(param_t, (0, 0, 0, pad_len), value=PAD_VALUE))\n",
    "\n",
    "#     # 3. Concatenate (Stack)\n",
    "#     batch_prim_ids = torch.stack(padded_prim_ids, dim=0)    # (B, L_max)\n",
    "#     batch_param_tensor = torch.stack(padded_param_tensors, dim=0) # (B, L_max, max_params)\n",
    "#     return batch_prim_ids, batch_param_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_programs = [\n",
    "    Program(tokens= [(0, [1.0, 2.0]), (4, []), (1, [3.0, 4.0]),(5, [1]) ]), # L=4\n",
    "    Program(tokens= [(2, [5.0]), (3, [6.0, 5.0])]) # L=2\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_batch_prim_ids, final_batch_param_tensor = batchify_programs(batch_programs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1,  2],\n",
       "          [-1, -1],\n",
       "          [ 3,  4],\n",
       "          [ 1, -1]],\n",
       " \n",
       "         [[ 5, -1],\n",
       "          [ 6,  5],\n",
       "          [-1, -1],\n",
       "          [-1, -1]]]),\n",
       " tensor([[ 0,  4,  1,  5],\n",
       "         [ 2,  3, -1, -1]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_batch_param_tensor, final_batch_prim_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 2]), torch.Size([2, 4]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_batch_param_tensor.shape, final_batch_prim_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_batch_prim_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2, -1, -1,  3,  4,  1, -1],\n",
       "        [ 5, -1,  6,  5, -1, -1, -1, -1]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_batch_param_tensor.reshape(2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_batch_param_tensor.reshape(2, -1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ProgramEmbedder(nn.Module):\n",
    "    \"Embeds a program into a fixed-size vector.\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_primitives,\n",
    "        param_cardinalities,   # list: for each slot, how many discrete values possible\n",
    "        max_params_per_primitive,\n",
    "        d_name=32,\n",
    "        d_param=32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_primitives = num_primitives\n",
    "        self.max_params = max_params_per_primitive\n",
    "        self.empty_name = nn.Parameter(torch.zeros(d_name))\n",
    "        self.empty_param = nn.Parameter(torch.zeros(d_param))\n",
    "        \n",
    "        self.name_embed = nn.Embedding(num_primitives + 1, d_name)\n",
    "        self.param_embeds = nn.ModuleList([\n",
    "            nn.Embedding(card, d_param) for card in param_cardinalities\n",
    "        ])\n",
    "\n",
    "    def forward(self, prim_ids, params_ids):\n",
    "        \"\"\"\n",
    "        prim_ids: (B, L) LongTensor of primitive IDs\n",
    "        params_ids: (B, L, max_params_per_primitive) LongTensor of parameter IDs\n",
    "        returns: (B, L, D) Tensor of embedded programs\n",
    "        \"\"\"\n",
    "        B, L = prim_ids.shape\n",
    "        d_name = self.name_embed.embedding_dim\n",
    "        d_param = self.param_embeds[0].embedding_dim\n",
    "        D = d_name + self.max_params * d_param\n",
    "\n",
    "        mask = (prim_ids == -1)\n",
    "        name_embeds_B_L_D = self.name_embed(torch.clamp(prim_ids, min=0))  # (B, L, d_name)\n",
    "        name_embeds_B_L_D[mask] = self.empty_name\n",
    "        \n",
    "        param_vecs = []\n",
    "        for slot, embed in enumerate(self.param_embeds):\n",
    "            slot_vals = params_ids[:, :, slot]  # [N]\n",
    "            mask = (slot_vals == -1)\n",
    "\n",
    "            slot_embed = embed(torch.clamp(slot_vals, min=0))\n",
    "            slot_embed[mask] = self.empty_param\n",
    "\n",
    "            param_vecs.append(slot_embed)\n",
    "\n",
    "        params_emb_stacked = torch.stack(param_vecs, dim=2) # (B, L, num_params, d)\n",
    "        params_vect_embeds = params_emb_stacked.view(B, L, -1) # (B, L, num_params * d)\n",
    "\n",
    "        program_embed_B_L_D = torch.cat([name_embeds_B_L_D, params_vect_embeds], dim=-1) # (B, L, D)\n",
    "\n",
    "        return program_embed_B_L_D\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "emb = ProgramEmbedder(\n",
    "    num_primitives= len(PRIMITIVE_TEMPLATES),\n",
    "    param_cardinalities= [7, 7],\n",
    "    max_params_per_primitive= 2,\n",
    ")\n",
    "emb(final_batch_prim_ids, final_batch_param_tensor).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = final_batch_prim_ids\n",
    "mask = (a == -1)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1, -1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 32])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_layer = nn.Embedding(6, 32)\n",
    "\n",
    "prim_vec = emb_layer(a.clamp(min=0))\n",
    "prim_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5241,  0.6214, -0.2335,  0.8660, -2.6045, -0.3754, -0.1755,\n",
       "           0.6233,  0.0270, -0.8318, -0.6073,  0.0662,  2.3057,  0.1164,\n",
       "          -0.2514, -0.3850, -0.4884, -2.8957, -1.4303, -0.8880, -0.1274,\n",
       "          -0.9298,  0.6627,  0.0627,  1.1778, -1.2716, -1.0698,  1.3303,\n",
       "          -1.4972, -0.7821, -0.6830,  0.5980],\n",
       "         [-0.9660,  0.6694, -0.2911, -1.2771,  0.3612, -1.9370,  0.7846,\n",
       "           0.1405, -0.3342,  0.1671,  1.1493,  0.4928,  0.0317, -1.3324,\n",
       "           1.0530, -0.6200,  0.6001,  0.5116, -0.2895,  0.1138,  0.5460,\n",
       "           0.5285, -0.8443,  1.0123, -0.0393, -0.2357,  0.0041,  0.7768,\n",
       "          -0.4302,  0.5859,  0.4848, -0.0973],\n",
       "         [-2.2233,  1.4776,  0.9002,  1.4389,  1.2548,  0.4807, -0.5116,\n",
       "          -0.9130,  0.6739, -0.4171,  0.3680,  1.0555, -0.1830, -0.1053,\n",
       "           1.0511,  0.9505, -0.8490, -0.2548, -1.8161,  0.5618,  0.6206,\n",
       "          -0.3532, -1.2162, -0.0491,  1.1764, -0.4762, -1.3392, -0.3072,\n",
       "          -0.0452, -1.1065, -0.2142, -0.2688],\n",
       "         [-0.0315,  1.2161, -0.0626,  0.9119, -0.2267, -1.5794,  0.2285,\n",
       "           1.2482, -1.0147, -0.8133,  1.3507,  0.6270, -0.9814,  0.3452,\n",
       "           0.7397,  0.9011,  0.2993,  0.7982, -0.3309,  1.1461, -0.7812,\n",
       "          -1.0940, -1.2089, -0.6191, -0.9015,  2.2008,  0.1947,  0.3217,\n",
       "           1.0726,  0.4520,  1.4605, -0.9129]],\n",
       "\n",
       "        [[ 0.1603, -0.3577,  0.7219, -0.2128, -0.9732, -0.1298,  0.8973,\n",
       "          -0.9747,  1.4579, -0.0235,  0.9503, -1.0543,  2.0910, -0.6613,\n",
       "           0.4819,  1.4121,  1.1645, -0.3497,  0.5378,  0.7334, -0.3049,\n",
       "          -1.6914,  0.8584,  0.3278, -0.1136,  1.1196, -1.7048, -1.4162,\n",
       "          -1.6465,  3.0813,  0.8280,  0.0337],\n",
       "         [-0.4772,  0.7409, -0.2055,  1.1334, -0.9902, -0.0287,  1.3537,\n",
       "           1.0487, -0.3696, -0.0452,  0.8232,  0.6450, -1.7416,  0.9575,\n",
       "          -1.2927, -0.4206,  0.3107,  1.1882, -0.9942,  1.3522,  1.5464,\n",
       "          -0.2102, -1.2271,  0.0372, -0.1467, -0.8834,  0.2201, -0.3428,\n",
       "           0.2689,  0.5000,  1.2018,  0.3673],\n",
       "         [ 1.5241,  0.6214, -0.2335,  0.8660, -2.6045, -0.3754, -0.1755,\n",
       "           0.6233,  0.0270, -0.8318, -0.6073,  0.0662,  2.3057,  0.1164,\n",
       "          -0.2514, -0.3850, -0.4884, -2.8957, -1.4303, -0.8880, -0.1274,\n",
       "          -0.9298,  0.6627,  0.0627,  1.1778, -1.2716, -1.0698,  1.3303,\n",
       "          -1.4972, -0.7821, -0.6830,  0.5980],\n",
       "         [ 1.5241,  0.6214, -0.2335,  0.8660, -2.6045, -0.3754, -0.1755,\n",
       "           0.6233,  0.0270, -0.8318, -0.6073,  0.0662,  2.3057,  0.1164,\n",
       "          -0.2514, -0.3850, -0.4884, -2.8957, -1.4303, -0.8880, -0.1274,\n",
       "          -0.9298,  0.6627,  0.0627,  1.1778, -1.2716, -1.0698,  1.3303,\n",
       "          -1.4972, -0.7821, -0.6830,  0.5980]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prim_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prim_vec[mask] = nn.Parameter(torch.zeros(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5241,  0.6214, -0.2335,  0.8660, -2.6045, -0.3754, -0.1755,\n",
       "           0.6233,  0.0270, -0.8318, -0.6073,  0.0662,  2.3057,  0.1164,\n",
       "          -0.2514, -0.3850, -0.4884, -2.8957, -1.4303, -0.8880, -0.1274,\n",
       "          -0.9298,  0.6627,  0.0627,  1.1778, -1.2716, -1.0698,  1.3303,\n",
       "          -1.4972, -0.7821, -0.6830,  0.5980],\n",
       "         [-0.9660,  0.6694, -0.2911, -1.2771,  0.3612, -1.9370,  0.7846,\n",
       "           0.1405, -0.3342,  0.1671,  1.1493,  0.4928,  0.0317, -1.3324,\n",
       "           1.0530, -0.6200,  0.6001,  0.5116, -0.2895,  0.1138,  0.5460,\n",
       "           0.5285, -0.8443,  1.0123, -0.0393, -0.2357,  0.0041,  0.7768,\n",
       "          -0.4302,  0.5859,  0.4848, -0.0973],\n",
       "         [-2.2233,  1.4776,  0.9002,  1.4389,  1.2548,  0.4807, -0.5116,\n",
       "          -0.9130,  0.6739, -0.4171,  0.3680,  1.0555, -0.1830, -0.1053,\n",
       "           1.0511,  0.9505, -0.8490, -0.2548, -1.8161,  0.5618,  0.6206,\n",
       "          -0.3532, -1.2162, -0.0491,  1.1764, -0.4762, -1.3392, -0.3072,\n",
       "          -0.0452, -1.1065, -0.2142, -0.2688],\n",
       "         [-0.0315,  1.2161, -0.0626,  0.9119, -0.2267, -1.5794,  0.2285,\n",
       "           1.2482, -1.0147, -0.8133,  1.3507,  0.6270, -0.9814,  0.3452,\n",
       "           0.7397,  0.9011,  0.2993,  0.7982, -0.3309,  1.1461, -0.7812,\n",
       "          -1.0940, -1.2089, -0.6191, -0.9015,  2.2008,  0.1947,  0.3217,\n",
       "           1.0726,  0.4520,  1.4605, -0.9129]],\n",
       "\n",
       "        [[ 0.1603, -0.3577,  0.7219, -0.2128, -0.9732, -0.1298,  0.8973,\n",
       "          -0.9747,  1.4579, -0.0235,  0.9503, -1.0543,  2.0910, -0.6613,\n",
       "           0.4819,  1.4121,  1.1645, -0.3497,  0.5378,  0.7334, -0.3049,\n",
       "          -1.6914,  0.8584,  0.3278, -0.1136,  1.1196, -1.7048, -1.4162,\n",
       "          -1.6465,  3.0813,  0.8280,  0.0337],\n",
       "         [-0.4772,  0.7409, -0.2055,  1.1334, -0.9902, -0.0287,  1.3537,\n",
       "           1.0487, -0.3696, -0.0452,  0.8232,  0.6450, -1.7416,  0.9575,\n",
       "          -1.2927, -0.4206,  0.3107,  1.1882, -0.9942,  1.3522,  1.5464,\n",
       "          -0.2102, -1.2271,  0.0372, -0.1467, -0.8834,  0.2201, -0.3428,\n",
       "           0.2689,  0.5000,  1.2018,  0.3673],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000]]], grad_fn=<IndexPutBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prim_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_embeds = nn.ModuleList([\n",
    "            nn.Embedding(card, 32) for card in [7, 7]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Embedding(7, 32)\n",
      "1 Embedding(7, 32)\n"
     ]
    }
   ],
   "source": [
    "for slot, embed in enumerate(layers_embeds):\n",
    "    print(slot, embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2],\n",
       "         [-1, -1],\n",
       "         [ 3,  4],\n",
       "         [ 1, -1]],\n",
       "\n",
       "        [[ 5, -1],\n",
       "         [ 6,  5],\n",
       "         [-1, -1],\n",
       "         [-1, -1]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_batch_param_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = final_batch_param_tensor\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 4],\n",
       "        [0, 0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_batch_param_tensor[:, 2].clamp(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[1, 0, 3, 1],\n",
      "        [5, 6, 0, 0]])\n",
      "1\n",
      "tensor([[2, 0, 4, 0],\n",
      "        [0, 5, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "params_emb = []\n",
    "for slot, embed in enumerate(layers_embeds):\n",
    "    print(slot)\n",
    "    param = final_batch_param_tensor[:, :, slot]\n",
    "    mask = (param == -1)\n",
    "    inp = param.clamp(0)\n",
    "    print(inp)\n",
    "    em = embed(inp)\n",
    "    em[mask] = nn.Parameter(torch.zeros(32))\n",
    "    params_emb.append(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 32]), torch.Size([2, 4, 32]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_emb[0].shape, params_emb[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 2, 32])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(params_emb, dim=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_emb_stacked = torch.stack(params_emb, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 32])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prim_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 32])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_emb_stacked[:, 0, :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_emb_stacked[:, 0, :, :].view(params_emb_stacked.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prim_vec[0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prim_vec[:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 32])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prim_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xy_embed_p0 = params_emb_stacked[:, 0, :, :] # (B, num_params, d)\n",
    "# prim_embed_p0 = prim_vec[:, 0, :]               # (B, d)\n",
    "torch.stack([torch.cat([prim_vec[:, i, :], params_emb_stacked[:, i, :, :].view(params_emb_stacked.shape[0], -1)], dim=-1) for i in range(prim_vec.shape[1])]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.cat([prim_vec[:, 0, :], params_emb_stacked[:, 0, :, :].view(params_emb_stacked.shape[0], -1)], dim=-1)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(params_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
