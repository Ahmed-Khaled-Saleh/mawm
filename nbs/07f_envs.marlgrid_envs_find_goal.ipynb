{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08bc68fb",
   "metadata": {},
   "source": [
    "# Findgoal Environment \n",
    "\n",
    "> Basic scenario where agents need to find and reach a goal in the grid world. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1603b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp envs.marl_grid.envs.findgoal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d54069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac0a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import *\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a8c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "\n",
    "from MAWM.envs.marl_grid.base import MultiGridEnv, MultiGrid\n",
    "from MAWM.envs.marl_grid.objects import Goal, Wall\n",
    "\n",
    "\n",
    "def dis_func(x, y, k=1):\n",
    "    return np.linalg.norm(x - y) / k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac67850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FindGoalMultiGrid(MultiGridEnv):\n",
    "    \"\"\"\n",
    "    A single cluttered room with a green goal at random position.\n",
    "    Each agent obtains a reward when reaching the goal.\n",
    "    All agents must be reach the goal to obtain a team reward.\n",
    "    \"\"\"\n",
    "    mission = 'get to the green square'\n",
    "    metadata = {}\n",
    "\n",
    "    def __init__(self, config):\n",
    "        n_clutter = config.get('n_clutter')\n",
    "        clutter_density = config.get('clutter_density')\n",
    "        randomize_goal = config.get('randomize_goal')\n",
    "\n",
    "        if (n_clutter is None) == (clutter_density is None):\n",
    "            raise ValueError('Must provide n_clutter or clutter_density.')\n",
    "\n",
    "        super().__init__(config)\n",
    "\n",
    "        if clutter_density is not None:\n",
    "            self.n_clutter = int(\n",
    "                clutter_density * (self.width - 2) * (self.height - 2))\n",
    "        else:\n",
    "            self.n_clutter = n_clutter\n",
    "\n",
    "        self.randomize_goal = randomize_goal\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        self.grid = MultiGrid((width, height))\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "        if getattr(self, 'randomize_goal', True):\n",
    "            goal_pos = self.place_obj(Goal(color='green', reward=1),\n",
    "                                      max_tries=100)\n",
    "        else:\n",
    "            goal_pos = np.asarray([width - 2, height - 2])\n",
    "            self.put_obj(Goal(color='green', reward=1), width - 2, height - 2)\n",
    "\n",
    "        for _ in range(getattr(self, 'n_clutter', 0)):\n",
    "            self.place_obj(Wall(), max_tries=100)\n",
    "\n",
    "        return goal_pos\n",
    "\n",
    "    def gen_global_obs(self, agent_done=None):\n",
    "        if agent_done is None:\n",
    "            # an integer array storing agent's done info\n",
    "            agent_done = np.zeros((len(self.agents, )), dtype=float)\n",
    "        self.sees_goal = np.array([self.agents[i].in_view(\n",
    "                self.goal_pos[0], self.goal_pos[1]) for i in range(\n",
    "                self.num_agents)]) * 1\n",
    "\n",
    "        obs = {\n",
    "            'adv_indices': self.adv_indices,\n",
    "            'agent_done': agent_done,  # (N,)\n",
    "            'goal_pos': self.goal_pos,  # (2,)\n",
    "            'sees_goal': self.sees_goal,  # (N,)\n",
    "            'pos': np.stack([self.get_agent_pos(a) for a in self.agents],\n",
    "                            axis=0),  # (N, 2)\n",
    "            'comm_act': np.stack([a.comm for a in self.agents],\n",
    "                                 axis=0),  # (N, comm_len)\n",
    "            'env_act': np.stack([a.env_act for a in self.agents],\n",
    "                                axis=0),  # (N, 1)\n",
    "        }\n",
    "        return obs\n",
    "\n",
    "    def reset(self, seed: int = None, options: dict = None):\n",
    "        obs_dict = MultiGridEnv.reset(self, seed=seed, options=options)\n",
    "\n",
    "        if self.num_adversaries < 0:\n",
    "            # need to count number of adversaries in the env\n",
    "            self.adv_indices = set()\n",
    "            for i, agent in enumerate(self.agents):\n",
    "                if agent.is_adversary:\n",
    "                    self.adv_indices.add(i)\n",
    "            self.num_adversaries = len(self.adv_indices)\n",
    "\n",
    "            obs_dict['global'] = self.gen_global_obs()\n",
    "            return obs_dict\n",
    "\n",
    "        else:\n",
    "            # randomize adv indices each episode\n",
    "            adv_indices = np.random.choice([i for i in range(self.num_agents)],\n",
    "                                           self.num_adversaries,\n",
    "                                           replace=False)\n",
    "            for i, agent in enumerate(self.agents):\n",
    "                if i in adv_indices:\n",
    "                    agent.is_adversary = True\n",
    "                else:\n",
    "                    agent.is_adversary = False\n",
    "            self.adv_indices = adv_indices\n",
    "\n",
    "            obs_dict['global'] = self.gen_global_obs()\n",
    "            return obs_dict\n",
    "\n",
    "    def _get_reward(self, rwd, agent_no):\n",
    "        step_rewards = np.zeros((len(self.agents, )), dtype=np.float)\n",
    "        env_rewards = np.zeros((len(self.agents, )), dtype=np.float)\n",
    "        if agent_no in self.adv_indices:\n",
    "            # agent can only receive rewards if it is non-adversarial\n",
    "            return env_rewards, step_rewards\n",
    "\n",
    "        env_rewards[agent_no] += rwd\n",
    "        if self.team_reward_type == 'share':\n",
    "            # assign zero-sum rewards to both teams\n",
    "            for agent_id in range(self.num_agents):\n",
    "                if agent_id not in self.adv_indices:\n",
    "\n",
    "                    step_rewards[agent_id] += rwd\n",
    "                    self.agents[agent_id].reward(rwd)\n",
    "                else:\n",
    "                    step_rewards[agent_id] -= rwd\n",
    "                    self.agents[agent_id].reward(-rwd)\n",
    "        else:\n",
    "            step_rewards[agent_no] += rwd\n",
    "            self.agents[agent_no].reward(rwd)\n",
    "        return env_rewards, step_rewards\n",
    "\n",
    "    def update_reward(self, step_rewards):\n",
    "        nonadv_done_n = []\n",
    "        adv_rew = 0.0\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            if i not in self.adv_indices:\n",
    "                # zero-sum reward between adversaries and non-adversaries\n",
    "                nonadv_done_n.append(agent.done)\n",
    "                adv_rew -= step_rewards[i]\n",
    "        nonadv_done = all(nonadv_done_n)\n",
    "\n",
    "        timeout = (self.step_count >= self.max_steps)\n",
    "\n",
    "        # normalized distance-to-goal to range [0, 1]\n",
    "        ndis_to_goal = [dis_func(agent.pos, self.goal_pos, k=self.max_dis)\n",
    "                        for agent in self.agents]\n",
    "\n",
    "        if self.team_reward_type == 'const':\n",
    "            # give constant team reward to non-adversaries\n",
    "            if nonadv_done:\n",
    "                team_rwd = self.team_reward_multiplier\n",
    "                for i, a in enumerate(self.agents):\n",
    "                    if i not in self.adv_indices:\n",
    "                        a.reward(team_rwd)\n",
    "                        step_rewards[i] += team_rwd\n",
    "\n",
    "                        # keep zero-sum reward between\n",
    "                        # adversaries and non-adversaries\n",
    "                        adv_rew -= team_rwd\n",
    "        else:\n",
    "            # no team reward\n",
    "            pass\n",
    "\n",
    "        if len(self.adv_indices) > 0:\n",
    "            adv_rew /= len(self.adv_indices)\n",
    "            for i in self.adv_indices:\n",
    "                step_rewards[i] += adv_rew\n",
    "        return timeout, nonadv_done, step_rewards, ndis_to_goal\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        obs_dict, rew_dict, _, info_dict = MultiGridEnv.step(self, action_dict)\n",
    "        if self.active_after_done:\n",
    "            done_n = [agent.at_pos(self.goal_pos) for agent in self.agents]\n",
    "        else:\n",
    "            done_n = [agent.done for agent in self.agents]\n",
    "\n",
    "        step_rewards = rew_dict['step_rewards']\n",
    "        env_rewards = rew_dict['env_rewards']\n",
    "        comm_rewards = rew_dict['comm_rewards']\n",
    "        comm_strs = info_dict['comm_strs']\n",
    "\n",
    "        timeout, nonadv_done, step_rewards, ndis_to_goal = self.update_reward(\n",
    "            step_rewards)\n",
    "\n",
    "        # The episode overall is done if ALL non-adversarial agents are done,\n",
    "        # or if it exceeds the step limit.\n",
    "        done = timeout or nonadv_done\n",
    "        if self.debug:\n",
    "            done = any(done_n)\n",
    "\n",
    "        step_rewards += comm_rewards\n",
    "\n",
    "        rew_dict = {f'agent_{i}': step_rewards[i] for i in range(\n",
    "            len(step_rewards))}\n",
    "        done_dict = {'__all__': done}\n",
    "        info_dict = {f'agent_{i}': {\n",
    "            'done': done_n[i],\n",
    "            'comm': self.agents[i].comm,\n",
    "            'nonadv_done': nonadv_done,\n",
    "            'posd': np.array([self.agents[i].pos[0], self.agents[i].pos[1],\n",
    "                              done_n[i]]),\n",
    "            'sees_goal': self.sees_goal[i],\n",
    "            'comm_str': comm_strs[i],\n",
    "        } for i in range(len(done_n))}\n",
    "\n",
    "        info_dict['rew_by_act'] = {\n",
    "            # env reward\n",
    "            0: {f'agent_{i}': env_rewards[i] for i in range(len(env_rewards))},\n",
    "\n",
    "            # designed comm reward\n",
    "            'comm': {f'agent_{i}': comm_rewards[i] for i in range(len(\n",
    "                comm_rewards))},\n",
    "        }\n",
    "\n",
    "        # team reward\n",
    "        if self.separate_rew_more:\n",
    "            info_dict['rew_by_act'][1] = {f'agent_{i}': (\n",
    "                    step_rewards[i] - env_rewards[i]) for i in range(\n",
    "                len(step_rewards))}\n",
    "        else:\n",
    "            info_dict['rew_by_act'][1] = {f'agent_{i}': (\n",
    "                step_rewards[i]) for i in range(len(step_rewards))}\n",
    "\n",
    "        obs_dict['global'] = self.gen_global_obs()\n",
    "        return obs_dict, rew_dict, done_dict, info_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309ce416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
