{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2864b51",
   "metadata": {},
   "source": [
    "# Vision encoder\n",
    "\n",
    "> a ConvNet module for percpetion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ae85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.vision.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87425da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcab1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import *\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f98f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from torch import nn\n",
    "from mawm.models.vision.enums import BackboneOutput\n",
    "\n",
    "\n",
    "class SequenceBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        collapse T and BS dimensions prior to passing to backbone\n",
    "        afterwards reshape to original shape\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_position_dim = 0\n",
    "\n",
    "    def _remove_pos_component_for_spatial(self, embeddings):\n",
    "        \"\"\"\n",
    "        remove the position component from spatial embeddings\n",
    "\n",
    "        Input:\n",
    "            embeddings: tensor\n",
    "            (T, BS, Ch, W, H) or\n",
    "            (BS, Ch, W, H) or\n",
    "            (T, BS, H) or\n",
    "            (BS, H)\n",
    "        \"\"\"\n",
    "        og_shape = tuple(embeddings.shape)\n",
    "        flattened_input = len(og_shape) < 4\n",
    "\n",
    "        # first reshape to spatial dimension if needed\n",
    "        if flattened_input:\n",
    "            spatial_shape = (*embeddings.shape[:-1], *self.output_dim)\n",
    "            embeddings = embeddings.view(spatial_shape)\n",
    "\n",
    "        position_channels = self.output_position_dim[0]\n",
    "\n",
    "        # remove the position dimensions\n",
    "        if len(embeddings.shape) == 5:\n",
    "            embeddings = embeddings[:, :, :-position_channels]\n",
    "        elif len(embeddings.shape) == 4:\n",
    "            embeddings = embeddings[:, :-position_channels]\n",
    "\n",
    "        # reflatten tensor if needed\n",
    "        if flattened_input:\n",
    "            embeddings = embeddings.view(*og_shape[:-1], -1)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def remove_pos_component(self, embeddings):\n",
    "        \"\"\"\n",
    "        remove the position component from embeddings\n",
    "        Input:\n",
    "            embeddings: tensor\n",
    "            (T, BS, Ch, W, H) or\n",
    "            (BS, Ch, W, H) or\n",
    "            (T, BS, H) or\n",
    "            (BS, H)\n",
    "        \"\"\"\n",
    "        if not self.output_position_dim:\n",
    "            return embeddings\n",
    "\n",
    "        if isinstance(self.output_dim, int):\n",
    "            return embeddings[..., : -self.output_position_dim]\n",
    "        else:\n",
    "            return self._remove_pos_component_for_spatial(embeddings)\n",
    "\n",
    "    def forward_multiple(self, x, position=None):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            x: [T, BS, *] or [BS, T, *]\n",
    "        output:\n",
    "            x: [T, BS, *] or [T, BS, *]\n",
    "        \"\"\"\n",
    "\n",
    "        # if no time dimension, just feed it directly to backbone\n",
    "        if x.dim() == 2 or x.dim() == 4:\n",
    "            if position is not None:\n",
    "                output = self.forward(x, position)\n",
    "            else:\n",
    "                output = self.forward(x)\n",
    "            return output\n",
    "\n",
    "        state = x.flatten(0, 1)\n",
    "        if position is not None:\n",
    "            position = position.flatten(0, 1)\n",
    "            output = self.forward(state, position)\n",
    "            \n",
    "        else:\n",
    "            output = self.forward(state)\n",
    "\n",
    "        state = output.encodings\n",
    "        new_shape = x.shape[:2] + state.shape[1:]\n",
    "        state = state.reshape(new_shape)\n",
    "\n",
    "        \n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a568f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
