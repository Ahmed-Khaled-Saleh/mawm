{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e97c73",
   "metadata": {},
   "source": [
    "# LLama-2 Transformer\n",
    "\n",
    "> Geenral architecture for transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f81cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f61d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    dim: int = 4096  # Dimension of the model\n",
    "    n_layers: int = 32  # Number of layers in the transformer\n",
    "    n_heads: int = 32  # Number of attention heads\n",
    "    n_kv_heads: Optional[int] = n_heads  # Number of key-value heads (optional, defaults to n_heads)\n",
    "    vocab_size: int = 50257  # Vocabulary size\n",
    "    norm_eps: float = 1e-5  # Epsilon value for normalization\n",
    "\n",
    "    max_batch_size: int = 32  # Maximum batch size for training\n",
    "    max_seq_len: int = 2048  # Maximum sequence length\n",
    "\n",
    "    device: str = None  # Device to run the model on (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4209c402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RotaryPositionEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, head_dim: int, seq_len: int, device: str) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = head_dim\n",
    "        assert self.dim % 2 == 0, \"head_dim must be divisible by 2\"\n",
    "\n",
    "        # Calculate the rotation frequencies for positional embeddings\n",
    "        theta_numerator = torch.arange(0, self.dim, 2, dtype=torch.float32)\n",
    "        theta = 1.0 / torch.pow(10000, theta_numerator / self.dim).to(device)\n",
    "\n",
    "        # Generate frequency values for positional embeddings\n",
    "        m = torch.arange(seq_len, dtype=torch.float32).to(device)\n",
    "        freqs = torch.outer(m, theta).float()\n",
    "\n",
    "        # Convert frequency values to complex numbers (polar form)\n",
    "        self.freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "        # self.register_buffer(\"freqs_complex\", self.freqs_complex)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int) -> torch.Tensor:\n",
    "        batch_size, seq_len, dim = x.shape\n",
    "        assert dim == self.dim, \"dim must be equal to self.dim\"\n",
    "\n",
    "        # Reshape the input into a complex tensor for rotational operations\n",
    "        # (B, SeqLen, H, Head_Dim) -> (B, SeqLen, H, Head_Dim // 2)\n",
    "        x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "\n",
    "        # Extract rotational frequencies for the given sequence length and start position\n",
    "        # (SeqLen, Head_Dim // 2) -> (1, SeqLen, 1, Head_Dim // 2)\n",
    "        freq_complex = self.freqs_complex[start_pos:start_pos + seq_len]\n",
    "        freq_complex = freq_complex.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "        # Apply rotational transformation to the input using frequency values\n",
    "        # (B, SeqLen, H, Head_Dim // 2) * (1, SeqLen, 1, Head_Dim // 2) -> (B, SeqLen, H, Head_Dim // 2)\n",
    "        x_rotated = x_complex * freq_complex\n",
    "\n",
    "        # Convert the rotated complex tensor back to real-valued tensor\n",
    "        # (B, SeqLen, H, Head_Dim // 2) -> (B, SeqLen, H , Head_Dim // 2, 2)\n",
    "        x_out = torch.view_as_real(x_rotated)\n",
    "\n",
    "        # Reshape to match the original input shape\n",
    "        # (B, SeqLen, H , Head_Dim // 2, 2) -> (B, SeqLen, H, Head_Dim)\n",
    "        x_out = x_out.reshape(*x.shape)\n",
    "\n",
    "        return x_out.type_as(x).to(x.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ba76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RMSNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, dim: int, eps: float) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps  # Epsilon value for numerical stability\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))  # Learnable parameter for scaling\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: Input tensor of shape (Batch_Size, SeqLen, Dim)\n",
    "\n",
    "        # Calculate the root-mean-square norm along the last dimension\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "\n",
    "        # Normalize the input by dividing by the root-mean-square norm and scale with gamma\n",
    "        normalized_x = (x / rms) * self.gamma\n",
    "\n",
    "        return normalized_x  # Return the normalized tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a11c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, args: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        # Determine the number of key-value heads (defaults to n_heads if not specified)\n",
    "        self.n_kv_heads = args.n_kv_heads if args.n_kv_heads is not None else args.n_heads\n",
    "\n",
    "        # Set the number of query heads and the number of repetitions for K and V\n",
    "        self.n_heads_q = args.n_heads\n",
    "        self.n_rep = self.n_heads_q // self.n_kv_heads\n",
    "\n",
    "        # Calculate the head dimension\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        # Linear transformations for queries, keys, values, and output\n",
    "        self.Wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.Wk = nn.Linear(args.dim, args.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.Wv = nn.Linear(args.dim, args.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.Wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "\n",
    "        # Initialize key and value caches with zeros\n",
    "        self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, args.n_kv_heads, self.head_dim))\n",
    "        self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, args.n_kv_heads, self.head_dim))\n",
    "\n",
    "        # Rotary Position Embedding\n",
    "        self.rope = RotaryPositionEmbedding(self.head_dim, args.max_seq_len, args.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def repeat_heads(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "\n",
    "        # Repeat the heads of K and V to match the number of heads in Q\n",
    "\n",
    "        batch_size, seq_len, n_kv_heads, head_dim = x.shape\n",
    "        if n_rep == 1:\n",
    "            return x\n",
    "        else:\n",
    "            return (x[:, :, :, None, :]\n",
    "                    .expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim)\n",
    "                    .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)\n",
    "                    )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int) -> torch.Tensor:\n",
    "        batch_size, seq_len, dim = x.shape  # (B, 1, dim)\n",
    "        assert dim == self.dim, \"dim must be equal to self.dim\"\n",
    "\n",
    "        # (B, 1, dim) -> (B, 1, n_heads_q * head_dim)\n",
    "        xq = self.Wq(x)\n",
    "\n",
    "        # (B, 1, dim) -> (B, 1, n_kv_heads * head_dim)\n",
    "        xk = self.Wk(x)\n",
    "\n",
    "        # (B, 1, dim) -> (B, 1, n_kv_heads * head_dim)\n",
    "        xv = self.Wv(x)\n",
    "\n",
    "        # (B, 1, n_heads_q * head_dim) -> (B, 1, n_heads_q, head_dim)\n",
    "        xq = xq.view(batch_size, seq_len, self.n_heads_q, self.head_dim)\n",
    "\n",
    "        # (B, 1, n_kv_heads * head_dim) -> (B, 1, n_kv_heads, head_dim)\n",
    "        xk = xk.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        xq = self.rope(xq, start_pos)\n",
    "        xk = self.rope(xk, start_pos)\n",
    "\n",
    "        # Update key and value caches\n",
    "        self.cache_k[:batch_size, start_pos:start_pos + seq_len] = xk\n",
    "        self.cache_v[:batch_size, start_pos:start_pos + seq_len] = xv\n",
    "\n",
    "        # Retrieve key and value caches\n",
    "        keys = self.cache_k[:batch_size, :start_pos + seq_len]\n",
    "        values = self.cache_v[:batch_size, :start_pos + seq_len]\n",
    "\n",
    "        # Repeat the heads of K and V to match the number of heads in Q\n",
    "        keys = self.repeat_heads(keys, self.n_rep)\n",
    "        values = self.repeat_heads(values, self.n_rep)\n",
    "\n",
    "        # (B, 1, n_heads_q, head_dim) -> (B, n_heads_q, 1, head_dim)\n",
    "        xq = xq.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # (B, n_heads_q, 1, head_dim) * (B, n_heads_q, head_dim, SeqLen) -> (B, n_heads_q, 1, SeqLen)\n",
    "        scores = torch.matmul(xq, keys.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "\n",
    "        # (B, n_heads_q, 1, SeqLen) * (B, n_heads_q, SeqLen, head_dim) -> (B, n_heads_q, 1, head_dim)\n",
    "        context = torch.matmul(scores, values)\n",
    "\n",
    "        # (B, n_heads_q, 1, head_dim) -> (B, 1, head_dim)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "\n",
    "        # (B, 1, head_dim) -> (B, 1, dim)\n",
    "        output = self.Wo(context)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c291f398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, args: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        # Calculate the hidden dimension based on the provided parameters\n",
    "        hidden_dim = 4 * args.dim\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "\n",
    "        # Adjust the hidden dimension based on ffn_dim_multiplier (if provided)\n",
    "        if args.ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(args.ffn_dim_multiplier * hidden_dim)\n",
    "\n",
    "        # Ensure hidden_dim is a multiple of args.multiple_of\n",
    "        hidden_dim = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)\n",
    "\n",
    "        # Define linear layers for the feedforward network\n",
    "        self.fc1 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "        self.fc2 = nn.Linear(hidden_dim, args.dim, bias=False)\n",
    "        self.fc3 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Input shape: (Batch_Size, SeqLen, Dim)\n",
    "\n",
    "        # Apply the first linear transformation and activation (swish)\n",
    "        swish = F.silu(self.fc1(x))\n",
    "\n",
    "        # Apply the second linear transformation\n",
    "        x_V = self.fc3(swish)\n",
    "\n",
    "        # Element-wise multiplication\n",
    "        x = swish * x_V\n",
    "\n",
    "        # Apply the third linear transformation\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x  # Return the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd7e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, args: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.attention = SelfAttention(args)\n",
    "        self.feed_forward = FeedForward(args)\n",
    "\n",
    "        self.norm1 = RMSNorm(args.dim, args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, args.norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int) -> torch.Tensor:\n",
    "        h = x + self.attention(self.norm1(x), start_pos)\n",
    "        out = h + self.feed_forward(self.ffn_norm(h))\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd43fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, args: ModelConfig) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Check if vocab_size is specified\n",
    "        assert args.vocab_size != -1, \"vocab_size must be specified\"\n",
    "\n",
    "        # Store model configuration and necessary parameters\n",
    "        self.args = args\n",
    "        self.vocab_size = args.vocab_size\n",
    "        self.n_layers = args.n_layers\n",
    "\n",
    "        # Embedding layer for token embeddings\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, args.dim)\n",
    "\n",
    "        # Create a list of transformer encoder blocks\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(args.n_layers):\n",
    "            self.layers.append(EncoderBlock(args))\n",
    "\n",
    "        # Layer normalization for the output\n",
    "        self.norm = RMSNorm(args.dim, args.norm_eps)\n",
    "\n",
    "        # Output linear layer\n",
    "        self.output = nn.Linear(args.dim, self.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int) -> torch.Tensor:\n",
    "        # Input shape: (Batch_Size, SeqLen)\n",
    "\n",
    "        # Ensure seq_len is 1\n",
    "        assert x.shape[1] == 1, \"seq_len must be 1\"\n",
    "\n",
    "        # Embedding lookup\n",
    "        x = self.embeddings(x)\n",
    "\n",
    "        # Pass through each transformer encoder block\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, start_pos)\n",
    "\n",
    "        # Layer normalization\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Output prediction\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x  # Return the output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8dace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# model_config = ModelConfig()\n",
    "# model = Transformer(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff8dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
