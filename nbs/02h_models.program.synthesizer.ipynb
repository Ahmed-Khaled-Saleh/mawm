{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e97c73",
   "metadata": {},
   "source": [
    "# LLama-2 Transformer\n",
    "\n",
    "> Geenral architecture for transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f81cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.program.synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59cb2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import *\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce73bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mawm.models.base.dense import DenseModel\n",
    "\n",
    "\n",
    "class Proposer(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoregressive Synthesizer that generates (primitive, params)\n",
    "    conditioned on observation latent z and previous program token.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dim,\n",
    "        num_prims,\n",
    "        max_params,\n",
    "        seq_len,\n",
    "        prog_emb_dim_x=64,\n",
    "        prog_emb_dim_y=64,\n",
    "        prog_emb_dim_prims=32,\n",
    "        model_info={ 'layers': 3,'node_size': 128,'activation': nn.ReLU,'dist': None}\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # observation -> hidden\n",
    "        self.obs_proj = nn.Linear(obs_dim, model_info['node_size'])\n",
    "        self.prog_embed = nn.Linear(seq_len * (prog_emb_dim_prims + prog_emb_dim_x + prog_emb_dim_y), model_info['node_size'])\n",
    "\n",
    "        # combine obs + prev token\n",
    "        self.fuse = nn.Linear(model_info['node_size'] + model_info['node_size'],\n",
    "                              model_info['node_size'])\n",
    "        self.proposer_mlp = DenseModel(output_shape= (model_info['node_size'],), input_size=model_info['node_size'], info= model_info)\n",
    "\n",
    "        # heads\n",
    "        self.prim_head = nn.Linear(model_info['node_size'], num_prims+ 1)  # +1 for EOS\n",
    "        self.param_head = nn.Linear(model_info['node_size'], max_params)\n",
    "        self.max_params = max_params\n",
    "        self.num_prims = num_prims\n",
    "        self.sos_idx = -1 #num_prims\n",
    "\n",
    "    def forward_step(self, z, p_embed):\n",
    "        \"\"\"\n",
    "        z:              (B, obs_dim)\n",
    "        p_embed:        (B, L, prog_embed_dim) - where prog_embed_dim = prog_emb_dim_prims + prog_emb_dim_x + prog_emb_dim_y\n",
    "        \"\"\"\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        z_h = F.relu(self.obs_proj(z))      # z_h shape: (B, H)\n",
    "        \n",
    "        B, L, dim = p_embed.shape\n",
    "        program = p_embed.view(B, L*dim)  # program shape: (B, L * prog_embed_dim)\n",
    "        program = F.relu(self.prog_embed(program)) # program shape: (B, L, H)\n",
    "\n",
    "        h = torch.cat([z_h, program], dim=-1)\n",
    "        \n",
    "        h = F.relu(self.fuse(h))\n",
    "        h = self.proposer_mlp(h)\n",
    "\n",
    "        prim_logits = self.prim_head(h)             \n",
    "        param_logits = self.param_head(h)           \n",
    "        param_probs = torch.sigmoid(param_logits)   \n",
    "\n",
    "        return prim_logits, param_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa2d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from mawm.core import Program, PRIMITIVE_TEMPLATES\n",
    "\n",
    "p = Program(tokens= [(0, [1.0, 2.0]), (4, []), (1, [3.0, 4.0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc7c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from mawm.models.program.embedder import batchify_programs, get_indices\n",
    "from mawm.core import Program, PRIMITIVE_TEMPLATES\n",
    "batch_programs = [\n",
    "    Program(tokens= [(0, [1.0, 2.0]), (4, []), (1, [3.0, 4.0]),(5, [1]) ]), # L=4\n",
    "    Program(tokens= [(2, [5.0]), (3, [6.0, 5.0])]) # L=2\n",
    "]\n",
    "\n",
    "\n",
    "prim_ids, params_ids = batchify_programs(batch_programs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da224208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  1,  5],\n",
       "        [ 2,  3, -1, -1]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prim_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa444f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "from mawm.models.program.embedder import ProgramEmbedder\n",
    "num_primitives = len(PRIMITIVE_TEMPLATES)\n",
    "p_embed = ProgramEmbedder(\n",
    "    num_primitives= num_primitives,\n",
    "    param_cardinalities= [7, 7],\n",
    "    max_params_per_primitive= 2,\n",
    "    d_name= 32,\n",
    "    d_param= 32,\n",
    ")\n",
    "p_vec = p_embed(prim_ids, params_ids)\n",
    "p_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5fd9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer= Proposer(\n",
    "    obs_dim= 32,\n",
    "    num_prims= num_primitives,\n",
    "    max_params= 2,\n",
    "    seq_len= 4,\n",
    "    prog_emb_dim_x= 32,\n",
    "    prog_emb_dim_y= 32,\n",
    "    prog_emb_dim_prims= 32,\n",
    ")\n",
    "a, b = synthesizer.forward_step(\n",
    "    z= torch.randn(2, 32),\n",
    "    p_embed= p_vec,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d0f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6]), torch.Size([2, 2]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11d684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0553, -0.0427, -0.2560,  0.0010,  0.0799,  0.0142],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb88d30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5361, 0.4981],\n",
       "        [0.5012, 0.5060]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd9cf5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0973, 0.1896, 0.0967, 0.2142, 0.1517, 0.2506]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_with_temperature = lambda logits, temp: F.softmax(logits / temp, dim=-1)\n",
    "softmax_with_temp = softmax_with_temperature(a, temp=0.2)\n",
    "softmax_with_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc952a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1512, 0.1728, 0.1510, 0.1771, 0.1653, 0.1827]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "topk = 2\n",
    "probs = F.softmax(a, dim=-1)\n",
    "print(probs)\n",
    "top_vals, top_idx = torch.topk(probs, k=min(topk, probs.size(0)), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a24849c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1827]], grad_fn=<TopkBackward0>), tensor([[5]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_vals, top_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6dea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4800, 0.5000]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a7c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdf9b43a",
   "metadata": {},
   "source": [
    "## Old trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f61d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    dim: int = 4096  # Dimension of the model\n",
    "    n_layers: int = 32  # Number of layers in the transformer\n",
    "    n_heads: int = 32  # Number of attention heads\n",
    "    n_kv_heads: Optional[int] = n_heads  # Number of key-value heads (optional, defaults to n_heads)\n",
    "    vocab_size: int = 50257  # Vocabulary size\n",
    "    norm_eps: float = 1e-5  # Epsilon value for normalization\n",
    "\n",
    "    max_batch_size: int = 32  # Maximum batch size for training\n",
    "    max_seq_len: int = 2048  # Maximum sequence length\n",
    "\n",
    "    device: str = None  # Device to run the model on (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4209c402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RotaryPositionEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, head_dim: int, seq_len: int, device: str) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = head_dim\n",
    "        assert self.dim % 2 == 0, \"head_dim must be divisible by 2\"\n",
    "\n",
    "        # Calculate the rotation frequencies for positional embeddings\n",
    "        theta_numerator = torch.arange(0, self.dim, 2, dtype=torch.float32)\n",
    "        theta = 1.0 / torch.pow(10000, theta_numerator / self.dim).to(device)\n",
    "\n",
    "        # Generate frequency values for positional embeddings\n",
    "        m = torch.arange(seq_len, dtype=torch.float32).to(device)\n",
    "        freqs = torch.outer(m, theta).float()\n",
    "\n",
    "        # Convert frequency values to complex numbers (polar form)\n",
    "        self.freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "        # self.register_buffer(\"freqs_complex\", self.freqs_complex)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int) -> torch.Tensor:\n",
    "        batch_size, seq_len, dim = x.shape\n",
    "        assert dim == self.dim, \"dim must be equal to self.dim\"\n",
    "\n",
    "        # Reshape the input into a complex tensor for rotational operations\n",
    "        # (B, SeqLen, H, Head_Dim) -> (B, SeqLen, H, Head_Dim // 2)\n",
    "        x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "\n",
    "        # Extract rotational frequencies for the given sequence length and start position\n",
    "        # (SeqLen, Head_Dim // 2) -> (1, SeqLen, 1, Head_Dim // 2)\n",
    "        freq_complex = self.freqs_complex[start_pos:start_pos + seq_len]\n",
    "        freq_complex = freq_complex.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "        # Apply rotational transformation to the input using frequency values\n",
    "        # (B, SeqLen, H, Head_Dim // 2) * (1, SeqLen, 1, Head_Dim // 2) -> (B, SeqLen, H, Head_Dim // 2)\n",
    "        x_rotated = x_complex * freq_complex\n",
    "\n",
    "        # Convert the rotated complex tensor back to real-valued tensor\n",
    "        # (B, SeqLen, H, Head_Dim // 2) -> (B, SeqLen, H , Head_Dim // 2, 2)\n",
    "        x_out = torch.view_as_real(x_rotated)\n",
    "\n",
    "        # Reshape to match the original input shape\n",
    "        # (B, SeqLen, H , Head_Dim // 2, 2) -> (B, SeqLen, H, Head_Dim)\n",
    "        x_out = x_out.reshape(*x.shape)\n",
    "\n",
    "        return x_out.type_as(x).to(x.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ba76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RMSNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, dim: int, eps: float) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps  # Epsilon value for numerical stability\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))  # Learnable parameter for scaling\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: Input tensor of shape (Batch_Size, SeqLen, Dim)\n",
    "\n",
    "        # Calculate the root-mean-square norm along the last dimension\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "\n",
    "        # Normalize the input by dividing by the root-mean-square norm and scale with gamma\n",
    "        normalized_x = (x / rms) * self.gamma\n",
    "\n",
    "        return normalized_x  # Return the normalized tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a11c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, args: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        # Determine the number of key-value heads (defaults to n_heads if not specified)\n",
    "        self.n_kv_heads = args.n_kv_heads if args.n_kv_heads is not None else args.n_heads\n",
    "\n",
    "        # Set the number of query heads and the number of repetitions for K and V\n",
    "        self.n_heads_q = args.n_heads\n",
    "        self.n_rep = self.n_heads_q // self.n_kv_heads\n",
    "\n",
    "        # Calculate the head dimension\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        # Linear transformations for queries, keys, values, and output\n",
    "        self.Wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.Wk = nn.Linear(args.dim, args.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.Wv = nn.Linear(args.dim, args.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.Wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "\n",
    "        # Initialize key and value caches with zeros\n",
    "        self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, args.n_kv_heads, self.head_dim))\n",
    "        self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, args.n_kv_heads, self.head_dim))\n",
    "\n",
    "        # Rotary Position Embedding\n",
    "        self.rope = RotaryPositionEmbedding(self.head_dim, args.max_seq_len, args.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def repeat_heads(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "\n",
    "        # Repeat the heads of K and V to match the number of heads in Q\n",
    "\n",
    "        batch_size, seq_len, n_kv_heads, head_dim = x.shape\n",
    "        if n_rep == 1:\n",
    "            return x\n",
    "        else:\n",
    "            return (x[:, :, :, None, :]\n",
    "                    .expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim)\n",
    "                    .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)\n",
    "                    )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int) -> torch.Tensor:\n",
    "        batch_size, seq_len, dim = x.shape  # (B, 1, dim)\n",
    "        assert dim == self.dim, \"dim must be equal to self.dim\"\n",
    "\n",
    "        # (B, 1, dim) -> (B, 1, n_heads_q * head_dim)\n",
    "        xq = self.Wq(x)\n",
    "\n",
    "        # (B, 1, dim) -> (B, 1, n_kv_heads * head_dim)\n",
    "        xk = self.Wk(x)\n",
    "\n",
    "        # (B, 1, dim) -> (B, 1, n_kv_heads * head_dim)\n",
    "        xv = self.Wv(x)\n",
    "\n",
    "        # (B, 1, n_heads_q * head_dim) -> (B, 1, n_heads_q, head_dim)\n",
    "        xq = xq.view(batch_size, seq_len, self.n_heads_q, self.head_dim)\n",
    "\n",
    "        # (B, 1, n_kv_heads * head_dim) -> (B, 1, n_kv_heads, head_dim)\n",
    "        xk = xk.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        xq = self.rope(xq, start_pos)\n",
    "        xk = self.rope(xk, start_pos)\n",
    "\n",
    "        # Update key and value caches\n",
    "        self.cache_k[:batch_size, start_pos:start_pos + seq_len] = xk\n",
    "        self.cache_v[:batch_size, start_pos:start_pos + seq_len] = xv\n",
    "\n",
    "        # Retrieve key and value caches\n",
    "        keys = self.cache_k[:batch_size, :start_pos + seq_len]\n",
    "        values = self.cache_v[:batch_size, :start_pos + seq_len]\n",
    "\n",
    "        # Repeat the heads of K and V to match the number of heads in Q\n",
    "        keys = self.repeat_heads(keys, self.n_rep)\n",
    "        values = self.repeat_heads(values, self.n_rep)\n",
    "\n",
    "        # (B, 1, n_heads_q, head_dim) -> (B, n_heads_q, 1, head_dim)\n",
    "        xq = xq.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # (B, n_heads_q, 1, head_dim) * (B, n_heads_q, head_dim, SeqLen) -> (B, n_heads_q, 1, SeqLen)\n",
    "        scores = torch.matmul(xq, keys.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "\n",
    "        # (B, n_heads_q, 1, SeqLen) * (B, n_heads_q, SeqLen, head_dim) -> (B, n_heads_q, 1, head_dim)\n",
    "        context = torch.matmul(scores, values)\n",
    "\n",
    "        # (B, n_heads_q, 1, head_dim) -> (B, 1, head_dim)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "\n",
    "        # (B, 1, head_dim) -> (B, 1, dim)\n",
    "        output = self.Wo(context)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c291f398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, args: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        # Calculate the hidden dimension based on the provided parameters\n",
    "        hidden_dim = 4 * args.dim\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "\n",
    "        # Adjust the hidden dimension based on ffn_dim_multiplier (if provided)\n",
    "        if args.ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(args.ffn_dim_multiplier * hidden_dim)\n",
    "\n",
    "        # Ensure hidden_dim is a multiple of args.multiple_of\n",
    "        hidden_dim = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)\n",
    "\n",
    "        # Define linear layers for the feedforward network\n",
    "        self.fc1 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "        self.fc2 = nn.Linear(hidden_dim, args.dim, bias=False)\n",
    "        self.fc3 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Input shape: (Batch_Size, SeqLen, Dim)\n",
    "\n",
    "        # Apply the first linear transformation and activation (swish)\n",
    "        swish = F.silu(self.fc1(x))\n",
    "\n",
    "        # Apply the second linear transformation\n",
    "        x_V = self.fc3(swish)\n",
    "\n",
    "        # Element-wise multiplication\n",
    "        x = swish * x_V\n",
    "\n",
    "        # Apply the third linear transformation\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x  # Return the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd7e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, args: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.attention = SelfAttention(args)\n",
    "        self.feed_forward = FeedForward(args)\n",
    "\n",
    "        self.norm1 = RMSNorm(args.dim, args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, args.norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int) -> torch.Tensor:\n",
    "        h = x + self.attention(self.norm1(x), start_pos)\n",
    "        out = h + self.feed_forward(self.ffn_norm(h))\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd43fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, args: ModelConfig) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Check if vocab_size is specified\n",
    "        assert args.vocab_size != -1, \"vocab_size must be specified\"\n",
    "\n",
    "        # Store model configuration and necessary parameters\n",
    "        self.args = args\n",
    "        self.vocab_size = args.vocab_size\n",
    "        self.n_layers = args.n_layers\n",
    "\n",
    "        # Embedding layer for token embeddings\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, args.dim)\n",
    "\n",
    "        # Create a list of transformer encoder blocks\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(args.n_layers):\n",
    "            self.layers.append(EncoderBlock(args))\n",
    "\n",
    "        # Layer normalization for the output\n",
    "        self.norm = RMSNorm(args.dim, args.norm_eps)\n",
    "\n",
    "        # Output linear layer\n",
    "        self.output = nn.Linear(args.dim, self.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int) -> torch.Tensor:\n",
    "        # Input shape: (Batch_Size, SeqLen)\n",
    "\n",
    "        # Ensure seq_len is 1\n",
    "        assert x.shape[1] == 1, \"seq_len must be 1\"\n",
    "\n",
    "        # Embedding lookup\n",
    "        x = self.embeddings(x)\n",
    "\n",
    "        # Pass through each transformer encoder block\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, start_pos)\n",
    "\n",
    "        # Layer normalization\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Output prediction\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x  # Return the output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8dace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# model_config = ModelConfig()\n",
    "# model = Transformer(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff8dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
