{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictors\n",
    "\n",
    "> Different architectures for predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import *\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "import numpy as np\n",
    "\n",
    "from mawm.models.misc import build_mlp\n",
    "from mawm.models.utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN predicotrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# class RNNPredictor(torch.nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         hidden_size: int = 512,\n",
    "#         num_layers: int = 1,\n",
    "#         action_dim: int = 2,\n",
    "#         z_dim: Optional[int] = None,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.z_dim = z_dim\n",
    "\n",
    "#         input_size = action_dim\n",
    "#         if z_dim is not None:\n",
    "#             input_size += z_dim\n",
    "\n",
    "#         self.rnn = torch.nn.GRU(\n",
    "#             input_size=input_size,  # action + optionally z_dim\n",
    "#             hidden_size=self.hidden_size,\n",
    "#             num_layers=num_layers,\n",
    "#         )\n",
    "\n",
    "#     def burn_in(self, *args, **kwargs):\n",
    "#         return None\n",
    "\n",
    "#     def predict_sequence(\n",
    "#         self,\n",
    "#         enc: torch.Tensor,\n",
    "#         h: torch.Tensor,\n",
    "#         actions: torch.Tensor,\n",
    "#         zs: Optional[torch.Tensor] = None,\n",
    "#     ):\n",
    "#         # in this version, encoding is directly used as h, and the passed h is ignored.\n",
    "#         # since h is obtained from burn_in, it's actually None.\n",
    "#         h = enc\n",
    "#         if zs is None:\n",
    "#             inputs = actions\n",
    "#         else:\n",
    "#             inputs = torch.cat([actions, zs], dim=-1)\n",
    "#         return self.rnn(inputs, h.unsqueeze(0).repeat(self.num_layers, 1, 1))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# class RNNPredictorV3(torch.nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         state_size: int = 10,\n",
    "#         hidden_size: int = 512,\n",
    "#         input_size: int = 2,\n",
    "#         arch: str = \"\",\n",
    "#     ):\n",
    "#         # state size is mapped with mlp to hidden size.\n",
    "#         super().__init__()\n",
    "#         self.state_size = state_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.input_size = input_size\n",
    "#         if arch != \"\":\n",
    "#             layer_dims = (\n",
    "#                 [state_size] + list(map(int, arch.split(\"-\"))) + [self.hidden_size]\n",
    "#             )\n",
    "#         else:\n",
    "#             layer_dims = [state_size, self.hidden_size]\n",
    "#         self.input_mlp = build_mlp(layer_dims)\n",
    "#         self.output_mlp = build_mlp(layer_dims[::-1])\n",
    "\n",
    "#         self.rnn = torch.nn.GRU(\n",
    "#             input_size=self.input_size,\n",
    "#             hidden_size=self.hidden_size,\n",
    "#             num_layers=1,\n",
    "#         )\n",
    "\n",
    "#     def convert_state(self, state):\n",
    "#         return self.input_mlp(state)\n",
    "\n",
    "#     def forward(self, rnn_input, rnn_state):\n",
    "#         # This only does one step\n",
    "#         res = self.rnn(rnn_input.unsqueeze(0), rnn_state.unsqueeze(0))\n",
    "#         output = self.output_mlp(res[1][0])\n",
    "#         return res[1][0], output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# class RNNPredictorBurnin(torch.nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         hidden_size: int = 512,\n",
    "#         output_size: int = 512,\n",
    "#         num_layers: int = 1,\n",
    "#         action_dim: int = 2,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.output_size = output_size\n",
    "#         self.action_dim = action_dim\n",
    "\n",
    "#         self.rnn = torch.nn.GRU(\n",
    "#             input_size=action_dim + output_size,\n",
    "#             hidden_size=self.hidden_size,\n",
    "#             num_layers=num_layers,\n",
    "#         )\n",
    "#         self.output_projector = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def burn_in(\n",
    "#         self,\n",
    "#         encs: torch.Tensor,\n",
    "#         actions: torch.Tensor,\n",
    "#         h: Optional[torch.Tensor] = None,\n",
    "#     ):\n",
    "#         \"\"\"Runs a few iterations of RNN with the provided GT encodings to obtain h0\"\"\"\n",
    "#         if h is None:\n",
    "#             h = torch.zeros(self.num_layers, actions.shape[1], self.hidden_size).to(\n",
    "#                 actions.device\n",
    "#             )\n",
    "\n",
    "#         for i in range(encs.shape[0]):\n",
    "#             rnn_input = torch.cat([encs[i], actions[i]], dim=1).unsqueeze(0)\n",
    "#             _, h = self.rnn(rnn_input, h)\n",
    "#         return h\n",
    "\n",
    "#     def predict_sequence(\n",
    "#         self, enc: torch.Tensor, actions: torch.Tensor, h: Optional[torch.Tensor] = None\n",
    "#     ):\n",
    "#         \"\"\"Predicts the sequence given gt encoding for the current time step\"\"\"\n",
    "#         outputs = []\n",
    "#         if h is None:\n",
    "#             h = torch.zeros(self.num_layers, actions.shape[1], self.hidden_size).to(\n",
    "#                 actions.device\n",
    "#             )\n",
    "#         for i in range(actions.shape[0]):\n",
    "#             rnn_input = torch.cat([enc, actions[i]], dim=1).unsqueeze(0)\n",
    "#             _, h = self.rnn(rnn_input, h)\n",
    "#             outputs.append(self.output_projector(h[-1]))\n",
    "#             enc = outputs[-1]  # autoregressive GRU\n",
    "#         outputs = torch.stack(outputs)\n",
    "#         return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# class RSSMPredictor(torch.nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         rnn_state_dim: int,\n",
    "#         z_dim: int,\n",
    "#         action_dim: int = 2,\n",
    "#         min_var: float = 0,\n",
    "#         use_action_only: bool = True,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.rnn_state_dim = rnn_state_dim\n",
    "#         self.z_dim = z_dim\n",
    "#         self.input_dim = z_dim + action_dim\n",
    "#         self.action_dim = action_dim\n",
    "#         self.prior_mu_net = nn.Linear(self.rnn_state_dim, self.z_dim)\n",
    "#         self.prior_var_net = nn.Linear(self.rnn_state_dim, self.z_dim)\n",
    "#         self.rnn = torch.nn.GRUCell(self.input_dim, self.rnn_state_dim)\n",
    "#         self.min_var = min_var\n",
    "#         self.use_action_only = use_action_only\n",
    "\n",
    "#     def forward(self, sampled_prior, action, rnn_state):\n",
    "#         if action is not None and self.use_action_only:\n",
    "#             rnn_input = action  # torch.cat([sampled_prior, action], dim=-1)\n",
    "#         elif action is not None and not self.use_action_only:\n",
    "#             rnn_input = torch.cat([sampled_prior, action], dim=-1)\n",
    "#         else:\n",
    "#             rnn_input = sampled_prior\n",
    "\n",
    "#         rnn_state_new = self.rnn(rnn_input, rnn_state)\n",
    "#         prior_mu = self.prior_mu_net(rnn_state_new)\n",
    "#         prior_var = self.prior_var_net(rnn_state_new)\n",
    "#         return rnn_state_new, prior_mu, prior_var\n",
    "\n",
    "#     def burn_in(self, *args, **kwargs):\n",
    "#         return None\n",
    "\n",
    "#     def predict_sequence(\n",
    "#         self,\n",
    "#         enc: torch.Tensor,\n",
    "#         actions: torch.Tensor,\n",
    "#         h: torch.Tensor,\n",
    "#         latents: Optional[torch.Tensor] = None,\n",
    "#     ):\n",
    "#         initial_belief = enc\n",
    "#         batch_size = enc.shape[0]\n",
    "#         sampled_prior_state = torch.zeros(batch_size, self.z_dim).to(enc.device)\n",
    "#         sampled_prior_states = []\n",
    "#         beliefs = []\n",
    "#         rnn_belief = initial_belief\n",
    "#         for i in range(len(actions)):\n",
    "#             rnn_belief, prior_mu, prior_var = self(\n",
    "#                 sampled_prior_state, actions[i], rnn_belief\n",
    "#             )\n",
    "#             prior_var = F.softplus(prior_var) + self.min_var\n",
    "#             z = Normal(prior_mu, (prior_var))\n",
    "#             if latents is not None:\n",
    "#                 sampled_prior_state = latents[i]\n",
    "#             else:\n",
    "#                 sampled_prior_state = z.sample()\n",
    "#             sampled_prior_states.append(sampled_prior_state)\n",
    "#             beliefs.append(rnn_belief)\n",
    "#         beliefs = torch.stack(beliefs, dim=0)\n",
    "#         return beliefs\n",
    "\n",
    "#     def predict_sequence_posterior(\n",
    "#         self,\n",
    "#         encs: torch.Tensor,\n",
    "#         h: torch.Tensor,\n",
    "#         hjepa: torch.nn.Module,\n",
    "#     ):\n",
    "#         result = []\n",
    "#         T = encs.shape[0] + 1\n",
    "#         batch_size = encs.shape[1]\n",
    "#         rnn_state = encs[0]\n",
    "#         sampled_posterior_state = torch.zeros(batch_size, self.z_dim).to(encs.device)\n",
    "#         for i in range(T - 1):\n",
    "#             rnn_state = hjepa.predictor_l2(\n",
    "#                 sampled_prior=sampled_posterior_state, action=None, rnn_state=rnn_state\n",
    "#             )\n",
    "#             posterior_mu, posterior_var = hjepa.posterior_l2(encs[i + 1], rnn_state)\n",
    "#             posterior_var = F.softplus(posterior_var) + self.min_var\n",
    "#             sampled_posterior_state = Normal(posterior_mu, (posterior_var)).sample()\n",
    "#             prediction = hjepa.decoder(rnn_state, sampled_posterior_state)\n",
    "#             result.append(prediction)\n",
    "#         return result\n",
    "\n",
    "#     def predict_decode_sequence(\n",
    "#         self,\n",
    "#         enc: torch.Tensor,\n",
    "#         h: torch.Tensor,\n",
    "#         latents: torch.Tensor,\n",
    "#         decoder: torch.nn.Module,\n",
    "#     ):\n",
    "#         beliefs = self.predict_sequence(enc, [None] * latents.shape[0], h, latents)\n",
    "#         return decoder(beliefs, latents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# class SequencePredictor(torch.nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         config,\n",
    "#         repr_dim,\n",
    "#         z_dim: Optional[int] = None,\n",
    "#         z_min_std: Optional[float] = None,\n",
    "#         z_discrete: Optional[bool] = None,\n",
    "#         z_discrete_dists: Optional[int] = None,\n",
    "#         z_discrete_dim: Optional[int] = None,\n",
    "#         posterior_drop_p: Optional[float] = None,\n",
    "#         predictor_ln: Optional[bool] = False,\n",
    "#         prior_arch: Optional[str] = None,\n",
    "#         posterior_arch: Optional[str] = None,\n",
    "#         posterior_input_type: Optional[str] = None,\n",
    "#         posterior_input_dim: Optional[str] = None,\n",
    "#         action_dim: Optional[int] = None,\n",
    "#         pred_propio_dim: Optional[Union[int, tuple]] = 0,\n",
    "#         pred_obs_dim: Optional[Union[int, tuple]] = 0,\n",
    "#         backbone_ln: Optional[torch.nn.Module] = None,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.config = config\n",
    "#         self.repr_dim = repr_dim  # may need to flatten for prior and posterior...\n",
    "#         self.posterior_drop_p = posterior_drop_p\n",
    "#         self.posterior_input_type = posterior_input_type\n",
    "#         self.z_discrete = z_discrete\n",
    "#         self.action_dim = action_dim\n",
    "#         self.pred_propio_dim = pred_propio_dim\n",
    "#         self.pred_obs_dim = pred_obs_dim\n",
    "\n",
    "#         if config.tie_backbone_ln:\n",
    "#             self.final_ln = backbone_ln\n",
    "#         elif config.predictor_ln:\n",
    "#             self.final_ln = nn.LayerNorm(repr_dim)\n",
    "#         else:\n",
    "#             self.final_ln = nn.Identity()\n",
    "\n",
    "#         self.prior_model = None\n",
    "#         self.posterior_model = None\n",
    "\n",
    "#     def forward_multiple(\n",
    "#         self,\n",
    "#         state_encs,\n",
    "#         actions,\n",
    "#         T,\n",
    "#         latents=None,\n",
    "#         flatten_output=False,\n",
    "#         compute_posterior=False,\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         This does multiple steps\n",
    "#         Parameters:\n",
    "#             state_encs: (t, BS, input_dim)\n",
    "#             actions: (t-1, BS, action_dim)\n",
    "#             T: timesteps to propagate forward\n",
    "#         Output:\n",
    "#             state_predictions: (T, BS, hidden_dim)\n",
    "#             rnn_states: (T, BS, hidden_dim)\n",
    "#         \"\"\"\n",
    "#         bs = state_encs.shape[1]\n",
    "#         current_state = state_encs[0]\n",
    "#         state_predictions = [current_state]\n",
    "#         prior_mus = []\n",
    "#         prior_vars = []\n",
    "#         prior_logits = []\n",
    "#         priors = []\n",
    "#         posterior_mus = []\n",
    "#         posterior_vars = []\n",
    "#         posterior_logits = []\n",
    "#         posteriors = []\n",
    "\n",
    "#         for i in range(T):\n",
    "#             predictor_input = []\n",
    "#             if self.prior_model is not None:\n",
    "#                 prior_stats = self.prior_model(flatten_conv_output(current_state))\n",
    "#                 # z is of shape BxD\n",
    "\n",
    "#                 if latents is not None:\n",
    "#                     prior = latents[i]\n",
    "#                 else:\n",
    "#                     prior = self.prior_model.sample(prior_stats)\n",
    "\n",
    "#                 if self.z_discrete:\n",
    "#                     prior = self.latent_merger(prior)\n",
    "#                     prior_logits.append(prior_stats)\n",
    "#                 else:\n",
    "#                     mu, var = prior_stats\n",
    "#                     prior_mus.append(mu)\n",
    "#                     prior_vars.append(var)\n",
    "\n",
    "#                 priors.append(prior)\n",
    "\n",
    "#                 if compute_posterior:\n",
    "#                     # compute posterior\n",
    "#                     if self.posterior_input_type == \"term_states\":\n",
    "#                         posterior_input = torch.cat(\n",
    "#                             [\n",
    "#                                 flatten_conv_output(current_state),\n",
    "#                                 flatten_conv_output(state_encs[i + 1]),\n",
    "#                             ],\n",
    "#                             dim=-1,\n",
    "#                         )\n",
    "#                     elif self.posterior_input_type == \"actions\":\n",
    "#                         posterior_input = actions[i]\n",
    "\n",
    "#                     posterior_stats = self.posterior_model(posterior_input)\n",
    "#                     posterior = self.posterior_model.sample(posterior_stats)\n",
    "\n",
    "#                     if self.z_discrete:\n",
    "#                         posterior = self.latent_merger(posterior)\n",
    "#                         posterior_logits.append(posterior_stats)\n",
    "#                     else:\n",
    "#                         posterior_mu, posterior_var = posterior_stats\n",
    "#                         posterior_mus.append(posterior_mu)\n",
    "#                         posterior_vars.append(posterior_var)\n",
    "\n",
    "#                     posteriors.append(posterior)\n",
    "\n",
    "#                     z_input = posterior\n",
    "\n",
    "#                     if (\n",
    "#                         self.posterior_drop_p\n",
    "#                         and np.random.random() < self.posterior_drop_p\n",
    "#                     ):\n",
    "#                         z_input = prior\n",
    "#                         # TODO check this. seems like a bug. not supposed to replace all posterior with prior\n",
    "#                     predictor_input.append(z_input)\n",
    "#                 else:\n",
    "#                     predictor_input.append(prior)\n",
    "#             else:\n",
    "#                 prior = None\n",
    "#                 predictor_input.append(actions[i])\n",
    "\n",
    "#             assert len(predictor_input) > 0\n",
    "\n",
    "#             next_state = self.forward(\n",
    "#                 current_state, torch.cat(predictor_input, dim=-1)\n",
    "#             )\n",
    "#             current_state = next_state\n",
    "\n",
    "#             state_predictions.append(next_state)\n",
    "\n",
    "#         t = len(state_predictions)\n",
    "#         state_predictions = torch.stack(state_predictions)\n",
    "#         if flatten_output:\n",
    "#             state_predictions = state_predictions.view(t, bs, -1)\n",
    "\n",
    "#         prior_mus = torch.stack(prior_mus) if prior_mus else None\n",
    "#         prior_vars = torch.stack(prior_vars) if prior_vars else None\n",
    "#         prior_logits = torch.stack(prior_logits) if prior_logits else None\n",
    "#         priors = torch.stack(priors) if priors else None\n",
    "#         posterior_mus = torch.stack(posterior_mus) if posterior_mus else None\n",
    "#         posterior_vars = torch.stack(posterior_vars) if posterior_vars else None\n",
    "#         posterior_logits = torch.stack(posterior_logits) if posterior_logits else None\n",
    "#         posteriors = torch.stack(posteriors) if posteriors else None\n",
    "\n",
    "#         if self.pred_propio_dim:\n",
    "#             if isinstance(self.pred_propio_dim, int):\n",
    "#                 obs_component = state_predictions[:, :, : -self.pred_propio_dim]\n",
    "#                 propio_component = state_predictions[:, :, -self.pred_propio_dim :]\n",
    "#             else:\n",
    "#                 pred_propio_channels = self.pred_propio_dim[0]\n",
    "#                 obs_component = state_predictions[:, :, :-pred_propio_channels]\n",
    "#                 propio_component = state_predictions[:, :, -pred_propio_channels:]\n",
    "#         else:\n",
    "#             obs_component = state_predictions\n",
    "#             propio_component = None\n",
    "\n",
    "#         output = PredictorOutput(\n",
    "#             predictions=state_predictions,\n",
    "#             obs_component=obs_component,\n",
    "#             propio_component=propio_component,\n",
    "#             prior_mus=prior_mus,\n",
    "#             prior_vars=prior_vars,\n",
    "#             prior_logits=prior_logits,\n",
    "#             priors=priors,\n",
    "#             posterior_mus=posterior_mus,\n",
    "#             posterior_vars=posterior_vars,\n",
    "#             posterior_logits=posterior_logits,\n",
    "#             posteriors=posteriors,\n",
    "#         )\n",
    "\n",
    "#         return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# class MLPPredictor(SequencePredictor):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         # parent inputs\n",
    "#         config,\n",
    "#         repr_dim,\n",
    "#         action_dim=2,  # action + z_dim\n",
    "#         pred_propio_dim=0,\n",
    "#         pred_obs_dim=0,\n",
    "#         backbone_ln: Optional[torch.nn.Module] = None,\n",
    "#     ):\n",
    "#         super().__init__(\n",
    "#             config=config,\n",
    "#             repr_dim=repr_dim,\n",
    "#             action_dim=action_dim,\n",
    "#             pred_propio_dim=pred_propio_dim,\n",
    "#             pred_obs_dim=pred_obs_dim,\n",
    "#             backbone_ln=backbone_ln,\n",
    "#         )\n",
    "\n",
    "#         self.fc = build_mlp(\n",
    "#             layers_dims=config.predictor_subclass,\n",
    "#             input_dim=repr_dim + action_dim,\n",
    "#             output_shape=repr_dim,\n",
    "#             norm=\"layer_norm\" if config.predictor_ln else None,\n",
    "#             activation=\"mish\",\n",
    "#         )\n",
    "\n",
    "#     def forward(self, current_state, curr_action):\n",
    "#         inp = torch.cat([current_state, curr_action], dim=-1)\n",
    "#         out = self.fc(inp)\n",
    "#         out = self.final_ln(out)\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# class RNNPredictorV2(SequencePredictor):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         # parent inputs\n",
    "#         config,\n",
    "#         hidden_size: int = 512,\n",
    "#         z_dim: Optional[int] = None,\n",
    "#         z_min_std: Optional[float] = None,\n",
    "#         posterior_drop_p: Optional[float] = None,\n",
    "#         prior_arch: Optional[str] = None,\n",
    "#         posterior_arch: Optional[str] = None,\n",
    "#         posterior_input_type: Optional[str] = None,\n",
    "#         posterior_input_dim: Optional[str] = None,\n",
    "#         action_dim: Optional[int] = None,\n",
    "#         pred_propio_dim=0,\n",
    "#         pred_obs_dim=0,\n",
    "#         # child inputs\n",
    "#         predictor_ln: Optional[bool] = False,\n",
    "#         num_layers: int = 1,\n",
    "#         input_size: int = 2,\n",
    "#         backbone_ln: Optional[torch.nn.Module] = None,\n",
    "#     ):\n",
    "#         super().__init__(\n",
    "#             config=config,\n",
    "#             repr_dim=hidden_size,\n",
    "#             z_dim=z_dim,\n",
    "#             z_min_std=z_min_std,\n",
    "#             posterior_drop_p=posterior_drop_p,\n",
    "#             prior_arch=prior_arch,\n",
    "#             posterior_arch=posterior_arch,\n",
    "#             posterior_input_type=posterior_input_type,\n",
    "#             posterior_input_dim=posterior_input_dim,\n",
    "#             action_dim=action_dim,\n",
    "#             pred_propio_dim=pred_propio_dim,\n",
    "#             pred_obs_dim=pred_obs_dim,\n",
    "#             backbone_ln=backbone_ln,\n",
    "#         )\n",
    "\n",
    "#         self.num_layers = num_layers\n",
    "#         self.input_size = input_size\n",
    "\n",
    "#         self.rnn = torch.nn.GRU(\n",
    "#             input_size=self.input_size,\n",
    "#             hidden_size=self.repr_dim,\n",
    "#             num_layers=num_layers,\n",
    "#         )\n",
    "\n",
    "#     def forward(self, rnn_state, rnn_input):\n",
    "#         \"\"\"\n",
    "#         Propagate one step forward\n",
    "#         Parameters:\n",
    "#             rnn_state: (num_layers, bs, dim)\n",
    "#             rnn_input: (bs, a_dim)\n",
    "#         Output:\n",
    "#             output: next_state (bs, dim), next_hidden_state (num_layers, bs, dim)\n",
    "#         \"\"\"\n",
    "#         # This only does one step\n",
    "\n",
    "#         next_state, next_hidden_state = self.rnn(rnn_input.unsqueeze(0), rnn_state)\n",
    "\n",
    "#         next_state = self.final_ln(next_state)\n",
    "#         next_hidden_state = self.final_ln(next_hidden_state)\n",
    "\n",
    "#         return next_state[0], next_hidden_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from mawm.models.utils import Expander2D\n",
    "ConvPredictorConfig = {\n",
    "    \"a\": [(18, 32, 3, 1, 1), (32, 32, 3, 1, 1), (32, 16, 3, 1, 1)],\n",
    "    \"b\": [(18, 32, 5, 1, 2), (32, 32, 5, 1, 2), (32, 16, 5, 1, 2)],\n",
    "    \"c\": [(18, 32, 7, 1, 3), (32, 32, 7, 1, 3), (32, 16, 7, 1, 3)],\n",
    "    \"a_propio\": [(20, 32, 3, 1, 1), (32, 32, 3, 1, 1), (32, 18, 3, 1, 1)],\n",
    "    \"d4rl_b_p\": [(20, 32, 3, 1, 1), (32, 64, 3, 1, 1), (64, 32, 3, 1, 1)],\n",
    "    \"d4rl_c_p\": [(36, 32, 3, 1, 1), (32, 32, 3, 1, 1), (32, 34, 3, 1, 1)],\n",
    "}\n",
    "\n",
    "class ConvPredictor(nn.Module):\n",
    "      def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        repr_dim,\n",
    "        action_dim=5,\n",
    "        msg_dim=32,\n",
    "        pred_pos_dim=0,\n",
    "        pred_obs_dim=0,\n",
    "        predictor_subclass=\"a\",\n",
    "        num_groups=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.repr_dim = repr_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.pred_pos_dim = pred_pos_dim\n",
    "        self.pred_obs_dim = pred_obs_dim\n",
    "        self.predictor_subclass = predictor_subclass if not config.predictor_subclass else config.predictor_subclass\n",
    "        self.num_groups = num_groups\n",
    "        self.msg_dim = msg_dim\n",
    "\n",
    "        # Define convolutional layers\n",
    "        layers = []\n",
    "        layers_config = ConvPredictorConfig[self.predictor_subclass]\n",
    "        in_channels, out_channels, k, s, p = layers_config[0]\n",
    "\n",
    "        if self.config.action_encoder_arch != \"id\":\n",
    "            action_inp_dim = int(self.config.action_encoder_arch.split(\"-\")[-1])\n",
    "        else:\n",
    "            action_inp_dim = self.action_dim\n",
    "\n",
    "        actual_in_channels = repr_dim[0] + action_inp_dim + self.msg_dim\n",
    "        for i in range(len(layers_config) - 1):\n",
    "            in_channels, out_channels, kernel_size, stride, padding = layers_config[i]\n",
    "\n",
    "            if i == 0:\n",
    "                # in_channels = repr_dim[0] + action_dim\n",
    "                in_channels = actual_in_channels\n",
    "\n",
    "            layers.append(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            )\n",
    "            layers.append(nn.GroupNorm(4, out_channels))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        # last layer\n",
    "        in_channels, out_channels, kernel_size, stride, padding = layers_config[-1]\n",
    "        layers.append(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        )\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "        if self.config.action_encoder_arch != \"id\":\n",
    "            self.action_encoder = nn.Sequential(\n",
    "                nn.Linear(self.action_dim, repr_dim[0]),\n",
    "                Expander2D(w=repr_dim[-2], h=repr_dim[-1]),\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            self.action_encoder = Expander2D(w=repr_dim[-2], h=repr_dim[-1])\n",
    "\n",
    "        self.msg_encoder = Expander2D(w=repr_dim[-2], h=repr_dim[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def forward(self: ConvPredictor, current_state, curr_action, curr_msg):\n",
    "    bs, _, h, w = current_state.shape\n",
    "    curr_action = self.action_encoder(curr_action)\n",
    "    curr_msg = self.msg_encoder(curr_msg)\n",
    "    x = torch.cat([current_state, curr_action, curr_msg], dim=1)\n",
    "    x = self.layers(x) \n",
    "    # if self.config.residual:\n",
    "    #     x = x + current_state\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def forward_multiple(\n",
    "    self: ConvPredictor,\n",
    "    z0,\n",
    "    actions,\n",
    "    msgs,\n",
    "    T,\n",
    "    flatten_output=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    This does multiple steps\n",
    "    Parameters:\n",
    "        z0: (T, BS, input_dim)\n",
    "        actions: (T-1, BS, action_dim)\n",
    "        T: timesteps to propagate forward\n",
    "    Output:\n",
    "        state_predictions: (T, BS, hidden_dim)\n",
    "    \"\"\"\n",
    "    bs = z0.shape[1]\n",
    "    current_state = z0[0]\n",
    "    state_predictions = [current_state]\n",
    "    \n",
    "    for i in range(T):\n",
    "        predictor_input = []\n",
    "        predictor_input.append(actions[i])\n",
    "\n",
    "        lst_msgs = []\n",
    "        lst_msgs.append(msgs[i])\n",
    "\n",
    "        next_state = self.forward(\n",
    "            current_state, torch.cat(predictor_input, dim=-1), torch.cat(lst_msgs, dim=-1)\n",
    "        )\n",
    "        current_state = next_state\n",
    "\n",
    "        state_predictions.append(next_state)\n",
    "\n",
    "    t = len(state_predictions)\n",
    "    state_predictions = torch.stack(state_predictions)\n",
    "    if flatten_output:\n",
    "        state_predictions = state_predictions.view(t, bs, -1)\n",
    "\n",
    "    if self.pred_pos_dim:\n",
    "        if isinstance(self.pred_pos_dim, int):\n",
    "            obs_component = state_predictions[:, :, : -self.pred_pos_dim]\n",
    "            pos_component = state_predictions[:, :, -self.pred_pos_dim :]\n",
    "        else:\n",
    "            pred_pos_channels = self.pred_pos_dim[0]\n",
    "            obs_component = state_predictions[:, :, :-pred_pos_channels]\n",
    "            pos_component = state_predictions[:, :, -pred_pos_channels:]\n",
    "    else:\n",
    "        obs_component = state_predictions\n",
    "        pos_component = None\n",
    "\n",
    "    return obs_component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action_dim': 5, 'residual': True, 'action_encoder_arch': '5-32-64-32', 'arch': 'ConvPredictor', 'predictor_subclass': 'd4rl_b_p', 'rnn_converter_arch': '', 'rnn_layers': 1, 'rnn_state_dim': 512, 'z_dim': 0, 'z_min_std': 0.1}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "cfg = OmegaConf.load(\"../cfgs/MPCJepa/mpc.yaml\")\n",
    "cfg.model.backbone.position_dim = 2\n",
    "cfg.model.predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "model = ConvPredictor(\n",
    "    config=cfg.model.predictor,\n",
    "    repr_dim=(32, 15, 15),\n",
    "    action_dim=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvPredictor(\n",
       "  (layers): Sequential(\n",
       "    (0): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (action_encoder): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=32, bias=True)\n",
       "    (1): Expander2D()\n",
       "  )\n",
       "  (msg_encoder): Expander2D()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 8, 32, 15, 15])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "# torch.Size([16, 8, 18, 15, 15])\n",
    "T = 6\n",
    "B = 8\n",
    "inp = torch.randn(T, B, 32, 15, 15)\n",
    "act = torch.randn(T-1, B, 5)\n",
    "msg = torch.randn(T-1, B, 32)\n",
    "out = model.forward_multiple(inp, act, msg, T=act.size(0), flatten_output=False)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action_dim': 5, 'residual': True, 'action_encoder_arch': '5-32-64-32', 'arch': 'ConvPredictor', 'predictor_subclass': 'd4rl_b_p', 'rnn_converter_arch': '', 'rnn_layers': 1, 'rnn_state_dim': 512, 'z_dim': 0, 'z_min_std': 0.1}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marlgrid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
