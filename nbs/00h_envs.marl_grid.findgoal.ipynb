{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08bc68fb",
   "metadata": {},
   "source": [
    "# Findgoal Environment \n",
    "\n",
    "> Basic scenario where agents need to find and reach a goal in the grid world. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1603b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp envs.marl_grid.findgoal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d54069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac0a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import *\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a8c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "\n",
    "from mawm.envs.marl_grid.base import MultiGridEnv, MultiGrid\n",
    "from mawm.envs.marl_grid.objects import Goal, Wall\n",
    "\n",
    "\n",
    "def dis_func(x, y, k=1):\n",
    "    return np.linalg.norm(x - y) / k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac67850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import gymnasium as gym\n",
    "from typing import Optional \n",
    "\n",
    "class FindGoalMultiGrid(MultiGridEnv):\n",
    "    \"\"\"\n",
    "    A single cluttered room with a green goal at random position.\n",
    "    Each agent obtains a reward when reaching the goal.\n",
    "    All agents must be reach the goal to obtain a team reward.\n",
    "    \"\"\"\n",
    "    mission = 'get to the green square'\n",
    "    metadata = {}\n",
    "\n",
    "    def __init__(self, config):\n",
    "        n_clutter = config.get('n_clutter')\n",
    "        clutter_density = config.get('clutter_density')\n",
    "        randomize_goal = config.get('randomize_goal')\n",
    "        self.spawn_without_goal_view = config.get('spawn_without_goal_view', True)\n",
    "        self.min_goal_spawn_distance = config.get('min_goal_spawn_distance', 7)\n",
    "\n",
    "        if (n_clutter is None) == (clutter_density is None):\n",
    "            raise ValueError('Must provide n_clutter or clutter_density.')\n",
    "\n",
    "        super().__init__(config)\n",
    "\n",
    "        if clutter_density is not None:\n",
    "            self.n_clutter = int(\n",
    "                clutter_density * (self.width - 2) * (self.height - 2))\n",
    "        else:\n",
    "            self.n_clutter = n_clutter\n",
    "\n",
    "        self.randomize_goal = randomize_goal\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        self.grid = MultiGrid((width, height))\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "        if getattr(self, 'randomize_goal', True):\n",
    "            goal_pos = self.place_obj(Goal(color='green', reward=1),\n",
    "                                      max_tries=100)\n",
    "        else:\n",
    "            goal_pos = np.asarray([width - 2, height - 2])\n",
    "            self.put_obj(Goal(color='green', reward=1), width - 2, height - 2)\n",
    "\n",
    "        for _ in range(getattr(self, 'n_clutter', 0)):\n",
    "            self.place_obj(Wall(), max_tries=100)\n",
    "\n",
    "        return goal_pos\n",
    "\n",
    "    def gen_global_obs(self, agent_done=None):\n",
    "        if agent_done is None:\n",
    "            # an integer array storing agent's done info\n",
    "            agent_done = np.zeros((len(self.agents, )), dtype=float)\n",
    "        self.sees_goal = np.array([self.agents[i].in_view(\n",
    "                self.goal_pos[0], self.goal_pos[1]) for i in range(\n",
    "                self.num_agents)]) * 1\n",
    "\n",
    "        obs = {\n",
    "            'adv_indices': self.adv_indices,\n",
    "            'agent_done': agent_done,  # (N,)\n",
    "            'goal_pos': self.goal_pos,  # (2,)\n",
    "            'sees_goal': self.sees_goal,  # (N,)\n",
    "            'pos': np.stack([self.get_agent_pos(a) for a in self.agents],\n",
    "                            axis=0),  # (N, 2)\n",
    "            'comm_act': np.stack([a.comm for a in self.agents],\n",
    "                                 axis=0),  # (N, comm_len)\n",
    "            'env_act': np.stack([a.env_act for a in self.agents],\n",
    "                                axis=0),  # (N, 1)\n",
    "        }\n",
    "        return obs\n",
    "\n",
    "    # def reset(self, seed: int = None, options: dict = None):\n",
    "    #     obs_dict = MultiGridEnv.reset(self, seed=seed, options=options)\n",
    "\n",
    "    #     if self.num_adversaries < 0:\n",
    "    #         # need to count number of adversaries in the env\n",
    "    #         self.adv_indices = set()\n",
    "    #         for i, agent in enumerate(self.agents):\n",
    "    #             if agent.is_adversary:\n",
    "    #                 self.adv_indices.add(i)\n",
    "    #         self.num_adversaries = len(self.adv_indices)\n",
    "\n",
    "    #         obs_dict['global'] = self.gen_global_obs()\n",
    "    #         return obs_dict\n",
    "\n",
    "    #     else:\n",
    "    #         # randomize adv indices each episode\n",
    "    #         adv_indices = np.random.choice([i for i in range(self.num_agents)],\n",
    "    #                                        self.num_adversaries,\n",
    "    #                                        replace=False)\n",
    "    #         for i, agent in enumerate(self.agents):\n",
    "    #             if i in adv_indices:\n",
    "    #                 agent.is_adversary = True\n",
    "    #             else:\n",
    "    #                 agent.is_adversary = False\n",
    "    #         self.adv_indices = adv_indices\n",
    "\n",
    "    #         obs_dict['global'] = self.gen_global_obs()\n",
    "    #         return obs_dict\n",
    "\n",
    "    # def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "    #     \"\"\"\n",
    "    #     Modified reset that ensures agents don't spawn with goal visible.\n",
    "    #     \"\"\"\n",
    "    #     # Do the parent's agent reset logic manually\n",
    "    #     for agent in self.agents:\n",
    "    #         agent.agents = []\n",
    "    #         agent.reset(new_episode=True)\n",
    "\n",
    "    #     # Generate grid and goal (this is done in parent's reset)\n",
    "    #     self.goal_pos = self._gen_grid(self.width, self.height)\n",
    "        \n",
    "    #     # Define rejection function based on config\n",
    "    #     if self.spawn_without_goal_view:\n",
    "    #         def reject_spawn_fn(pos):\n",
    "    #             \"\"\"Reject positions too close to goal (goal might be visible)\"\"\"\n",
    "    #             dist = abs(pos[0] - self.goal_pos[0]) + abs(pos[1] - self.goal_pos[1])\n",
    "    #             return dist < self.min_goal_spawn_distance\n",
    "    #     else:\n",
    "    #         reject_spawn_fn = None\n",
    "        \n",
    "    #     # Place agents with rejection function\n",
    "    #     for agent in self.agents:\n",
    "    #         if agent.spawn_delay == 0:\n",
    "    #             self.place_obj(\n",
    "    #                 agent, \n",
    "    #                 reject_fn=reject_spawn_fn,\n",
    "    #                 max_tries=1000,  # Increase tries since we're constraining placement\n",
    "    #                 **self.agent_spawn_kwargs\n",
    "    #             )\n",
    "    #             agent.activate()\n",
    "\n",
    "    #     self.step_count = 0\n",
    "    #     obs = self.gen_obs()\n",
    "    #     obs_dict = {f'agent_{i}': obs[i] for i in range(len(obs))}\n",
    "        \n",
    "    #     # Your existing adversary logic\n",
    "    #     if self.num_adversaries < 0:\n",
    "    #         # need to count number of adversaries in the env\n",
    "    #         self.adv_indices = set()\n",
    "    #         for i, agent in enumerate(self.agents):\n",
    "    #             if agent.is_adversary:\n",
    "    #                 self.adv_indices.add(i)\n",
    "    #         self.num_adversaries = len(self.adv_indices)\n",
    "    #     else:\n",
    "    #         # randomize adv indices each episode\n",
    "    #         adv_indices = np.random.choice(\n",
    "    #             [i for i in range(self.num_agents)],\n",
    "    #             self.num_adversaries,\n",
    "    #             replace=False\n",
    "    #         )\n",
    "    #         for i, agent in enumerate(self.agents):\n",
    "    #             if i in adv_indices:\n",
    "    #                 agent.is_adversary = True\n",
    "    #             else:\n",
    "    #                 agent.is_adversary = False\n",
    "    #         self.adv_indices = adv_indices\n",
    "        \n",
    "    #     obs_dict['global'] = self.gen_global_obs()\n",
    "    #     return obs_dict\n",
    "    \n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        \"\"\"\n",
    "        Override reset to add goal visibility constraint during agent placement.\n",
    "        \n",
    "        This replicates MultiGridEnv.reset() but adds a rejection function\n",
    "        for agent placement.\n",
    "        \"\"\"\n",
    "        # Only call gym.Env.reset() for proper seeding (not MultiGridEnv.reset())\n",
    "        # gym.Env.reset() just handles seeding and returns empty info\n",
    "        gym.Env.reset(self, seed=seed)\n",
    "        \n",
    "        # Now do MultiGridEnv's reset logic with our modifications\n",
    "        for agent in self.agents:\n",
    "            agent.agents = []\n",
    "            agent.reset(new_episode=True)\n",
    "\n",
    "        # Generate grid and goal\n",
    "        self.goal_pos = self._gen_grid(self.width, self.height)\n",
    "        \n",
    "        # Define rejection function if enabled\n",
    "        if self.spawn_without_goal_view:\n",
    "            def reject_spawn_fn(pos):\n",
    "                \"\"\"Reject positions too close to goal (where goal might be visible)\"\"\"\n",
    "                dist = abs(pos[0] - self.goal_pos[0]) + abs(pos[1] - self.goal_pos[1])\n",
    "                return dist < self.min_goal_spawn_distance\n",
    "        else:\n",
    "            reject_spawn_fn = None\n",
    "        \n",
    "        # Place agents with our custom rejection function\n",
    "        for agent in self.agents:\n",
    "            if agent.spawn_delay == 0:\n",
    "                self.place_obj(\n",
    "                    agent,\n",
    "                    reject_fn=reject_spawn_fn,  # Add rejection function\n",
    "                    max_tries=1000,  # Increase max tries\n",
    "                    **self.agent_spawn_kwargs\n",
    "                )\n",
    "                agent.activate()\n",
    "\n",
    "        self.step_count = 0\n",
    "        obs = self.gen_obs()\n",
    "        obs_dict = {f'agent_{i}': obs[i] for i in range(len(obs))}\n",
    "        \n",
    "        # FindGoalMultiGrid-specific: handle adversaries\n",
    "        if self.num_adversaries < 0:\n",
    "            # Count number of adversaries in the env\n",
    "            self.adv_indices = set()\n",
    "            for i, agent in enumerate(self.agents):\n",
    "                if agent.is_adversary:\n",
    "                    self.adv_indices.add(i)\n",
    "            self.num_adversaries = len(self.adv_indices)\n",
    "        else:\n",
    "            # Randomize adversary indices each episode\n",
    "            adv_indices = np.random.choice(\n",
    "                [i for i in range(self.num_agents)],\n",
    "                self.num_adversaries,\n",
    "                replace=False\n",
    "            )\n",
    "            for i, agent in enumerate(self.agents):\n",
    "                if i in adv_indices:\n",
    "                    agent.is_adversary = True\n",
    "                else:\n",
    "                    agent.is_adversary = False\n",
    "            self.adv_indices = adv_indices\n",
    "        \n",
    "        obs_dict['global'] = self.gen_global_obs()\n",
    "        return obs_dict\n",
    "    \n",
    "    \n",
    "    def _get_reward(self, rwd, agent_no):\n",
    "        step_rewards = np.zeros((len(self.agents, )), dtype=float)\n",
    "        env_rewards = np.zeros((len(self.agents, )), dtype=float)\n",
    "        if agent_no in self.adv_indices:\n",
    "            # agent can only receive rewards if it is non-adversarial\n",
    "            return env_rewards, step_rewards\n",
    "\n",
    "        env_rewards[agent_no] += rwd\n",
    "        if self.team_reward_type == 'share':\n",
    "            # assign zero-sum rewards to both teams\n",
    "            for agent_id in range(self.num_agents):\n",
    "                if agent_id not in self.adv_indices:\n",
    "\n",
    "                    step_rewards[agent_id] += rwd\n",
    "                    self.agents[agent_id].reward(rwd)\n",
    "                else:\n",
    "                    step_rewards[agent_id] -= rwd\n",
    "                    self.agents[agent_id].reward(-rwd)\n",
    "        else:\n",
    "            step_rewards[agent_no] += rwd\n",
    "            self.agents[agent_no].reward(rwd)\n",
    "        return env_rewards, step_rewards\n",
    "\n",
    "    def update_reward(self, step_rewards):\n",
    "        nonadv_done_n = []\n",
    "        adv_rew = 0.0\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            if i not in self.adv_indices:\n",
    "                # zero-sum reward between adversaries and non-adversaries\n",
    "                nonadv_done_n.append(agent.done)\n",
    "                adv_rew -= step_rewards[i]\n",
    "        nonadv_done = all(nonadv_done_n)\n",
    "\n",
    "        timeout = (self.step_count >= self.max_steps)\n",
    "\n",
    "        # normalized distance-to-goal to range [0, 1]\n",
    "        ndis_to_goal = [dis_func(agent.pos, self.goal_pos, k=self.max_dis)\n",
    "                        for agent in self.agents]\n",
    "\n",
    "        if self.team_reward_type == 'const':\n",
    "            # give constant team reward to non-adversaries\n",
    "            if nonadv_done:\n",
    "                team_rwd = self.team_reward_multiplier\n",
    "                for i, a in enumerate(self.agents):\n",
    "                    if i not in self.adv_indices:\n",
    "                        a.reward(team_rwd)\n",
    "                        step_rewards[i] += team_rwd\n",
    "\n",
    "                        # keep zero-sum reward between\n",
    "                        # adversaries and non-adversaries\n",
    "                        adv_rew -= team_rwd\n",
    "        else:\n",
    "            # no team reward\n",
    "            pass\n",
    "\n",
    "        if len(self.adv_indices) > 0:\n",
    "            adv_rew /= len(self.adv_indices)\n",
    "            for i in self.adv_indices:\n",
    "                step_rewards[i] += adv_rew\n",
    "        return timeout, nonadv_done, step_rewards, ndis_to_goal\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        obs_dict, rew_dict, _, info_dict = MultiGridEnv.step(self, action_dict)\n",
    "        if self.active_after_done:\n",
    "            done_n = [agent.at_pos(self.goal_pos) for agent in self.agents]\n",
    "        else:\n",
    "            done_n = [agent.done for agent in self.agents]\n",
    "\n",
    "        step_rewards = rew_dict['step_rewards']\n",
    "        env_rewards = rew_dict['env_rewards']\n",
    "        comm_rewards = rew_dict['comm_rewards']\n",
    "        comm_strs = info_dict['comm_strs']\n",
    "\n",
    "        timeout, nonadv_done, step_rewards, ndis_to_goal = self.update_reward(\n",
    "            step_rewards)\n",
    "\n",
    "        # The episode overall is done if ALL non-adversarial agents are done,\n",
    "        # or if it exceeds the step limit.\n",
    "        done = timeout or nonadv_done\n",
    "        if self.debug:\n",
    "            done = any(done_n)\n",
    "\n",
    "        step_rewards += comm_rewards\n",
    "\n",
    "        rew_dict = {f'agent_{i}': step_rewards[i] for i in range(\n",
    "            len(step_rewards))}\n",
    "        done_dict = {'__all__': done}\n",
    "        info_dict = {f'agent_{i}': {\n",
    "            'done': done_n[i],\n",
    "            'comm': self.agents[i].comm,\n",
    "            'nonadv_done': nonadv_done,\n",
    "            'posd': np.array([self.agents[i].pos[0], self.agents[i].pos[1],\n",
    "                              done_n[i]]),\n",
    "            'sees_goal': self.sees_goal[i],\n",
    "            'comm_str': comm_strs[i],\n",
    "        } for i in range(len(done_n))}\n",
    "\n",
    "        info_dict['rew_by_act'] = {\n",
    "            # env reward\n",
    "            0: {f'agent_{i}': env_rewards[i] for i in range(len(env_rewards))},\n",
    "\n",
    "            # designed comm reward\n",
    "            'comm': {f'agent_{i}': comm_rewards[i] for i in range(len(\n",
    "                comm_rewards))},\n",
    "        }\n",
    "\n",
    "        # team reward\n",
    "        if self.separate_rew_more:\n",
    "            info_dict['rew_by_act'][1] = {f'agent_{i}': (\n",
    "                    step_rewards[i] - env_rewards[i]) for i in range(\n",
    "                len(step_rewards))}\n",
    "        else:\n",
    "            info_dict['rew_by_act'][1] = {f'agent_{i}': (\n",
    "                step_rewards[i]) for i in range(len(step_rewards))}\n",
    "\n",
    "        obs_dict['global'] = self.gen_global_obs()\n",
    "        return obs_dict, rew_dict, done_dict, info_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8534b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_goal(self:FindGoalMultiGrid, agent, goal_pos, direction=None):\n",
    "    \"\"\"\n",
    "    Generate the observation the agent would see one step BEFORE reaching the goal.\n",
    "    \n",
    "    Args:\n",
    "        agent: The agent object\n",
    "        goal_pos: np.arrray[(gx, gy)] of goal position\n",
    "        direction: Optional specific direction to approach from (0=right, 1=down, 2=left, 3=up)\n",
    "                   If None, tries all 4 directions and returns the first valid one\n",
    "    \n",
    "    Returns:\n",
    "        obs: The observation image, or None if goal is unreachable\n",
    "        approach_dir: The direction used to approach the goal\n",
    "    \"\"\"\n",
    "    # Unwrap environment if needed\n",
    "    # print(type(self))\n",
    "    if hasattr(self, 'env'):\n",
    "        env = self.env\n",
    "    else:\n",
    "        env = self\n",
    "    \n",
    "    gx, gy = goal_pos[0], goal_pos[1]\n",
    "    \n",
    "    # Save original agent state\n",
    "    old_pos = agent.pos.copy() if agent.pos is not None else None\n",
    "    old_dir = agent.dir\n",
    "    \n",
    "    # Direction vectors: 0=right, 1=down, 2=left, 3=up\n",
    "    dir_vecs = {\n",
    "        0: np.array([1, 0]),   # right\n",
    "        1: np.array([0, 1]),   # down\n",
    "        2: np.array([-1, 0]),  # left\n",
    "        3: np.array([0, -1]),  # up\n",
    "    }\n",
    "    \n",
    "    # If direction specified, try only that one; otherwise try all 4\n",
    "    directions_to_try = [direction] if direction is not None else [0, 1, 2, 3]\n",
    "    \n",
    "    for try_dir in directions_to_try:\n",
    "        # Compute position one step before goal when approaching from this direction\n",
    "        # If agent is at pre_pos facing try_dir, moving forward reaches the goal\n",
    "        fv = dir_vecs[try_dir]\n",
    "        goal_pre_pos = np.array([gx, gy]) - fv\n",
    "        \n",
    "        # Check if this position is valid (inside grid)\n",
    "        if (goal_pre_pos[0] < 0 or goal_pre_pos[0] >= env.width or\n",
    "            goal_pre_pos[1] < 0 or goal_pre_pos[1] >= env.height):\n",
    "            continue\n",
    "        \n",
    "        # Check if this position is walkable (not a wall)\n",
    "        cell_at_pre_pos = env.grid.get(*goal_pre_pos)\n",
    "        if cell_at_pre_pos is not None and not cell_at_pre_pos.can_overlap():\n",
    "            continue  # Can't stand here (wall or blocking object)\n",
    "        \n",
    "        # Valid position found! Temporarily place agent here\n",
    "        try:\n",
    "            # Remove agent from old position (if it was placed)\n",
    "            if old_pos is not None:\n",
    "                old_cell = env.grid.get(*old_pos)\n",
    "                if old_cell == agent:\n",
    "                    env.grid.set(*old_pos, None)\n",
    "                elif old_cell is not None and hasattr(old_cell, 'agents'):\n",
    "                    if agent in old_cell.agents:\n",
    "                        old_cell.agents.remove(agent)\n",
    "            \n",
    "            # Place agent at pre-goal position facing the goal\n",
    "            agent.pos = goal_pre_pos\n",
    "            agent.dir = try_dir\n",
    "            \n",
    "            # Handle if there's already something at this position\n",
    "            if cell_at_pre_pos is not None and cell_at_pre_pos.can_overlap():\n",
    "                # Temporarily add agent to this cell's agent list\n",
    "                if not hasattr(cell_at_pre_pos, 'agents'):\n",
    "                    cell_at_pre_pos.agents = []\n",
    "                cell_at_pre_pos.agents.append(agent)\n",
    "                placed_in_agents = True\n",
    "            else:\n",
    "                # Place agent directly\n",
    "                env.grid.set(*goal_pre_pos, agent)\n",
    "                placed_in_agents = False\n",
    "            \n",
    "            # Generate observation\n",
    "            obs = env.gen_agent_obs(agent, image_only=True)\n",
    "            \n",
    "            # Restore agent to original position\n",
    "            if placed_in_agents:\n",
    "                cell_at_pre_pos.agents.remove(agent)\n",
    "            else:\n",
    "                env.grid.set(*goal_pre_pos, None)\n",
    "            \n",
    "            if old_pos is not None:\n",
    "                env.grid.set(*old_pos, agent)\n",
    "                agent.pos = old_pos\n",
    "            else:\n",
    "                agent.pos = None\n",
    "            agent.dir = old_dir\n",
    "            \n",
    "            return obs, try_dir\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Restore state on error\n",
    "            if old_pos is not None:\n",
    "                env.grid.set(*old_pos, agent)\n",
    "                agent.pos = old_pos\n",
    "            else:\n",
    "                agent.pos = None\n",
    "            agent.dir = old_dir\n",
    "            raise e\n",
    "    \n",
    "    # No valid approach direction found\n",
    "    print(f\"Warning: Goal at ({gx}, {gy}) is unreachable - surrounded by walls\")\n",
    "    return None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111e84e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import cv2\n",
    "@patch\n",
    "def get_layout(self: FindGoalMultiGrid, video_scale = 8, render_kwargs={}):\n",
    "    for agent in self.agents:\n",
    "        agent.active = False\n",
    "    layout = self.render(mode=\"rgb_array\", show_more=True, show_agent_views= False,\n",
    "                                        **render_kwargs)\n",
    "\n",
    "    if isinstance(layout, list) or len(layout.shape) > 3:\n",
    "        layout = layout[0]\n",
    "\n",
    "    if video_scale != 1:\n",
    "        layout = cv2.resize(layout, None,\n",
    "                                fx=video_scale,\n",
    "                                fy=video_scale,\n",
    "                                interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    for agent in self.agents:\n",
    "        agent.active = True\n",
    "    return layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309ce416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
