{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b4bdaba",
   "metadata": {},
   "source": [
    "# Vision Encoder model\n",
    "\n",
    "> CNN for image feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b1ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b4c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f9d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributions as td\n",
    "import torch.nn as nn\n",
    "from fastcore.utils import *\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2364280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "ENCODER_LAYERS_CONFIG = {\n",
    "    # L1\n",
    "    \"a\": [(2, 32, 5, 1, 0), (32, 32, 4, 2, 0), (32, 32, 3, 1, 0), (32, 16, 1, 1, 0)],\n",
    "    \"b\": [(2, 16, 5, 1, 0), (16, 32, 4, 2, 0), (32, 32, 3, 1, 0), (32, 16, 1, 1, 0)],\n",
    "    \"c\": [(2, 16, 5, 1, 0), (16, 16, 4, 2, 0), (16, 16, 3, 1, 0)],\n",
    "    \"f\": [(2, 16, 5, 1, 0), (16, 16, 5, 2, 0), (16, 16, 5, 1, 2)],\n",
    "    \"g\": [(2, 32, 5, 1, 0), (32, 32, 5, 2, 0), (32, 32, 5, 1, 2), (32, 16, 1, 1, 0)],\n",
    "    \"h\": [(2, 16, 5, 1, 0), (16, 16, 5, 2, 0), (16, 16, 3, 1, 0)],\n",
    "    \"i\": [(2, 16, 5, 1, 0), (16, 16, 5, 2, 0), (16, 16, 3, 1, 1)],\n",
    "    \"i_fc\": [\n",
    "        (2, 16, 5, 1, 0),\n",
    "        (16, 16, 5, 2, 0),\n",
    "        (16, 16, 3, 1, 1),\n",
    "        (\"fc\", 13456, 512),\n",
    "    ],\n",
    "    \"i_b\": [(6, 16, 5, 1, 0), (16, 16, 5, 2, 0), (16, 16, 3, 1, 0)],\n",
    "    \"d4rl_a\": [\n",
    "        (6, 16, 5, 1, 0),\n",
    "        (16, 32, 5, 2, 0),\n",
    "        (32, 32, 3, 1, 0),\n",
    "        (32, 32, 3, 1, 1),\n",
    "        (32, 16, 1, 1, 0),\n",
    "    ],\n",
    "    \"d4rl_b\": [\n",
    "        (6, 16, 5, 1, 0),\n",
    "        (16, 32, 5, 2, 0),\n",
    "        (32, 32, 3, 1, 0),\n",
    "        (32, 32, 3, 1, 1),\n",
    "        (32, 32, 3, 1, 1),\n",
    "        (32, 16, 1, 1, 0),\n",
    "    ],\n",
    "    \"d4rl_c\": [\n",
    "        (6, 16, 5, 1, 0),\n",
    "        (16, 32, 5, 2, 0),\n",
    "        (32, 32, 3, 1, 0),\n",
    "        (32, 32, 3, 1, 1),\n",
    "        (32, 32, 3, 1, 1),\n",
    "    ],\n",
    "    \"j\": [(2, 32, 5, 1, 0), (32, 32, 5, 2, 0), (32, 32, 3, 1, 1), (32, 16, 1, 1, 0)],\n",
    "    \"k\": [(2, 16, 5, 1, 0), (16, 32, 5, 2, 0), (32, 32, 3, 1, 1), (32, 16, 1, 1, 0)],\n",
    "    # L2\n",
    "    \"d\": [(16, 16, 3, 1, 0), (16, 16, 3, 1, 0)],\n",
    "    \"e\": [\n",
    "        (\"pad\", (0, 1, 0, 1)),\n",
    "        (16, 16, 3, 1, 0),\n",
    "        (\"avg_pool\", 2, 2, 0),\n",
    "        (16, 16, 3, 1, 0),\n",
    "    ],\n",
    "    \"l2a\": [(16, 16, 5, 1, 2), (16, 16, 5, 2, 2), (16, 16, 3, 1, 1)],  # (8, 16, 15, 15)\n",
    "    \"l2b\": [(16, 16, 3, 1, 1), (16, 16, 3, 2, 1), (16, 16, 3, 1, 1)],  # (8, 16, 15, 15)\n",
    "    \"l2c\": [(16, 32, 5, 1, 2), (32, 32, 5, 2, 2), (32, 32, 3, 1, 1)],  # (8, 32, 15, 15)\n",
    "    \"l2d\": [(16, 32, 3, 1, 1), (32, 32, 3, 2, 1), (32, 32, 3, 1, 1)],  # (8, 32, 15, 15)\n",
    "    \"l2e\": [(16, 16, 3, 2, 1), (16, 16, 3, 1, 1)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e05351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 16, 5, 1, 0),\n",
       " (16, 32, 5, 2, 0),\n",
       " (32, 32, 3, 1, 0),\n",
       " (32, 32, 3, 1, 1),\n",
       " (32, 16, 1, 1, 0)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "ENCODER_LAYERS_CONFIG['d4rl_a']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cc3e16",
   "metadata": {},
   "source": [
    "### Other Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PassThrough(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f77e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# from torch import nn\n",
    "\n",
    "# class SequenceBackbone(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         \"\"\"\n",
    "#         collapse T and BS dimensions prior to passing to backbone\n",
    "#         afterwards reshape to original shape\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.output_position_dim = 0\n",
    "\n",
    "#     def _remove_pos_component_for_spatial(self, embeddings):\n",
    "#         \"\"\"\n",
    "#         remove the position component from spatial embeddings\n",
    "\n",
    "#         Input:\n",
    "#             embeddings: tensor\n",
    "#             (T, BS, Ch, W, H) or\n",
    "#             (BS, Ch, W, H) or\n",
    "#             (T, BS, H) or\n",
    "#             (BS, H)\n",
    "#         \"\"\"\n",
    "#         og_shape = tuple(embeddings.shape)\n",
    "#         flattened_input = len(og_shape) < 4\n",
    "\n",
    "#         # first reshape to spatial dimension if needed\n",
    "#         if flattened_input:\n",
    "#             spatial_shape = (*embeddings.shape[:-1], *self.output_dim)\n",
    "#             embeddings = embeddings.view(spatial_shape)\n",
    "\n",
    "#         position_channels = self.output_position_dim[0]\n",
    "\n",
    "#         # remove the position dimensions\n",
    "#         if len(embeddings.shape) == 5:\n",
    "#             embeddings = embeddings[:, :, :-position_channels]\n",
    "#         elif len(embeddings.shape) == 4:\n",
    "#             embeddings = embeddings[:, :-position_channels]\n",
    "\n",
    "#         # reflatten tensor if needed\n",
    "#         if flattened_input:\n",
    "#             embeddings = embeddings.view(*og_shape[:-1], -1)\n",
    "\n",
    "#         return embeddings\n",
    "\n",
    "#     def remove_pos_component(self, embeddings):\n",
    "#         \"\"\"\n",
    "#         remove the position component from embeddings\n",
    "#         Input:\n",
    "#             embeddings: tensor\n",
    "#             (T, BS, Ch, W, H) or\n",
    "#             (BS, Ch, W, H) or\n",
    "#             (T, BS, H) or\n",
    "#             (BS, H)\n",
    "#         \"\"\"\n",
    "#         if not self.output_position_dim:\n",
    "#             return embeddings\n",
    "\n",
    "#         if isinstance(self.output_dim, int):\n",
    "#             return embeddings[..., : -self.output_position_dim]\n",
    "#         else:\n",
    "#             return self._remove_pos_component_for_spatial(embeddings)\n",
    "\n",
    "#     def forward_multiple(self, x, position=None):\n",
    "#         \"\"\"\n",
    "#         input:\n",
    "#             x: [T, BS, *] or [BS, T, *]\n",
    "#         output:\n",
    "#             x: [T, BS, *] or [T, BS, *]\n",
    "#         \"\"\"\n",
    "\n",
    "#         # if no time dimension, just feed it directly to backbone\n",
    "#         if x.dim() == 2 or x.dim() == 4:\n",
    "#             if position is not None:\n",
    "#                 output = self.forward(x, position)\n",
    "#             else:\n",
    "#                 output = self.forward(x)\n",
    "#             return output\n",
    "\n",
    "#         state = x.flatten(0, 1)\n",
    "#         if position is not None:\n",
    "#             position = position.flatten(0, 1)\n",
    "#             output = self.forward(state, position)\n",
    "            \n",
    "#         else:\n",
    "#             output = self.forward(state)\n",
    "\n",
    "#         state = output.encodings\n",
    "#         new_shape = x.shape[:2] + state.shape[1:]\n",
    "#         state = state.reshape(new_shape)\n",
    "\n",
    "        \n",
    "#         return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab1181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# from mawm.models.vision.enums import BackboneOutput\n",
    "# class MLPNet(nn.Module):\n",
    "#     def __init__(self, output_dim: int = 64):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(28 * 28, output_dim)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(output_dim, output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = x.flatten(1)\n",
    "#         out = self.fc1(out)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.fc2(out)\n",
    "#         out = BackboneOutput(encodings=out)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e55886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# from mawm.models.vision.enums import BackboneOutput\n",
    "# from mawm.models.vision.base import SequenceBackbone\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class MeNet5(SequenceBackbone):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         input_dim: int,\n",
    "#         output_dim: int = 64,\n",
    "#         input_channels: int = 1,\n",
    "#         width_factor: int = 1,\n",
    "#         conv_out_dim: int = 9 * 32,\n",
    "#         backbone_norm: str = \"batch_norm\",\n",
    "#         backbone_pool: str = \"backbone_pool\",\n",
    "#         backbone_final_fc: bool = True,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.width_factor = width_factor\n",
    "#         self.conv_out_dim = conv_out_dim\n",
    "#         self.backbone_final_fc = backbone_final_fc\n",
    "#         self.layer1 = nn.Sequential(\n",
    "#             nn.Conv2d(\n",
    "#                 input_channels, 16 * width_factor, kernel_size=5, stride=2, padding=2\n",
    "#             ),\n",
    "#             nn.ReLU(),\n",
    "#             (\n",
    "#                 nn.BatchNorm2d(16 * width_factor)\n",
    "#                 if backbone_norm == \"batch_norm\"\n",
    "#                 else nn.GroupNorm(4, 16 * width_factor)\n",
    "#             ),\n",
    "#             nn.Conv2d(\n",
    "#                 16 * width_factor, 32 * width_factor, kernel_size=5, stride=2, padding=2\n",
    "#             ),\n",
    "#             nn.ReLU(),\n",
    "#             (\n",
    "#                 nn.BatchNorm2d(32 * width_factor)\n",
    "#                 if backbone_norm == \"batch_norm\"\n",
    "#                 else nn.GroupNorm(4, 32 * width_factor)\n",
    "#             ),\n",
    "#             nn.Conv2d(\n",
    "#                 32 * width_factor, 32 * width_factor, kernel_size=3, stride=1, padding=1\n",
    "#             ),\n",
    "#             nn.ReLU(),\n",
    "#             (\n",
    "#                 nn.BatchNorm2d(32 * width_factor)\n",
    "#                 if backbone_norm == \"batch_norm\"\n",
    "#                 else nn.GroupNorm(4, 32 * width_factor)\n",
    "#             ),\n",
    "#         )\n",
    "#         if backbone_pool == \"avg_pool\":\n",
    "#             self.pool = nn.AvgPool2d(2, stride=2)\n",
    "#         else:\n",
    "#             self.pool = nn.Conv2d(\n",
    "#                 in_channels=32 * width_factor, out_channels=32, kernel_size=1\n",
    "#             )\n",
    "#         sample_input = torch.randn(input_dim).unsqueeze(0)\n",
    "#         sample_output = self.pool(self.layer1(sample_input)).reshape(1, -1)\n",
    "#         if backbone_final_fc:\n",
    "#             self.fc = nn.Linear(sample_output.shape[-1], output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.layer1(x)  # [bs,64,16,16]\n",
    "#         out = self.pool(out)  # [bs, 32, 16, 16]\n",
    "#         if self.backbone_final_fc:\n",
    "#             out = out.reshape(out.size(0), -1)\n",
    "#             out = self.fc(out)\n",
    "#         out = BackboneOutput(encodings=out)\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77425d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# class ResizeConv2d(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         in_channels,\n",
    "#         out_channels,\n",
    "#         kernel_size,\n",
    "#         scale_factor,\n",
    "#         mode=\"nearest\",\n",
    "#         groups=1,\n",
    "#         bias=False,\n",
    "#         padding=1,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.scale_factor = scale_factor\n",
    "#         self.mode = mode\n",
    "#         self.conv = nn.Conv2d(\n",
    "#             in_channels,\n",
    "#             out_channels,\n",
    "#             kernel_size,\n",
    "#             stride=1,\n",
    "#             padding=padding,\n",
    "#             groups=groups,\n",
    "#             bias=bias,\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n",
    "#         x = self.conv(x)\n",
    "#         x = BackboneOutput(encodings=x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e197269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# class Canonical(nn.Module):\n",
    "#     def __init__(self, output_dim: int = 64):\n",
    "#         super().__init__()\n",
    "#         res = int(np.sqrt(output_dim / 64))\n",
    "#         assert (\n",
    "#             res * res * 64 == output_dim\n",
    "#         ), \"canonical backbone resolution error: cant fit desired output_dim\"\n",
    "\n",
    "#         self.backbone = nn.Sequential(\n",
    "#             nn.Conv2d(1, 32, 4, stride=2, padding=0),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(32, 64, 4, stride=2, padding=0),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(64, 64, 3, stride=1, padding=0),\n",
    "#             nn.ReLU(),\n",
    "#             nn.AdaptiveAvgPool2d((res, res)),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.backbone(x).flatten(1)\n",
    "#         x = BackboneOutput(encodings=x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75292fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# from mawm.models.misc import  (\n",
    "#     build_mlp,\n",
    "#     Projector,\n",
    "#     MLP,\n",
    "#     build_norm1d,\n",
    "#     PartialAffineLayerNorm,\n",
    "# )\n",
    "\n",
    "# class MLPEncoder(SequenceBackbone):\n",
    "#     def __init__(self, cfg, input_dim):\n",
    "#         super().__init__()\n",
    "#         self.encoder = MLP(\n",
    "#             arch=cfg.backbone_subclass,\n",
    "#             input_dim=input_dim,\n",
    "#             norm=cfg.backbone_norm,\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.encoder(x)\n",
    "#         x = BackboneOutput(encodings=x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f8a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# class ObposEncoder1(SequenceBackbone):\n",
    "#     \"\"\"\n",
    "#     Fused encoder for observation and pos state.\n",
    "#     cat(obs, pos) --> encoder --> encodings\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, config, obs_dim):\n",
    "#         super().__init__()\n",
    "#         self.config = config\n",
    "\n",
    "#         self.encoder = MLP(\n",
    "#             arch=config.backbone_subclass,\n",
    "#             input_dim=obs_dim + config.pos_dim,\n",
    "#             norm=config.backbone_norm,\n",
    "#             activation=\"mish\",\n",
    "#         )\n",
    "\n",
    "#         out_dim = int(config.backbone_subclass.split(\"-\")[-1])\n",
    "\n",
    "#         if config.final_ln:\n",
    "#             self.final_ln = build_norm1d(config.backbone_norm, out_dim)\n",
    "#         else:\n",
    "#             self.final_ln = nn.Identity()\n",
    "\n",
    "#     def forward(self, obs, pos):\n",
    "#         x = torch.cat([obs, pos], dim=-1)\n",
    "#         x = self.encoder(x)\n",
    "#         x = self.final_ln(x)\n",
    "\n",
    "#         return BackboneOutput(encodings=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a4854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# class ObposEncoder2(SequenceBackbone):\n",
    "#     \"\"\"\n",
    "#     Distangled encoder for observation and pos state.\n",
    "#     obs --> obs_encoder --> obs_out\n",
    "#     pos --> pos_encoder --> pos_out\n",
    "#     encodings = cat(obs_out, pos_out)\n",
    "#     return: encodings, obs_out, pos_out\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, config, obs_dim):\n",
    "#         super().__init__()\n",
    "#         self.config = config\n",
    "\n",
    "#         obs_subclass, pos_subclass = config.backbone_subclass.split(\",\")\n",
    "\n",
    "#         if obs_subclass == \"id\":\n",
    "#             self.obs_encoder = nn.Identity()\n",
    "#             obs_out_dim = obs_dim\n",
    "#         else:\n",
    "#             self.obs_encoder = build_mlp(\n",
    "#                 layers_dims=obs_subclass,\n",
    "#                 input_dim=obs_dim,\n",
    "#                 norm=config.backbone_norm,\n",
    "#                 activation=\"mish\",\n",
    "#             )\n",
    "#             obs_out_dim = int(obs_subclass.split(\"-\")[-1])\n",
    "\n",
    "#         if pos_subclass == \"id\":\n",
    "#             self.pos_encoder = nn.Identity()\n",
    "#             pos_out_dim = config.pos_dim\n",
    "#         else:\n",
    "#             self.pos_encoder = build_mlp(\n",
    "#                 layers_dims=pos_subclass,\n",
    "#                 input_dim=config.pos_dim,\n",
    "#                 norm=config.backbone_norm,\n",
    "#                 activation=\"mish\",\n",
    "#             )\n",
    "#             pos_out_dim = int(pos_subclass.split(\"-\")[-1])\n",
    "\n",
    "#         if config.final_ln:\n",
    "#             self.final_ln = PartialAffineLayerNorm(\n",
    "#                 first_dim=obs_out_dim,\n",
    "#                 second_dim=pos_out_dim,\n",
    "#                 first_affine=(obs_subclass != \"id\"),\n",
    "#                 second_affine=(pos_subclass != \"id\"),\n",
    "#             )\n",
    "#         else:\n",
    "#             self.final_ln = nn.Identity()\n",
    "\n",
    "#     def forward(self, obs, pos):\n",
    "#         obs_out = self.obs_encoder(obs)\n",
    "#         pos_out = self.pos_encoder(pos)\n",
    "\n",
    "#         next_state = torch.cat([obs_out, pos_out], dim=1)\n",
    "#         next_state = self.final_ln(next_state)\n",
    "\n",
    "#         return BackboneOutput(\n",
    "#             encodings=next_state,\n",
    "#             obs_component=obs_out,\n",
    "#             pos_component=pos_out,\n",
    "#         )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aefd08f",
   "metadata": {},
   "source": [
    "### Our Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d806fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from mawm.models.utils import Expander2D, build_conv\n",
    "\n",
    "class GridMLP(nn.Module):\n",
    "    def __init__(self, grid_width= 15, grid_height= 15, embed_dim= 8, w= 15, h= 15):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.x_embeddings = nn.Embedding(grid_width, embed_dim)\n",
    "        self.y_embeddings = nn.Embedding(grid_height, embed_dim)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            Expander2D(w, h)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input Shape:  [T, B, C, H, W]\n",
    "        Output Shape: [T, B, C`, H`, W`]\n",
    "        \"\"\"\n",
    "\n",
    "        if x.dim() == 3:\n",
    "            x_coords, y_coords = x[:,:, 0], x[:,:, 1]\n",
    "        else:\n",
    "            x_coords, y_coords = x[:, 0], x[:, 1]\n",
    "            \n",
    "        x_vec = self.x_embeddings(x_coords)\n",
    "        y_vec = self.y_embeddings(y_coords)\n",
    "        \n",
    "        combined = torch.cat([x_vec, y_vec], dim=-1)\n",
    "        return self.mlp(combined)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658961b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 17, 16, 15, 15])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "T= 17\n",
    "B= 32\n",
    "model = GridMLP(grid_width=15, grid_height=15, embed_dim=8, w= 15, h= 15)\n",
    "inp_pos = torch.randint(0, 15, (B, T, 2))  # Example x and y coordinates\n",
    "model_output = model(inp_pos)\n",
    "model_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e968fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from mawm.models.utils import Expander2D, build_conv\n",
    "\n",
    "class MeNet6(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        input_dim: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.input_dim = input_dim\n",
    "        subclass = config.backbone_subclass\n",
    "        layers_config = ENCODER_LAYERS_CONFIG[subclass]\n",
    "\n",
    "        if \"l2\" in subclass:\n",
    "            # add prenormalization and relu layers?\n",
    "            pre_conv = nn.Sequential(nn.GroupNorm(4, layers_config[0][0]), nn.ReLU())\n",
    "        else:\n",
    "            pre_conv = nn.Identity()\n",
    "        conv_layers = build_conv(layers_config, (input_dim[0],))\n",
    "\n",
    "        self.layers = nn.Sequential(pre_conv, conv_layers)\n",
    "\n",
    "        if config.position_dim:\n",
    "            # infer output dim of encoder\n",
    "            sample_input = torch.randn(input_dim).unsqueeze(0) # [1, C, H, W]\n",
    "            sample_output = self.layers(sample_input)\n",
    "            encoder_output_dim = tuple(sample_output.shape[1:])\n",
    "\n",
    "            if (\n",
    "                self.config.position_encoder_arch\n",
    "                and self.config.position_encoder_arch != \"id\"\n",
    "            ):\n",
    "                self.position_encoder = GridMLP(grid_width= 15, grid_height= 15, embed_dim= 8,\n",
    "                                               w=encoder_output_dim[-2], h=encoder_output_dim[-1])\n",
    "            else:\n",
    "                self.position_encoder = Expander2D(\n",
    "                    w=encoder_output_dim[-2], h=encoder_output_dim[-1]\n",
    "                )\n",
    "        \n",
    "    @property\n",
    "    def repr_dim(self):\n",
    "        with torch.no_grad():\n",
    "            sample_inp = torch.randn(self.input_dim).unsqueeze(0)  # [1, 1, C, H, W]\n",
    "            sample_out = self.layers(sample_inp).unsqueeze(0)\n",
    "            encoder_output_dim = tuple(sample_out.shape[1:])\n",
    "\n",
    "            if self.config.position_dim:\n",
    "                sample_pos = torch.randint(0, 15, (1, self.config.position_dim)).unsqueeze(0)\n",
    "                sample_out_pos = self.position_encoder(sample_pos)\n",
    "                pos_enc_output_dim = tuple(sample_out_pos.shape[1:])\n",
    "                t = ([ax1 + ax2 for ax1, ax2 in zip(encoder_output_dim[1:2], pos_enc_output_dim[1:2])][0],\n",
    "                     *encoder_output_dim[2:])\n",
    "                return t\n",
    "                \n",
    "            return encoder_output_dim\n",
    "    \n",
    "    def forward(self, x, position=None):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            x: [T, BS, *] or [BS, T, *]\n",
    "        output:\n",
    "            x: [T, BS, *] or [T, BS, *]\n",
    "        \"\"\"\n",
    "        time= False\n",
    "        if x.dim() == 2 or x.dim() == 4:\n",
    "            encoded_obs = self.layers(x) # # [BS, C, H, W]\n",
    "        else:\n",
    "            time= True\n",
    "            obs = x.flatten(0, 1) # [T*BS, C, H, W]\n",
    "            encoded_obs = self.layers(obs) # [T*BS, 16, 10, 10]\n",
    "\n",
    "        if position is not None:\n",
    "            if position.dim() == 3:\n",
    "                position = position.flatten(0, 1)  # [T*BS, pos_dim]\n",
    "            encoded_pos = self.position_encoder(position)  # [T*BS, pos_dim, H, W]\n",
    "            z = torch.cat([encoded_obs, encoded_pos], dim=1)\n",
    "        else:\n",
    "            encoded_pos = None\n",
    "            z = encoded_obs\n",
    "            \n",
    "        if time:\n",
    "            new_shape = x.shape[:2] + z.shape[1:]\n",
    "            z = z.reshape(new_shape)\n",
    "            return z\n",
    "        return z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b3ab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from omegaconf import OmegaConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e262888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "cfg = OmegaConf.load(\"../cfgs/MPCJepa/mpc.yaml\")\n",
    "\n",
    "cfg.model.backbone.position_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c668905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "model = MeNet6(\n",
    "    config=cfg.model.backbone,\n",
    "    input_dim=(3, cfg.model.img_size, cfg.model.img_size),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93139f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridMLP(\n",
       "  (x_embeddings): Embedding(15, 8)\n",
       "  (y_embeddings): Embedding(15, 8)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): Expander2D()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "model.position_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac9f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "T = 8\n",
    "BS = 16\n",
    "inp = torch.randn(BS, T, 3, 42, 42)\n",
    "inp_pos = torch.randint(0, 15, (BS, T, 2))  # Example x and y coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725062b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8, 32, 15, 15])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "out = model(inp, position=inp_pos)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf170bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16, 15, 15) (1, 16, 15, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 15, 15)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## | hide\n",
    "model.repr_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3db35f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
