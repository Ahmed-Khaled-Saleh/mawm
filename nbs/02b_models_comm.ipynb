{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Communication Module\n",
    "\n",
    "> Model architecture that handles communicaion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import *\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CommunicationModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Handles Agent J's message output. \n",
    "    1. During training, outputs the continuous message context (h_t^j).\n",
    "    2. During execution, outputs the hard-sampled discrete symbol index (m_hard).\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim: int, message_length: int, vocab_size: int, embed_dim: int, ffn_dim: int, num_heads: int = 4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state_dim (int): Dimension of the encoded observation (z_t^j).\n",
    "            message_length (int): Length of the discrete message sequence L.\n",
    "            vocab_size (int): The number of unique symbolic tokens in the vocabulary |V|.\n",
    "            embed_dim (int): The dimension of the continuous symbol embeddings (also the K/V input dim).\n",
    "            ffn_dim (int): Dimension of the intermediate Feed-Forward Network layer.\n",
    "            num_heads (int): Number of attention heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.message_length = message_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ffn_dim = ffn_dim\n",
    "        \n",
    "        # --- 1. Symbol Embedding Matrix (E) ---\n",
    "        self.symbol_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # --- 2. Multi-Head Attention (MHA) ---\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.total_attention_dim = self.head_dim * num_heads \n",
    "        \n",
    "        if self.total_attention_dim != embed_dim:\n",
    "             raise ValueError(\"embed_dim must be divisible by num_heads.\")\n",
    "\n",
    "        # Q, K, V Projections\n",
    "        self.query_proj = nn.Linear(state_dim, self.total_attention_dim, bias=False) # Q from z_t^j\n",
    "        self.key_proj = nn.Linear(embed_dim, self.total_attention_dim, bias=False)  # K from message embedding\n",
    "        self.value_proj = nn.Linear(embed_dim, self.total_attention_dim, bias=False) # V from message embedding\n",
    "        self.output_proj = nn.Linear(self.total_attention_dim, self.total_attention_dim) # MHA output\n",
    "\n",
    "        # --- 3. Feed-Forward Network (FFN) ---\n",
    "        # Processes the MHA context vector\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(self.total_attention_dim, ffn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ffn_dim, embed_dim) # Output dimension matches the K/V dim for potential stacking, but here is just the input to the Logit Head\n",
    "        )\n",
    "        \n",
    "        # --- 4. Prediction Head (Logits h_t^j) ---\n",
    "        self.logit_proj = nn.Linear(embed_dim, vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _split_heads(self: CommunicationModule, x: torch.Tensor):\n",
    "        \"\"\" Reshape (B, D) to (B, H, D_h) \"\"\"\n",
    "        new_shape = x.size()[:-1] + (self.num_heads, self.head_dim)\n",
    "        x = x.view(*new_shape)\n",
    "        return x.unsqueeze(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _combine_heads(self: CommunicationModule, x: torch.Tensor):\n",
    "        \"\"\" Reshape (B, H, 1, D_h) back to (B, D) \"\"\"\n",
    "        x = x.squeeze(2) \n",
    "        new_shape = x.size()[:-2] + (self.total_attention_dim,)\n",
    "        return x.view(*new_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _multi_head_attention(self: CommunicationModule, Q_input: torch.Tensor, KV_input: torch.Tensor):\n",
    "        \"\"\" Computes MHA where Q is the state and K/V is the message context. \"\"\"\n",
    "\n",
    "        Q_proj = self.query_proj(Q_input)\n",
    "        K_proj = self.key_proj(KV_input)\n",
    "        V_proj = self.value_proj(KV_input)\n",
    "\n",
    "        Q = self._split_heads(Q_proj)\n",
    "        K = self._split_heads(K_proj)\n",
    "        V = self._split_heads(V_proj)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1) \n",
    "        context_head_output = torch.matmul(weights, V)\n",
    "\n",
    "        context_vector_combined = self._combine_heads(context_head_output)\n",
    "        context_vector = self.output_proj(context_vector_combined)\n",
    "\n",
    "        return context_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([71, 18, 85, 93, 17, 44, 66, 97, 63, 17]) torch.int64 torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import torch\n",
    "vocab_size = 100\n",
    "L = 10\n",
    "inp = torch.randint(1, vocab_size, (L,))\n",
    "print(inp, inp.dtype, inp.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.Size([10, 32])\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "embed_size = 32\n",
    "embedding_layer = nn.Embedding(vocab_size, embed_size)\n",
    "embedded_inp = embedding_layer(inp)\n",
    "print(embedded_inp.dtype, embedded_inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def embed_discrete_message(self: CommunicationModule, m_j: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Embeds the discrete message sequence m_j into a single continuous vector (mean). \"\"\"\n",
    "        embedded_tokens = self.symbol_embeddings(m_j.long())\n",
    "        # Averaging the embeddings across the message length L \n",
    "        return embedded_tokens.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def forward(self: CommunicationModule, \n",
    "            z_t_j: torch.Tensor, \n",
    "            m_prev_j: torch.Tensor, \n",
    "            is_training: bool):\n",
    "    \"\"\"\n",
    "    Calculates the Logits based on the current state and previous message.\n",
    "\n",
    "    Args:\n",
    "        z_t_j (torch.Tensor): Agent J's current state. Shape: (Batch, state_dim)\n",
    "        m_prev_j (torch.Tensor): Agent J's discrete message from t-1. Shape: (Batch, message_length)\n",
    "        is_training (bool): Flag to determine output mode.\n",
    "\n",
    "    Returns:\n",
    "        - If is_training=True: \n",
    "            - torch.Tensor (h_t_j_logits), Continuous logits. Shape: (Batch, vocab_size)\n",
    "        - If is_training=False: \n",
    "            - torch.Tensor (m_t_j), New discrete message sequence. Shape: (Batch, message_length)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Embed Previous Message (m_{t-1}^j)\n",
    "    prev_message_embed = self.embed_discrete_message(m_prev_j) # (B, embed_dim)\n",
    "    \n",
    "    # 2. Multi-Head Cross-Attention (Context Vector)\n",
    "    context_vector = self._multi_head_attention(\n",
    "        Q_input=z_t_j, \n",
    "        KV_input=prev_message_embed\n",
    "    )\n",
    "    print(context_vector.shape)\n",
    "    # 3. Feed-Forward Processing\n",
    "    ffn_output = self.feed_forward(context_vector)\n",
    "    print(ffn_output.shape)\n",
    "\n",
    "    # 4. Logit Prediction (h_t^j)\n",
    "    h_t_j_logits = self.logit_proj(ffn_output) # (B, vocab_size)\n",
    "\n",
    "    # 5. Output Determination\n",
    "    if is_training:\n",
    "        # --- TRAINING PATH: Send Logits ---\n",
    "        # Agent I receives these continuous logits for its loss calculation.\n",
    "        return h_t_j_logits\n",
    "    else:\n",
    "        # --- EXECUTION PATH: Update Discrete Message Sequence ---\n",
    "        \n",
    "        # Predict the next symbol index (Argmax/Hard Decoding)\n",
    "        predicted_token = torch.argmax(h_t_j_logits, dim=-1).unsqueeze(1) # (B, 1)\n",
    "\n",
    "        # Drop the first symbol (oldest) and append the predicted token\n",
    "        m_truncated = m_prev_j[:, 1:] # (B, L-1)\n",
    "        m_t_j = torch.cat([m_truncated, predicted_token], dim=1) # (B, L)\n",
    "        \n",
    "        # The message sent is the full discrete sequence m_t^j\n",
    "        return m_t_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Agent J Non-Recurrent MHA Communication Module Initialization ---\n",
      "\n",
      "=============================================\n",
      "SIMULATION: TRAINING STEP (Logits Output)\n",
      "=============================================\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4, 32])\n",
      "Input message m_prev_j (symbols): [4, 2, 7]\n",
      "Output Message (Logits) shape: torch.Size([4, 10])\n",
      "Expected Logit Dimension: 10. Matches: True\n",
      "\n",
      "=============================================\n",
      "SIMULATION: EXECUTION STEP (Discrete Message Update)\n",
      "=============================================\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4, 32])\n",
      "Output to Agent I: New discrete message m_t^j (symbols): [2, 7, 9]\n",
      "Verification: Message shifted and new token appended.\n",
      "Old first token: 4\n",
      "New first token: 2\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define Dimensions\n",
    "    BATCH_SIZE = 4\n",
    "    STATE_DIM = 64\n",
    "    MESSAGE_LENGTH = 3 \n",
    "    VOCAB_SIZE = 10    \n",
    "    EMBED_DIM = 32     \n",
    "    FFN_DIM = 128\n",
    "    NUM_HEADS = 4\n",
    "\n",
    "    print(f\"--- Agent J Non-Recurrent MHA Communication Module Initialization ---\")\n",
    "    comm_module_j = CommunicationModule(\n",
    "        STATE_DIM, MESSAGE_LENGTH, VOCAB_SIZE, EMBED_DIM, FFN_DIM, NUM_HEADS\n",
    "    )\n",
    "\n",
    "    # Mock Inputs \n",
    "    mock_z_t_j = torch.randn(BATCH_SIZE, STATE_DIM)\n",
    "    m_t_minus_1_j = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, MESSAGE_LENGTH)).long()\n",
    "    \n",
    "    # ====================================================================\n",
    "    #                  SIMULATION: TRAINING STEP\n",
    "    # ====================================================================\n",
    "    print(\"\\n=============================================\")\n",
    "    print(\"SIMULATION: TRAINING STEP (Logits Output)\")\n",
    "    print(\"=============================================\")\n",
    "    \n",
    "    h_t_j_logits = comm_module_j(\n",
    "        z_t_j=mock_z_t_j, \n",
    "        m_prev_j=m_t_minus_1_j, \n",
    "        is_training=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Input message m_prev_j (symbols): {m_t_minus_1_j[0].tolist()}\")\n",
    "    print(f\"Output Message (Logits) shape: {h_t_j_logits.shape}\")\n",
    "    print(f\"Expected Logit Dimension: {VOCAB_SIZE}. Matches: {h_t_j_logits.shape[1] == VOCAB_SIZE}\")\n",
    "    \n",
    "    # ====================================================================\n",
    "    #                  SIMULATION: EXECUTION STEP\n",
    "    # ====================================================================\n",
    "    print(\"\\n=============================================\")\n",
    "    print(\"SIMULATION: EXECUTION STEP (Discrete Message Update)\")\n",
    "    print(\"=============================================\")\n",
    "    \n",
    "    m_t_j = comm_module_j(\n",
    "        z_t_j=mock_z_t_j, \n",
    "        m_prev_j=m_t_minus_1_j, \n",
    "        is_training=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Output to Agent I: New discrete message m_t^j (symbols): {m_t_j[0].tolist()}\")\n",
    "    print(f\"Verification: Message shifted and new token appended.\")\n",
    "    print(f\"Old first token: {m_t_minus_1_j[0, 0]}\")\n",
    "    print(f\"New first token: {m_t_j[0, 0]}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
