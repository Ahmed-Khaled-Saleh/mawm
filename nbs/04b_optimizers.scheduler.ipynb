{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities for Optimization\n",
    "\n",
    "> This module handles all aspects of the world model, including state representation, environment dynamics, and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp optimizers.schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import *\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from functools import partial\n",
    "from torch.optim import Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Learning Rate Scheduler Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "class Scheduler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        schedule: str,\n",
    "        base_lr: float,\n",
    "        data_loader,\n",
    "        epochs: int,\n",
    "        optimizer,\n",
    "        batch_steps=None,\n",
    "        batch_size=None,\n",
    "    ):\n",
    "        self.schedule = schedule\n",
    "        self.base_lr = base_lr\n",
    "        self.data_loader = data_loader\n",
    "        self.epochs = epochs\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if batch_size is None:\n",
    "            self.batch_size = data_loader.config.batch_size\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "        if batch_steps is None:\n",
    "            self.batch_steps = len(data_loader)\n",
    "        else:\n",
    "            self.batch_steps = batch_steps\n",
    "\n",
    "    def adjust_learning_rate(self, step: int):\n",
    "        if self.schedule == \"constant\":\n",
    "            return self.base_lr\n",
    "        else:\n",
    "            max_steps = self.epochs * self.batch_steps\n",
    "            warmup_steps = int(0.10 * max_steps)\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                base_lr = (\n",
    "                    param_group[\"base_lr\"] if \"base_lr\" in param_group else self.base_lr\n",
    "                )\n",
    "                base_lr = base_lr * self.batch_size / 256\n",
    "                if step < warmup_steps:\n",
    "                    lr = base_lr * step / warmup_steps\n",
    "                else:\n",
    "                    step -= warmup_steps\n",
    "                    max_steps -= warmup_steps\n",
    "                    q = 0.5 * (1 + math.cos(math.pi * step / max_steps))\n",
    "                    end_lr = base_lr * 0.001\n",
    "                    lr = base_lr * q + end_lr * (1 - q)\n",
    "                param_group[\"lr\"] = lr\n",
    "            return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "cfg = OmegaConf.load(\"../cfgs/MPCJepa/mpc.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from mawm.data.utils import init_data\n",
    "cfg.data.data_dir = \"./data_test/\"\n",
    "dl, _ = init_data(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CellEmpty': 0, 'CellObstacle': 1, 'CellItem': 2, 'CellGoal': 3, 'CellAgent': 4, 'GoalAt': 5, 'ItemAt': 6, 'Near': 7, 'SeeGoal': 8, 'CanMove': 9, 'OtherAgentAt': 10, 'OtherAgentNear': 11, 'OtherAgentDirection': 12}\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "from mawm.models.jepa import JEPA\n",
    "\n",
    "model = JEPA(cfg.model, input_dim=(3, 42, 42), action_dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from mawm.optimizers.utils import init_opt\n",
    "optimizer = init_opt(cfg, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "scheduler = Scheduler(\n",
    "        schedule=cfg.optimizer.scheduler.name,\n",
    "        base_lr=cfg.optimizer.lr,\n",
    "        data_loader=dl,\n",
    "        epochs=cfg.epochs,\n",
    "        optimizer=optimizer,\n",
    "        batch_size=cfg.data.batch_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V-JEPA schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math\n",
    "\n",
    "\n",
    "class WSDSchedule(object):\n",
    "\n",
    "    def __init__(self, optimizer, warmup_steps, anneal_steps, T_max, start_lr, ref_lr, final_lr=0.0):\n",
    "        self.optimizer = optimizer\n",
    "        self.start_lr = start_lr\n",
    "        self.ref_lr = ref_lr\n",
    "        self.final_lr = final_lr\n",
    "        self.anneal_steps = anneal_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.T_max = T_max - warmup_steps - anneal_steps\n",
    "        self._step = 0.0\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        if self._step < self.warmup_steps:\n",
    "            progress = float(self._step) / float(max(1, self.warmup_steps))\n",
    "            new_lr = self.start_lr + progress * (self.ref_lr - self.start_lr)\n",
    "        elif self._step < self.T_max + self.warmup_steps:\n",
    "            new_lr = self.ref_lr\n",
    "        else:\n",
    "            _step = self._step - (self.T_max + self.warmup_steps)\n",
    "            progress = float(_step) / float(max(1, self.anneal_steps))\n",
    "            new_lr = self.ref_lr + progress * (self.final_lr - self.ref_lr)\n",
    "\n",
    "        for group in self.optimizer.param_groups:\n",
    "            group[\"lr\"] = new_lr\n",
    "            if \"lr_scale\" in group:\n",
    "                group[\"lr\"] *= group[\"lr_scale\"]\n",
    "\n",
    "        return new_lr\n",
    "\n",
    "\n",
    "class WarmupCosineSchedule(object):\n",
    "\n",
    "    def __init__(self, optimizer, warmup_steps, start_lr, ref_lr, T_max, last_epoch=-1, final_lr=0.0):\n",
    "        self.optimizer = optimizer\n",
    "        self.start_lr = start_lr\n",
    "        self.ref_lr = ref_lr\n",
    "        self.final_lr = final_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.T_max = T_max - warmup_steps\n",
    "        self._step = 0.0\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        if self._step < self.warmup_steps:\n",
    "            progress = float(self._step) / float(max(1, self.warmup_steps))\n",
    "            new_lr = self.start_lr + progress * (self.ref_lr - self.start_lr)\n",
    "        else:\n",
    "            # -- progress after warmup\n",
    "            progress = float(self._step - self.warmup_steps) / float(max(1, self.T_max))\n",
    "            new_lr = max(\n",
    "                self.final_lr,\n",
    "                self.final_lr + (self.ref_lr - self.final_lr) * 0.5 * (1.0 + math.cos(math.pi * progress)),\n",
    "            )\n",
    "\n",
    "        for group in self.optimizer.param_groups:\n",
    "            group[\"lr\"] = new_lr\n",
    "\n",
    "        return new_lr\n",
    "\n",
    "\n",
    "class CosineWDSchedule(object):\n",
    "\n",
    "    def __init__(self, optimizer, ref_wd, T_max, final_wd=0.0):\n",
    "        self.optimizer = optimizer\n",
    "        self.ref_wd = ref_wd\n",
    "        self.final_wd = final_wd\n",
    "        self.T_max = T_max\n",
    "        self._step = 0.0\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        progress = self._step / self.T_max\n",
    "        new_wd = self.final_wd + (self.ref_wd - self.final_wd) * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "        if self.final_wd <= self.ref_wd:\n",
    "            new_wd = max(self.final_wd, new_wd)\n",
    "        else:\n",
    "            new_wd = min(self.final_wd, new_wd)\n",
    "\n",
    "        for group in self.optimizer.param_groups:\n",
    "            if (\"WD_exclude\" not in group) or not group[\"WD_exclude\"]:\n",
    "                group[\"weight_decay\"] = new_wd\n",
    "        return new_wd\n",
    "\n",
    "\n",
    "class LinearDecaySchedule(object):\n",
    "\n",
    "    def __init__(self, optimizer, ref_lr, T_max, last_epoch=-1, final_lr=0.0):\n",
    "        self.optimizer = optimizer\n",
    "        self.ref_lr = ref_lr\n",
    "        self.final_lr = final_lr\n",
    "        self.T_max = T_max\n",
    "        self._step = 0.0\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        progress = float(self._step) / float(max(1, self.T_max))\n",
    "        new_lr = self.ref_lr + progress * (self.final_lr - self.ref_lr)\n",
    "        for group in self.optimizer.param_groups:\n",
    "            group[\"lr\"] = new_lr\n",
    "\n",
    "        return new_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
