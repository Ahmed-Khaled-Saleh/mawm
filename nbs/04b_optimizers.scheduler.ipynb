{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities for Optimization\n",
    "\n",
    "> This module handles all aspects of the world model, including state representation, environment dynamics, and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp optimizers.schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import *\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from functools import partial\n",
    "from torch.optim import Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Learning Rate Scheduler Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import math\n",
    "class Scheduler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        schedule: str,\n",
    "        base_lr: float,\n",
    "        data_loader,\n",
    "        epochs: int,\n",
    "        optimizer,\n",
    "        batch_steps=None,\n",
    "        batch_size=None,\n",
    "    ):\n",
    "        self.schedule = schedule\n",
    "        self.base_lr = base_lr\n",
    "        self.data_loader = data_loader\n",
    "        self.epochs = epochs\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if batch_size is None:\n",
    "            self.batch_size = data_loader.config.batch_size\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "        if batch_steps is None:\n",
    "            self.batch_steps = len(data_loader)\n",
    "        else:\n",
    "            self.batch_steps = batch_steps\n",
    "\n",
    "    # def adjust_learning_rate(self, step: int):\n",
    "    #     if self.schedule == \"constant\":\n",
    "    #         return self.base_lr\n",
    "    #     else:\n",
    "    #         max_steps = self.epochs * self.batch_steps\n",
    "    #         warmup_steps = int(0.10 * max_steps)\n",
    "    #         for param_group in self.optimizer.param_groups:\n",
    "    #             base_lr = (\n",
    "    #                 param_group[\"base_lr\"] if \"base_lr\" in param_group else self.base_lr\n",
    "    #             )\n",
    "    #             base_lr = base_lr * self.batch_size / 256\n",
    "    #             if step < warmup_steps:\n",
    "    #                 lr = base_lr * step / warmup_steps\n",
    "    #             else:\n",
    "    #                 step -= warmup_steps\n",
    "    #                 max_steps -= warmup_steps\n",
    "    #                 q = 0.5 * (1 + math.cos(math.pi * step / max_steps))\n",
    "    #                 end_lr = base_lr * 0.001\n",
    "    #                 lr = base_lr * q + end_lr * (1 - q)\n",
    "    #             param_group[\"lr\"] = lr\n",
    "    #         return lr\n",
    "\n",
    "    # def adjust_learning_rate(self, step: int):\n",
    "    #     if self.schedule == \"constant\":\n",
    "    #         return self.base_lr\n",
    "        \n",
    "    #     # 1. Calculate totals outside the loop\n",
    "    #     total_max_steps = self.epochs * self.batch_steps\n",
    "    #     warmup_steps = int(0.10 * total_max_steps)\n",
    "        \n",
    "    #     # 2. Prevent division by zero if total_max_steps is very small\n",
    "    #     if total_max_steps == warmup_steps:\n",
    "    #         warmup_steps = max(1, warmup_steps - 1)\n",
    "\n",
    "    #     for param_group in self.optimizer.param_groups:\n",
    "    #         base_lr = param_group.get(\"base_lr\", self.base_lr)\n",
    "    #         # Scaling LR by batch size (Linear Scaling Rule)\n",
    "    #         # base_lr = base_lr * self.batch_size / 256\n",
    "            \n",
    "    #         if step < warmup_steps:\n",
    "    #             lr = base_lr * step / warmup_steps\n",
    "    #         else:\n",
    "    #             # Use local variables for the decay calculation \n",
    "    #             # to avoid modifying the outer scope variables\n",
    "    #             current_decay_step = step - warmup_steps\n",
    "    #             decay_period = total_max_steps - warmup_steps\n",
    "                \n",
    "    #             # Ensure we don't divide by zero if epochs are too low\n",
    "    #             if decay_period <= 0:\n",
    "    #                 lr = base_lr\n",
    "    #             else:\n",
    "    #                 q = 0.5 * (1 + math.cos(math.pi * current_decay_step / decay_period))\n",
    "    #                 end_lr = base_lr * 0.001\n",
    "    #                 lr = base_lr * q + end_lr * (1 - q)\n",
    "            \n",
    "    #         param_group[\"lr\"] = lr\n",
    "    #     return lr\n",
    "    \n",
    "    def adjust_learning_rate(self, step: int):\n",
    "        if self.schedule == \"constant\":\n",
    "            return self.base_lr\n",
    "        \n",
    "        total_max_steps = self.epochs * self.batch_steps\n",
    "        warmup_steps = int(0.10 * total_max_steps)\n",
    "        \n",
    "        if total_max_steps == warmup_steps:\n",
    "            warmup_steps = max(1, warmup_steps - 1)\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            # Get the ORIGINAL base_lr for this group\n",
    "            if 'base_lr' not in param_group:\n",
    "                param_group['base_lr'] = param_group['lr']  # Store initial LR\n",
    "            \n",
    "            group_base_lr = param_group['base_lr']\n",
    "            \n",
    "            # Apply warmup/cosine schedule to this group's base LR\n",
    "            if step < warmup_steps:\n",
    "                lr = group_base_lr * step / warmup_steps\n",
    "            else:\n",
    "                current_decay_step = step - warmup_steps\n",
    "                decay_period = total_max_steps - warmup_steps\n",
    "                \n",
    "                if decay_period <= 0:\n",
    "                    lr = group_base_lr\n",
    "                else:\n",
    "                    q = 0.5 * (1 + math.cos(math.pi * current_decay_step / decay_period))\n",
    "                    end_lr = group_base_lr * 0.001\n",
    "                    lr = group_base_lr * q + end_lr * (1 - q)\n",
    "            \n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "            # Log LR for different groups\n",
    "            if step % 100 == 0:\n",
    "                group_name = param_group.get('name', 'unnamed')\n",
    "                print(f\"Step {step}, Group {group_name}: LR = {lr:.6e}\")\n",
    "        \n",
    "        # Return the base LR (not group-specific)\n",
    "        return self.base_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "cfg = OmegaConf.load(\"../cfgs/findgoal/mawm/main/mawm-seq-40.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path found for hostname: local\n",
      "Using all 10 rollouts in dataset.\n",
      "Using all 10 rollouts in dataset.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "from mawm.data.utils import init_data\n",
    "dl, _ = init_data(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:JEPA Parameters: 98560\n",
      "INFO:root:CommModule Parameters: 56005\n",
      "INFO:root:MSgEncoder Parameters: 32608\n",
      "INFO:root:Projector Parameters: 2241536\n",
      "INFO:root:--------------------------------------------------\n",
      "INFO:root:Total Parameters: 2462245\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "from mawm.models import init_models\n",
    "from omegaconf import OmegaConf\n",
    "cfg = OmegaConf.load(\"../cfgs/MPCJepa/mpc.yaml\")\n",
    "\n",
    "model = init_models(cfg, \"cpu\", distributed= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from mawm.optimizers.utils import init_opt\n",
    "optimizer = init_opt(cfg, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "scheduler = Scheduler(\n",
    "        schedule=cfg.optimizer.scheduler.name,\n",
    "        base_lr=cfg.optimizer.lr,\n",
    "        data_loader=dl,\n",
    "        epochs=cfg.epochs,\n",
    "        optimizer=optimizer,\n",
    "        batch_size=cfg.data.batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000001e-07\n",
      "Step 300, Group jepa: LR = 1.000000e-07\n",
      "Step 300, Group encoders: LR = 1.500000e-07\n",
      "Step 300, Group comm_module: LR = 2.000000e-07\n",
      "Step 300, Group proj: LR = 1.500000e-07\n",
      "1.0000000000000001e-07\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "epoch = 300\n",
    "global_step = epoch * len(dl) + 0\n",
    "print(optimizer.param_groups[0][\"lr\"])\n",
    "lr = scheduler.adjust_learning_rate(global_step)\n",
    "print(optimizer.param_groups[0][\"lr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V-JEPA schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math\n",
    "\n",
    "\n",
    "class WSDSchedule(object):\n",
    "\n",
    "    def __init__(self, optimizer, warmup_steps, anneal_steps, T_max, start_lr, ref_lr, final_lr=0.0):\n",
    "        self.optimizer = optimizer\n",
    "        self.start_lr = start_lr\n",
    "        self.ref_lr = ref_lr\n",
    "        self.final_lr = final_lr\n",
    "        self.anneal_steps = anneal_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.T_max = T_max - warmup_steps - anneal_steps\n",
    "        self._step = 0.0\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        if self._step < self.warmup_steps:\n",
    "            progress = float(self._step) / float(max(1, self.warmup_steps))\n",
    "            new_lr = self.start_lr + progress * (self.ref_lr - self.start_lr)\n",
    "        elif self._step < self.T_max + self.warmup_steps:\n",
    "            new_lr = self.ref_lr\n",
    "        else:\n",
    "            _step = self._step - (self.T_max + self.warmup_steps)\n",
    "            progress = float(_step) / float(max(1, self.anneal_steps))\n",
    "            new_lr = self.ref_lr + progress * (self.final_lr - self.ref_lr)\n",
    "\n",
    "        for group in self.optimizer.param_groups:\n",
    "            group[\"lr\"] = new_lr\n",
    "            if \"lr_scale\" in group:\n",
    "                group[\"lr\"] *= group[\"lr_scale\"]\n",
    "\n",
    "        return new_lr\n",
    "\n",
    "\n",
    "class WarmupCosineSchedule(object):\n",
    "\n",
    "    def __init__(self, optimizer, warmup_steps, start_lr, ref_lr, T_max, last_epoch=-1, final_lr=0.0):\n",
    "        self.optimizer = optimizer\n",
    "        self.start_lr = start_lr\n",
    "        self.ref_lr = ref_lr\n",
    "        self.final_lr = final_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.T_max = T_max - warmup_steps\n",
    "        self._step = 0.0\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        if self._step < self.warmup_steps:\n",
    "            progress = float(self._step) / float(max(1, self.warmup_steps))\n",
    "            new_lr = self.start_lr + progress * (self.ref_lr - self.start_lr)\n",
    "        else:\n",
    "            # -- progress after warmup\n",
    "            progress = float(self._step - self.warmup_steps) / float(max(1, self.T_max))\n",
    "            new_lr = max(\n",
    "                self.final_lr,\n",
    "                self.final_lr + (self.ref_lr - self.final_lr) * 0.5 * (1.0 + math.cos(math.pi * progress)),\n",
    "            )\n",
    "\n",
    "        for group in self.optimizer.param_groups:\n",
    "            group[\"lr\"] = new_lr\n",
    "\n",
    "        return new_lr\n",
    "\n",
    "\n",
    "class CosineWDSchedule(object):\n",
    "\n",
    "    def __init__(self, optimizer, ref_wd, T_max, final_wd=0.0):\n",
    "        self.optimizer = optimizer\n",
    "        self.ref_wd = ref_wd\n",
    "        self.final_wd = final_wd\n",
    "        self.T_max = T_max\n",
    "        self._step = 0.0\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        progress = self._step / self.T_max\n",
    "        new_wd = self.final_wd + (self.ref_wd - self.final_wd) * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "        if self.final_wd <= self.ref_wd:\n",
    "            new_wd = max(self.final_wd, new_wd)\n",
    "        else:\n",
    "            new_wd = min(self.final_wd, new_wd)\n",
    "\n",
    "        for group in self.optimizer.param_groups:\n",
    "            if (\"WD_exclude\" not in group) or not group[\"WD_exclude\"]:\n",
    "                group[\"weight_decay\"] = new_wd\n",
    "        return new_wd\n",
    "\n",
    "\n",
    "class LinearDecaySchedule(object):\n",
    "\n",
    "    def __init__(self, optimizer, ref_lr, T_max, last_epoch=-1, final_lr=0.0):\n",
    "        self.optimizer = optimizer\n",
    "        self.ref_lr = ref_lr\n",
    "        self.final_lr = final_lr\n",
    "        self.T_max = T_max\n",
    "        self._step = 0.0\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        progress = float(self._step) / float(max(1, self.T_max))\n",
    "        new_lr = self.ref_lr + progress * (self.final_lr - self.ref_lr)\n",
    "        for group in self.optimizer.param_groups:\n",
    "            group[\"lr\"] = new_lr\n",
    "\n",
    "        return new_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
