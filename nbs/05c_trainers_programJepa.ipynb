{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb656fc",
   "metadata": {},
   "source": [
    "# Program Jepa trainer\n",
    "\n",
    "> This module implements Program Jepa training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bbb56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp trainers.trainer_progjepa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd36cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import *\n",
    "from fastcore.utils import *\n",
    "from torchvision.utils import save_image\n",
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# import torch\n",
    "# from torchvision.transforms import v2\n",
    "# train_tf = v2.Compose(\n",
    "#     [\n",
    "#         # Keep the size at 42 to match your model architecture\n",
    "#         v2.ToPILImage(),\n",
    "#         # v2.RandomResizedCrop(42, scale=(0.8, 1.0)), \n",
    "#         # v2.RandomApply([v2.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
    "#         # v2.RandomGrayscale(p=0.2),\n",
    "#         # Reduced kernel size for smaller image resolution\n",
    "#         # v2.RandomApply([v2.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))], p=0.1),\n",
    "#         # v2.RandomHorizontalFlip(),\n",
    "#         v2.ToImage(),\n",
    "#         v2.ToDtype(torch.float32, scale=True),\n",
    "#         # Normalizes to [-1, 1] to match Tanh output\n",
    "#         v2.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35e878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading file buffer ...: 100%|██████████| 10/10 \n"
     ]
    }
   ],
   "source": [
    "# #| hide\n",
    "# from MAWM.data.loaders import RolloutObservationDataset\n",
    "\n",
    "# from torchvision import transforms\n",
    "# import numpy as np\n",
    "\n",
    "# ASIZE, LSIZE, RSIZE, RED_SIZE, SIZE =\\\n",
    "#     3, 32, 256, 32, 40\n",
    "\n",
    "# dataset = RolloutObservationDataset(\n",
    "#     agent='agent_0',\n",
    "#     root='../marl_grid_data/',\n",
    "#     transform=train_tf,\n",
    "#     buffer_size=10,\n",
    "#     train=True\n",
    "# )\n",
    "# dataset.load_next_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee784bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #| hide\n",
    "# import matplotlib.pyplot as plt\n",
    "# def denormalize(tensor):\n",
    "#     # If using mean=0.5, std=0.5:\n",
    "#     return tensor * 0.5 + 0.5\n",
    "    \n",
    "#     # If using ImageNet stats (your current code):\n",
    "#     # mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "#     # std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "#     # return tensor * std + mean\n",
    "\n",
    "# # Visualization\n",
    "# img_tensor = dataset[0][0]\n",
    "# img_to_show = denormalize(img_tensor)\n",
    "\n",
    "# # Clip just in case of tiny floating point errors to satisfy plt.imshow\n",
    "# # img_to_show = torch.clamp(img_to_show, 0, 1)\n",
    "\n",
    "# plt.imshow(img_to_show.permute(1, 2, 0).numpy())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cdce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "# batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a043c2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 42, 42])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29780c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs, dones, agent_id = batch\n",
    "# mask = ~dones.bool()     # keep only where done is False\n",
    "\n",
    "# if mask.sum() == 0:\n",
    "#     print(\"yes\")  # entire batch is terminals\n",
    "\n",
    "# obs = obs[mask]          # filter observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fedc339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 3, 42, 42])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3714e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import torch\n",
    "class SIGReg(torch.nn.Module):\n",
    "    def __init__(self, knots=17):\n",
    "        super().__init__()\n",
    "        t = torch.linspace(0, 3, knots, dtype=torch.float32)\n",
    "        dt = 3 / (knots - 1)\n",
    "        weights = torch.full((knots,), 2 * dt, dtype=torch.float32)\n",
    "        weights[[0, -1]] = dt\n",
    "        window = torch.exp(-t.square() / 2.0)\n",
    "        self.register_buffer(\"t\", t)\n",
    "        self.register_buffer(\"phi\", window)\n",
    "        self.register_buffer(\"weights\", weights * window)\n",
    "\n",
    "    def forward(self, proj):\n",
    "        A = torch.randn(proj.size(-1), 256, device=\"cpu\")\n",
    "        A = A.div_(A.norm(p=2, dim=0))\n",
    "        x_t = (proj @ A).unsqueeze(-1) * self.t\n",
    "        err = (x_t.cos().mean(-3) - self.phi).square() + x_t.sin().mean(-3).square()\n",
    "        statistic = (err @ self.weights) * proj.size(-2)\n",
    "        return statistic.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49875f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# import numpy as np\n",
    "# from MAWM.program.creator import create_specs_from_image, batchify_programs\n",
    "# # from MAWM.optimizer.losses import SIGReg\n",
    "# sigreg = SIGReg()\n",
    "\n",
    "# programs = [create_specs_from_image(denormalize(img).permute(1, 2, 0).numpy()) for img in obs]\n",
    "# batch_prim_ids, batch_param_tensor = batchify_programs(programs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962f7f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([11, 49]), torch.Size([11, 49, 2]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_prim_ids.shape, batch_param_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6e6630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# from MAWM.models.encoder import ResNet18\n",
    "# v_encoder = ResNet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bec1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# from MAWM.core import *\n",
    "# num_primitives = len(PRIMITIVE_TEMPLATES) + 1  # +1 for padding primitive\n",
    "# grid_size = 7\n",
    "# from MAWM.models.program_encoder import ProgramEncoder\n",
    "# p_encoder = ProgramEncoder(num_primitives, [grid_size, grid_size],2, seq_len=49)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbdfebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 3, 42, 42])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #| hide\n",
    "# obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38255bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #| hide\n",
    "# img_proj = v_encoder(obs)\n",
    "# img_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56781188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #| hide\n",
    "# prog_proj = p_encoder(batch_prim_ids, batch_param_tensor)\n",
    "# prog_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1399105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.4533, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #| hide\n",
    "# sigreg(img_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# sigreg(prog_proj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f6ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# sigreg_loss = sigreg(img_proj) + sigreg(prog_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec25d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# inv_loss = (img_proj.mean(0) - prog_proj).square().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81068e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2671, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #| hide\n",
    "# inv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d00577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256]), torch.Size([11, 256]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #| hide\n",
    "# img_proj.mean(0).shape, prog_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf3dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cb85e38",
   "metadata": {},
   "source": [
    "## Lejepa Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from MAWM.trainers.trainer import Trainer\n",
    "from MAWM.core import *\n",
    "from MAWM.models.utils import save_checkpoint\n",
    "from MAWM.optimizer.losses import SIGReg\n",
    "from MAWM.program.creator import create_specs_from_image, batchify_programs\n",
    "\n",
    "class ProgLejepaTrainer(Trainer):\n",
    "    def __init__(self, cfg, v_encoder, p_encoder, train_loader, val_loader=None, \n",
    "                 criterion=None, optimizer=None,\n",
    "                 device=None, scheduler=None, writer= None):\n",
    "        \n",
    "        self.cfg = cfg\n",
    "        self.v_encoder = v_encoder\n",
    "        self.p_encoder = p_encoder\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.scheduler = scheduler\n",
    "        self.writer = writer\n",
    "        self.sigreg = SIGReg().to(self.device)\n",
    "        self.lambda_ = self.cfg.lambda_\n",
    "\n",
    "        self.prog_lejepa_dir = os.path.join(self.cfg.log_dir, 'prog_lejepa_marlrid')\n",
    "        if not os.path.exists(self.prog_lejepa_dir):\n",
    "            os.mkdir(self.prog_lejepa_dir)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11438888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _sample_idx( L):\n",
    "#         valid_idx = False\n",
    "#         while not valid_idx:\n",
    "#             idx = np.random.randint(0,  if self.full else self.idx - L + 1)\n",
    "#             idxs = np.arange(idx, idx + L) % self.capacity\n",
    "#             valid_idx = not self.idx in idxs[1:] \n",
    "#         return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed8ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def train_epoch(self: ProgLejepaTrainer, epoch):\n",
    "    self.v_encoder.train()\n",
    "    self.p_encoder.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    actual_len = 0\n",
    "\n",
    "    def denormalize(tensor):\n",
    "        return tensor * 0.5 + 0.5\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            self.train_loader.dataset.load_next_buffer()\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "        for batch_idx, data in enumerate(self.train_loader):\n",
    "\n",
    "            obs, dones, agent_id = data\n",
    "            mask = ~dones.bool()     # keep only where done is False\n",
    "\n",
    "            if mask.sum() == 0:\n",
    "                continue  # entire batch is terminals\n",
    "\n",
    "            obs = obs[mask]          # filter observations\n",
    "\n",
    "            programs = [create_specs_from_image(denormalize(img).permute(1, 2, 0).numpy()) for img in obs]\n",
    "            batch_prim_ids, batch_param_tensor = batchify_programs(programs)\n",
    "\n",
    "            batch_prim_ids = batch_prim_ids.to(self.device)\n",
    "            batch_param_tensor = batch_param_tensor.to(self.device)\n",
    "            obs = obs.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            img_proj = self.v_encoder(obs)\n",
    "            prog_proj = self.p_encoder(batch_prim_ids, batch_param_tensor)\n",
    "\n",
    "            sigreg_loss = self.sigreg(img_proj) + self.sigreg(prog_proj)\n",
    "            inv_loss = (img_proj.mean(0) - prog_proj).square().mean()\n",
    "\n",
    "            loss = (1- self.lambda_) * inv_loss + self.lambda_ * sigreg_loss\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            actual_len += len(obs)\n",
    "            \n",
    "            if batch_idx % 20 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(obs), len(self.train_loader.dataset),\n",
    "                    100. * batch_idx / len(self.train_loader),\n",
    "                    loss.item() / len(obs)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / actual_len))\n",
    "\n",
    "    return train_loss / actual_len\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72ef3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def eval_epoch(self: ProgLejepaTrainer):\n",
    "    self.v_encoder.eval()\n",
    "    self.p_encoder.eval()\n",
    "\n",
    "    def denormalize(tensor):\n",
    "        return tensor * 0.5 + 0.5\n",
    "\n",
    "    test_loss = 0\n",
    "    actual_len = 0\n",
    "    while True:\n",
    "        try:\n",
    "            self.val_loader.dataset.load_next_buffer()\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in self.val_loader:\n",
    "                obs, dones, agent_id = data\n",
    "                mask = ~dones.bool()     # keep only where done is False\n",
    "\n",
    "                if mask.sum() == 0:\n",
    "                    continue  # entire batch is terminals\n",
    "\n",
    "                obs = obs[mask]          # filter observations\n",
    "                programs = [create_specs_from_image(denormalize(img).permute(1, 2, 0).numpy()) for img in obs]\n",
    "                batch_prim_ids, batch_param_tensor = batchify_programs(programs)\n",
    "\n",
    "                batch_prim_ids = batch_prim_ids.to(self.device)\n",
    "                batch_param_tensor = batch_param_tensor.to(self.device)\n",
    "                obs = obs.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                img_proj = self.v_encoder(obs)\n",
    "                prog_proj = self.p_encoder(batch_prim_ids, batch_param_tensor)\n",
    "\n",
    "                sigreg_loss = self.sigreg(img_proj) + self.sigreg(prog_proj)\n",
    "                inv_loss = (img_proj.mean(0) - prog_proj).square().mean()\n",
    "                \n",
    "                loss = (1- self.lambda_) * inv_loss + self.lambda_ * sigreg_loss\n",
    "                test_loss += loss.item()\n",
    "                actual_len += obs.size(0)\n",
    "            \n",
    "    test_loss /= actual_len\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1069e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def linear_probing(self: ProgLejepaTrainer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0041e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import wandb\n",
    "\n",
    "@patch\n",
    "def fit(self: ProgLejepaTrainer):\n",
    "    cur_best = None\n",
    "    lst_dfs = []\n",
    "    \n",
    "    for epoch in range(1, self.cfg.epochs + 1):\n",
    "        train_loss = self.train_epoch(epoch)\n",
    "        test_loss = self.eval_epoch()\n",
    "\n",
    "        best_filename = os.path.join(self.prog_lejepa_dir, 'best.pth')\n",
    "        filename = os.path.join(self.prog_lejepa_dir, 'checkpoint.pth')\n",
    "\n",
    "        is_best = not cur_best or test_loss < cur_best\n",
    "        if is_best:\n",
    "            cur_best = test_loss\n",
    "\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'test_loss': test_loss,\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "        }\n",
    "        save_checkpoint(state= state, is_best= is_best, filename= filename, best_filename= best_filename)\n",
    "\n",
    "        to_log = {\n",
    "            \"train_loss\": train_loss, \n",
    "            \"test_loss\": test_loss,\n",
    "        }\n",
    "\n",
    "        self.writer.write(to_log)\n",
    "        df = pd.DataFrame.from_records([{\"epoch\": epoch ,\"train_loss\": train_loss, \"test_loss\":test_loss}], index= \"epoch\")\n",
    "        lst_dfs.append(df)\n",
    "\n",
    "        self.train_loader.dataset.reset_buffer()\n",
    "        self.val_loader.dataset.reset_buffer()\n",
    "\n",
    "    df_res = pd.concat(lst_dfs)\n",
    "    df_reset = df_res.reset_index()\n",
    "    self.writer.write({'Train-Val Loss Table': wandb.Table(dataframe= df_reset)})\n",
    "\n",
    "    self.writer.finish()\n",
    "    return df_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69feff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
