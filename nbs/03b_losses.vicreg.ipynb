{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VicReg Loss Function\n",
    "\n",
    "> As implemented in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp losses.vicreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import *\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from mawm.models.utils import flatten_conv_output\n",
    "from functools import reduce\n",
    "import operator\n",
    "from mawm.models.misc import Projector\n",
    "from einops import rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VICReg(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "        repr_dim: int = (18, 15, 15),\n",
    "        pred_attr: str = \"state\",\n",
    "        name_prefix: str = \"\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if isinstance(repr_dim, tuple):\n",
    "            repr_dim = reduce(operator.mul, repr_dim)\n",
    "        self.cfg = cfg\n",
    "        self.name_prefix = name_prefix\n",
    "        self.pred_attr = pred_attr\n",
    "        # self.projector = Projector(\n",
    "        #     arch=cfg.loss.vicreg.projector,\n",
    "        #     embedding=repr_dim,\n",
    "        #     # random=cfg.random_projector,\n",
    "        # )#.cuda() #TODO: REMOVE\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple losses in the paper:\n",
    "- The similarity loss between the predicted and target representations.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\operatorname{sim}}=\\sum_{k=1}^K \\sum_{t=0}^H \\frac{1}{N} \\sum_{b=0}^N\\left\\|\\hat{Z}_{t, b}^k-Z_{t, b}\\right\\|_2^2\n",
    "$$\n",
    "\n",
    "\n",
    "- The vicReg losses are other three losses:\n",
    "  - Invariance loss.\n",
    "  - Variance loss.\n",
    "  - Covariance loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __call__(self: VICReg, encodings, state_predictions, mask= None):\n",
    "    device = encodings.device\n",
    "    \n",
    "    valid_mask = mask#rearrange(mask, 'b t -> t b')\n",
    "    valid_mask = valid_mask.to(device)\n",
    "    transition_mask = valid_mask[1:] * valid_mask[:-1]# (T-1, B)\n",
    "    \n",
    "\n",
    "    diff = (encodings[1:] - state_predictions[1:]).pow(2).mean(dim=(2, 3, 4)) # (T-1, B)\n",
    "    sim_loss = (diff * transition_mask).sum() / transition_mask.sum().clamp_min(1)\n",
    "\n",
    "    if self.cfg.loss.vicreg.sim_coeff_t:\n",
    "        diff_t = (encodings[1:] - encodings[:-1]).pow(2).mean(dim=(2, 3, 4))# (T-1, B)\n",
    "        sim_loss_t = (diff_t * transition_mask).sum() / transition_mask.sum().clamp_min(1)\n",
    "    else:\n",
    "        sim_loss_t = torch.zeros([1])\n",
    "\n",
    "    flat_encodings = flatten_conv_output(encodings) # [T, B, D]\n",
    "    std_loss = self.std_loss(flat_encodings[:1])\n",
    "\n",
    "    if self.cfg.loss.vicreg.cov_per_feature:\n",
    "        T, B, ch, h, w = encodings.shape\n",
    "        # reshape (1, bs, ch, h, w) --> (h*w, bs, ch)\n",
    "        per_feature_encodings = (\n",
    "            encodings[:1].reshape(1, B, ch, h * w).permute(0, 3, 1, 2).squeeze(0)\n",
    "        )\n",
    "        cov_loss = self.cov_loss(per_feature_encodings)\n",
    "    else:\n",
    "        # reshape (1, bs, ch, h, w) --> (w, bs, ch * h * w)\n",
    "        cov_loss = self.cov_loss(flat_encodings[:1])\n",
    "\n",
    "    # flat_encodings: (T, B, D)\n",
    "    flat_enc = flat_encodings[1:]          # drop t=0\n",
    "    valid = valid_mask[1:]                 # (T-1, B)\n",
    "\n",
    "    # reshape to (B, T-1, D)\n",
    "    flat_enc = flat_enc.permute(1, 0, 2)\n",
    "    valid = valid.permute(1, 0)\n",
    "\n",
    "    std_losses, cov_losses = [], []\n",
    "\n",
    "    for b in range(flat_enc.shape[0]):\n",
    "        idx = valid[b].bool()\n",
    "        if idx.sum() > 1:   # must have at least 2 steps\n",
    "            x = flat_enc[b, idx]   # (T_valid, D)\n",
    "            std_losses.append(self.std_loss(x.unsqueeze(0), across_time=True))\n",
    "            cov_losses.append(self.cov_loss(x.unsqueeze(0), across_time=True))\n",
    "\n",
    "    if len(std_losses) > 0:\n",
    "        std_loss_t = torch.stack(std_losses).mean()\n",
    "        cov_loss_t = torch.stack(cov_losses).mean()\n",
    "    else:\n",
    "        std_loss_t = cov_loss_t = torch.zeros(1, device=flat_enc.device)\n",
    "\n",
    "    total_loss = (\n",
    "        self.cfg.loss.vicreg.sim_coeff * sim_loss\n",
    "        + self.cfg.loss.vicreg.cov_coeff * cov_loss.mean()\n",
    "        + self.cfg.loss.vicreg.std_coeff * std_loss.mean()\n",
    "        + self.cfg.loss.vicreg.cov_coeff_t * cov_loss_t.mean()\n",
    "        + self.cfg.loss.vicreg.std_coeff_t * std_loss_t.mean()\n",
    "        + self.cfg.loss.vicreg.sim_coeff_t * sim_loss_t.mean()\n",
    "    )\n",
    "\n",
    "    losses = {\n",
    "        \"total_loss\": total_loss,\n",
    "        \"sim_loss\": sim_loss,\n",
    "        \"std_loss\": std_loss.mean(),\n",
    "        \"cov_loss\": cov_loss.mean(),\n",
    "        \"sim_loss_t\": sim_loss_t.mean(),\n",
    "        \"std_loss_t\": std_loss_t.mean(),\n",
    "        \"cov_loss_t\": cov_loss_t.mean(),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 512])\n",
      "torch.Size([40, 128, 512])\n",
      "torch.Size([40, 512])\n",
      "torch.Size([40])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "std_margin = 1.0\n",
    "x = torch.randn(40, 128, 512)\n",
    "print(x[:1].shape)\n",
    "x = x - x.mean(dim=1, keepdim=True)\n",
    "print(x.shape)\n",
    "std = torch.sqrt(x.var(dim=1) + 0.0001)\n",
    "print(std.shape)\n",
    "std_loss = torch.mean(F.relu(std_margin- std), dim=-1)\n",
    "print(std_loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def std_loss(self:VICReg, x: torch.Tensor, across_time=False):\n",
    "    x = x - x.mean(dim=1, keepdim=True)  # mean for each dim across batch samples\n",
    "\n",
    "    if (\n",
    "        not across_time\n",
    "        and self.cfg.loss.vicreg.std_coeff\n",
    "        or across_time\n",
    "        and self.cfg.loss.vicreg.std_coeff_t\n",
    "    ):\n",
    "        std = torch.sqrt(x.var(dim=1) + 0.0001)\n",
    "\n",
    "        std_margin = (\n",
    "            self.cfg.loss.vicreg.std_margin_t if across_time else self.cfg.loss.vicreg.std_margin\n",
    "        )\n",
    "        std_loss = torch.mean(F.relu(std_margin - std), dim=-1)\n",
    "    else:\n",
    "        std_loss = torch.zeros([1], device= x.device)\n",
    "\n",
    "    return std_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def cov_loss(self: VICReg, x: torch.Tensor, across_time=False):\n",
    "    batch_size = x.shape[1]\n",
    "    num_features = x.shape[-1]\n",
    "\n",
    "    x = x - x.mean(dim=1, keepdim=True)\n",
    "\n",
    "    if (\n",
    "        not across_time\n",
    "        and self.cfg.loss.vicreg.cov_coeff\n",
    "        or across_time\n",
    "        and self.cfg.loss.vicreg.cov_coeff_t\n",
    "    ):\n",
    "        cov = torch.einsum(\"bki,bkj->bij\", x, x) / (batch_size - 1)\n",
    "        diagonals = torch.einsum(\"bii->bi\", cov).pow(2).sum(dim=-1)\n",
    "        # cov shape is TxDxD\n",
    "\n",
    "        cov_loss = (cov.pow(2).sum(dim=[-1, -2]) - diagonals).div(num_features)\n",
    "        if self.cfg.loss.vicreg.adjust_cov:\n",
    "            cov_loss = cov_loss / (\n",
    "                num_features - 1\n",
    "            )  # divide by num of elements on off-diagonal.\n",
    "            # in orig paper they divide by num_features\n",
    "            # but the correct version is (num_features - 1)*num_features\n",
    "    else:\n",
    "        cov_loss = torch.zeros([1], device= x.device)\n",
    "\n",
    "    return cov_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from omegaconf import OmegaConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from mawm.models.jepa import JEPA\n",
    "cfg = OmegaConf.load(\"../cfgs/MPCJepa/mpc.yaml\")\n",
    "model = JEPA(cfg.model, input_dim=(3, 42, 42), action_dim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import torch\n",
    "B = 16\n",
    "T = 6\n",
    "x = torch.randn(B, T, 3, 42, 42)\n",
    "pos = torch.randint(0, 15, (B, T, 2))\n",
    "\n",
    "actions = torch.randn(B, T, 5)\n",
    "msgs = torch.randn(B, T, 32)\n",
    "z0, Z = model(x, pos=pos, repr_input=False, actions=actions, msgs=msgs, T=actions.size(1) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 16, 32, 15, 15]), torch.Size([6, 16, 32, 15, 15]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "Z.shape, z0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 16, 32, 15, 15])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z0[1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "mask = torch.zeros(T, B)\n",
    "mask[0, 3:] = 1\n",
    "mask[1, 4:] = 1\n",
    "mask[2, 2:] = 1\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = rearrange(mask, 't b -> b t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = z0\n",
    "state_predictions = Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "loss = VICReg(cfg, repr_dim=None, name_prefix=\"JEPA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_loss': tensor(3.0981, grad_fn=<AddBackward0>),\n",
       " 'sim_loss': tensor(0.3333, grad_fn=<DivBackward0>),\n",
       " 'std_loss': tensor(0.8302, grad_fn=<MeanBackward0>),\n",
       " 'cov_loss': tensor(0.0001, grad_fn=<MeanBackward0>),\n",
       " 'sim_loss_t': tensor(0.0833, grad_fn=<MeanBackward0>),\n",
       " 'std_loss_t': tensor(0.8620, grad_fn=<MeanBackward0>),\n",
       " 'cov_loss_t': tensor(0.)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "loss_dict =loss(encodings, state_predictions, mask=mask)\n",
    "loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
