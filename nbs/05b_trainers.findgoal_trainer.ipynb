{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb656fc",
   "metadata": {},
   "source": [
    "# World Model trainer\n",
    "\n",
    "> This module implements LeJepa training procedure with three predictors and two input modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bbb56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp trainers.findgoal_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd36cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import *\n",
    "from fastcore.utils import *\n",
    "from torchvision.utils import save_image\n",
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb85e38",
   "metadata": {},
   "source": [
    "## WorldModel Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from mawm.models.utils import save_checkpoint\n",
    "from mawm.loggers.base import AverageMeter\n",
    "from mawm.losses.sigreg import SIGRegFunctional\n",
    "from mawm.losses.idm import IDMLoss    \n",
    "from mawm.models.utils import flatten_conv_output\n",
    "from einops import rearrange\n",
    "\n",
    "class WMTrainer:\n",
    "    def __init__(self, cfg, model, train_loader, sampler,\n",
    "                 optimizer=None, device=None,earlystopping=None, \n",
    "                 scheduler=None, writer= None, verbose= None, logger= None):\n",
    "        \n",
    "        self.cfg = cfg\n",
    "        self.device = device\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.sampler = sampler\n",
    "\n",
    "        self.jepa = model['rec']['jepa']\n",
    "        self.obs_enc = model[\"send\"][\"obs_enc\"]\n",
    "        self.msg_enc = model[\"send\"][\"msg_enc\"]\n",
    "        self.proj = model[\"send\"][\"proj\"]\n",
    "        self.comm_module = model[\"send\"][\"comm_module\"]\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.earlystopping = earlystopping\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "        self.writer = writer\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "        self.sigreg = SIGRegFunctional().to(self.device)\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        self.idm = IDMLoss(cfg.loss.idm, (32, 15, 15), device= self.device)\n",
    "\n",
    "        self.lambda_ = self.cfg.loss.lambda_\n",
    "        self.W_H_PRED = self.cfg.loss.W_H_PRED\n",
    "        self.W_SIM_T = self.cfg.loss.W_SIM_T\n",
    "\n",
    "        self.schedule_start_epoch = 5  # Start mixing at epoch 5\n",
    "        self.schedule_end_epoch = 20    # Fully use predictions by epoch 20\n",
    "    \n",
    "        self.agents = [f\"agent_{i}\" for i in range(len(self.cfg.env.agents))]\n",
    "\n",
    "        self.dmpc_dir = os.path.join(self.cfg.log_dir, self.cfg.log_subdir, self.cfg.now)\n",
    "        if not os.path.exists(self.dmpc_dir):\n",
    "            os.makedirs(self.dmpc_dir , exist_ok=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c89df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def criterion(self: WMTrainer, global_step, z0, z, actions, msg_target, msg_hat, proj_h, proj_z, mask_t):\n",
    "\n",
    "    flat_encodings = flatten_conv_output(z0) # [T, B, c`, h`, w`] => [T, B, D]\n",
    "    sigreg_img = self.sigreg(flat_encodings, global_step= global_step, mask= mask_t)\n",
    "\n",
    "    transition_mask = mask_t[1:] * mask_t[:-1]\n",
    "    diff = (z0[1:] - z[1:]).pow(2).mean(dim=(2, 3, 4)) # (T-1, B)\n",
    "    sim_loss = (diff * transition_mask).sum() / transition_mask.sum().clamp_min(1)\n",
    "\n",
    "    if self.cfg.loss.vicreg.sim_coeff_t:\n",
    "        diff_t = ( z0[1:] -  z0[:-1]).pow(2).mean(dim=(2, 3, 4))# (T-1, B)\n",
    "        sim_loss_t = (diff_t * transition_mask).sum() / transition_mask.sum().clamp_min(1)\n",
    "    else:\n",
    "        sim_loss_t = torch.zeros([1], device=self.device)\n",
    "    \n",
    "    idm_loss = self.idm(embeddings= z0, predictions= z, actions= actions)\n",
    "    \n",
    "    # SENDER LOSSES\n",
    "    msg_pred_loss = self.cross_entropy(msg_hat.flatten(0,1), msg_target.flatten(0,1)) #msg_hat: [B, T, 5, 7, 7], targe: [B, T, 7, 7] with long() dtype.\n",
    "\n",
    "    sigreg_msg = self.sigreg(proj_h, global_step= global_step, mask= mask_t)\n",
    "    sigreg_obs = self.sigreg(proj_z, global_step= global_step, mask= mask_t)\n",
    "\n",
    "    inv_loss_sender = (proj_z - proj_h).square().mean(dim= -1)  # [T, B, d= 128] => [T, B]\n",
    "    inv_loss_sender = (inv_loss_sender * mask_t).sum() / mask_t.sum().clamp_min(1) \n",
    "\n",
    "    return {\n",
    "        'sigreg_img': sigreg_img,\n",
    "        'sigreg_msg': sigreg_msg,\n",
    "        'sigreg_obs': sigreg_obs,\n",
    "        'sim_loss_dynamics': sim_loss,\n",
    "        'sim_loss_t': sim_loss_t,\n",
    "        'inv_loss_sender': inv_loss_sender,\n",
    "        'msg_pred_loss': msg_pred_loss,\n",
    "        'idm_loss': idm_loss\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c67e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# @patch\n",
    "# def criterion(\n",
    "#     self: WMTrainer,\n",
    "#     global_step,\n",
    "#     z0,            # encoded latents [T, B, C, H, W] (stop-grad)\n",
    "#     z_hat,         # predicted latents [T, B, C, H, W]\n",
    "#     msg_target,\n",
    "#     msg_hat,\n",
    "#     proj_h,\n",
    "#     proj_z,\n",
    "#     mask_t,\n",
    "#     actions=None,          # [T-1, B, A]  (for action-sep)\n",
    "#     anchor_target=None     # optional (dx, dy) or moved flag\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Combined JEPA loss with:\n",
    "#     - delta dynamics (B)\n",
    "#     - action separation (A)\n",
    "#     - weak control anchor (C)\n",
    "#     \"\"\"\n",
    "\n",
    "#     losses = {}\n",
    "\n",
    "#     # ---------------------------------------------------------\n",
    "#     # 1. SIGReg on image / message / obs (UNCHANGED)\n",
    "#     # ---------------------------------------------------------\n",
    "#     flat_encodings = flatten_conv_output(z0)  # [T, B, d]\n",
    "#     losses['sigreg_img'] = self.disSigReg(flat_encodings[:1], global_step)\n",
    "#     losses['sigreg_msg'] = self.disSigReg(proj_h, global_step)\n",
    "#     losses['sigreg_obs'] = self.disSigReg(proj_z, global_step)\n",
    "\n",
    "#     # ---------------------------------------------------------\n",
    "#     # 2. Transition mask\n",
    "#     # ---------------------------------------------------------\n",
    "#     transition_mask = mask_t[1:] * mask_t[:-1]   # [T-1, B]\n",
    "\n",
    "#     # ---------------------------------------------------------\n",
    "#     # 3. DELTA DYNAMICS LOSS (Option B)\n",
    "#     # ---------------------------------------------------------\n",
    "#     delta_hat = z_hat[1:] - z_hat[:-1]\n",
    "#     delta_true = z0[1:] - z0[:-1]\n",
    "\n",
    "#     diff_delta = (delta_hat - delta_true).pow(2).mean(dim=(2, 3, 4))\n",
    "#     losses['sim_loss_dynamics'] = (\n",
    "#         (diff_delta * transition_mask).sum()\n",
    "#         / transition_mask.sum().clamp_min(1)\n",
    "#     )\n",
    "\n",
    "#     # ---------------------------------------------------------\n",
    "#     # 4. GATED TIME-SMOOTHNESS (important!)\n",
    "#     # ---------------------------------------------------------\n",
    "#     if self.cfg.loss.vicreg.sim_coeff_t:\n",
    "#         # penalize only unexplained change\n",
    "#         resid = (z0[1:] - z0[:-1]) - delta_hat.detach()\n",
    "#         diff_t = resid.pow(2).mean(dim=(2, 3, 4))\n",
    "\n",
    "#         losses['sim_loss_t'] = (\n",
    "#             (diff_t * transition_mask).sum()\n",
    "#             / transition_mask.sum().clamp_min(1)\n",
    "#         )\n",
    "#     else:\n",
    "#         losses['sim_loss_t'] = torch.zeros(1, device=self.device)\n",
    "\n",
    "#     # ---------------------------------------------------------\n",
    "#     # 5. MESSAGE PREDICTION (UNCHANGED)\n",
    "#     # ---------------------------------------------------------\n",
    "#     losses['msg_pred_loss'] = self.cross_entropy(\n",
    "#         msg_hat.flatten(0, 1),\n",
    "#         msg_target.flatten(0, 1)\n",
    "#     )\n",
    "\n",
    "#     # ---------------------------------------------------------\n",
    "#     # 6. SENDER / RECEIVER INVARIANCE (UNCHANGED)\n",
    "#     # ---------------------------------------------------------\n",
    "#     inv_loss_sender = (proj_z - proj_h).square().mean(dim=-1)\n",
    "#     losses['inv_loss_sender'] = (\n",
    "#         (inv_loss_sender * transition_mask).sum()\n",
    "#         / transition_mask.sum().clamp_min(1)\n",
    "#     )\n",
    "\n",
    "#     # ---------------------------------------------------------\n",
    "#     # # 7. ACTION SEPARATION LOSS (Option A)\n",
    "#     # # ---------------------------------------------------------\n",
    "#     # if actions is not None:\n",
    "#     #     # sample a second action (shuffle within batch)\n",
    "#     #     perm = torch.randperm(actions.shape[1])\n",
    "#     #     actions_alt = actions[:, perm]\n",
    "\n",
    "#     #     delta_a1 = self.predict_delta(z_hat[:-1], actions)\n",
    "#     #     delta_a2 = self.predict_delta(z_hat[:-1], actions_alt)\n",
    "\n",
    "#     #     act_sep = (delta_a1 - delta_a2).pow(2).mean(dim=(2, 3, 4))\n",
    "#     #     losses['action_separation'] = -(\n",
    "#     #         (act_sep * transition_mask).sum()\n",
    "#     #         / transition_mask.sum().clamp_min(1)\n",
    "#     #     )\n",
    "#     # else:\n",
    "#     #     losses['action_separation'] = torch.zeros(1, device=self.device)\n",
    "\n",
    "#     # ---------------------------------------------------------\n",
    "#     # 8. WEAK CONTROL ANCHOR (Option C)\n",
    "#     # ---------------------------------------------------------\n",
    "#     # if anchor_target is not None:\n",
    "#     #     latents= flatten_conv_output(z0)  # [T, B, d]\n",
    "#     #     latents = rearrange(latents, 't b d -> (t b) d')  # [(T*B), d]\n",
    "#     #     anchor_pred = self.anchor_head(latents[:-1])\n",
    "#     #     anchor_loss = self.anchor_loss(anchor_pred, anchor_target)\n",
    "\n",
    "#     #     losses['anchor_loss'] = (\n",
    "#     #         (anchor_loss * transition_mask).sum()\n",
    "#     #         / transition_mask.sum().clamp_min(1)\n",
    "#     #     )\n",
    "#     # else:\n",
    "#     #     losses['anchor_loss'] = torch.zeros(1, device=self.device)\n",
    "\n",
    "#     return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# @patch\n",
    "# def setup_ddp_hooks(self, sender, rec):\n",
    "#     # Map the symbols to the actual DDP-wrapped modules\n",
    "#     modules_to_track = {\n",
    "#         \"phi_sender\": self.get_module(self.model[sender].backbone),\n",
    "#         \"p_theta\": self.get_module(self.comm_module),\n",
    "#         \"q_theta\": self.get_module(self.model[sender].msg_enc),\n",
    "#         \"f_theta_rec\": self.get_module(self.model[rec].jepa.dynamics) \n",
    "#     }\n",
    "\n",
    "#     for name, m in modules_to_track.items():\n",
    "#         m.register_full_backward_hook(self._hook_fn(name))\n",
    "\n",
    "# def get_module(self, m):\n",
    "#     \"\"\"Helper to unwrap DDP module if necessary.\"\"\"\n",
    "#     return m.module if hasattr(m, 'module') else m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08159b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_sampling_prob(self: WMTrainer, epoch):\n",
    "    if epoch < self.schedule_start_epoch:\n",
    "        return 0.0  # Always use ground truth\n",
    "    elif epoch >= self.schedule_end_epoch:\n",
    "        return 1.0  # Always use predictions\n",
    "    else:\n",
    "        # Linear interpolation\n",
    "        progress = (epoch - self.schedule_start_epoch) / (self.schedule_end_epoch - self.schedule_start_epoch)\n",
    "        return progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3759b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def sender_jepa(self: WMTrainer, data, sampling_prob):\n",
    "\n",
    "    obs_sender, pos_sender, msg, msg_target, _,_, _ = data\n",
    "    obs_sender = obs_sender.to(self.device)\n",
    "    pos_sender = pos_sender.to(self.device)\n",
    "    msg = msg.to(self.device)\n",
    "    msg_target = msg_target.to(self.device)\n",
    "\n",
    "        \n",
    "    self.logger.info(f\"device used for other agent data: {obs_sender.device}, {msg.device}\")\n",
    "\n",
    "    z = self.obs_enc(obs_sender, position = pos_sender)  #[B, T, c, h, w] => [B, T, c`, h`, w`]\n",
    "    h_target = self.msg_enc(msg)  # [B, T, C, H, W] => [B, T, dim=32]\n",
    "    proj_z, proj_h = self.proj(z, h_target) # True JEPA alignment\n",
    "\n",
    "    msg_hat = self.comm_module(z)  # [B, T, c`, h`, w`] => [B, T, C=5, H=7, W=7]\n",
    "    if torch.rand(1).item() < sampling_prob:\n",
    "        sample = F.one_hot(msg_hat.argmax(dim=2), num_classes=5)  # [B, T, 7, 7, 5]\n",
    "        sample = rearrange(sample, 'b t h w c -> b t c h w')# [B, T, 5, 7, 7]\n",
    "        probs = F.softmax(msg_hat, dim=2)  # [B, T, 5, 7, 7]\n",
    "        msg_used = sample + probs - probs.detach() # [B, T, C, H, W] `one-hot with straight-through`\n",
    "        h_for_receiver = self.msg_enc(msg_used.to(probs.dtype)) # [B, T, C, H, W] => [B, T, dim=32]\n",
    "\n",
    "    else:\n",
    "        msg_used = msg  # [B, T, C, H, W]\n",
    "        h_for_receiver = h_target  # Use target encoding when not sampling\n",
    "    \n",
    "    return msg_hat, msg_target, h_for_receiver, proj_z, proj_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d831911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def rec_jepa(self: WMTrainer, data, h):\n",
    "    obs, pos, _, _, act, _, dones = data\n",
    "    mask = (~dones.bool()).float().to(self.device) # [B, T, d=1]\n",
    "    mask = rearrange(mask, 'b t d-> b (t d)', d=1)\n",
    "    mask_t = rearrange(mask, 'b t -> t b')\n",
    "\n",
    "    if mask.sum() == 0:\n",
    "        return  \n",
    " \n",
    "    obs = obs.to(self.device)\n",
    "    pos = pos.to(self.device)\n",
    "    act = act.to(self.device)\n",
    "\n",
    "    self.logger.info(f\"device used for main agent data: {mask_t.device} {obs.device}, {act.device}\")\n",
    "    \n",
    "    z0, z = self.jepa(x= obs, #[B, T, c, h, w] =>  [T, B, c, h, w]\n",
    "                      pos= pos,\n",
    "                      actions= act,\n",
    "                      msgs= h,\n",
    "                      T= act.size(1)-1)\n",
    "    \n",
    "    return z0, z, act, mask_t, mask, len(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed8ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# from mawm.models.utils import flatten_conv_output\n",
    "# from einops import rearrange\n",
    "# @patch\n",
    "# def train_epoch(self: WMTrainer, epoch):\n",
    "#     self.model.train()\n",
    "#     self.msg_enc.train()\n",
    "#     self.comm_module.train()\n",
    "#     self.proj.train()\n",
    "    \n",
    "#     total_running_loss = 0.0\n",
    "#     total_valid_steps = 0\n",
    "\n",
    "#     self.logger.info(f\"Device used: {self.device}\")\n",
    "#     self.sampler.set_epoch(epoch) if epoch > 0 else None\n",
    "\n",
    "#     sampling_prob = self.get_sampling_prob(epoch)\n",
    "#     for batch_idx, data in enumerate(self.train_loader):\n",
    "        \n",
    "#         global_step = epoch * len(self.train_loader) + batch_idx\n",
    "#         self.optimizer.zero_grad()\n",
    "#         batch_loss = 0\n",
    "\n",
    "#         for rec in self.agents:\n",
    "#             #### SENDER JEPA\n",
    "#             for sender in self.agents:\n",
    "#                 if sender == rec: continue\n",
    "#                 msg_hat, msg_target, h, proj_z, proj_h = self.sender_jepa(data[sender].values(), sender, sampling_prob)\n",
    "            \n",
    "#             ### RECEIVER JEPA\n",
    "#             z0, z, act, mask_t, mask, len_obs = self.rec_jepa(data[rec].values(), rec, h)\n",
    "            \n",
    "#             losses = self.criterion(global_step, z0, z, act, msg_target, msg_hat, proj_h, proj_z, mask_t)\n",
    "            \n",
    "#             if self.verbose:\n",
    "#                 to_log = {\n",
    "#                     f'{rec}/sigreg_img': losses['sigreg_img'].item(),\n",
    "#                     f'{rec}/sim_loss': losses['sim_loss_dynamics'].item(),\n",
    "#                     f'{rec}/sim_loss_t': losses['sim_loss_t'].item(),\n",
    "#                     f'{rec}/idm_loss': losses['idm_loss'].item(),\n",
    "\n",
    "#                     f'{sender}/sigreg_obs': losses['sigreg_obs'].item(),\n",
    "\n",
    "#                     'shared/msg_pred_loss': losses['msg_pred_loss'].item(),\n",
    "#                     'shared/sigreg_msg': losses['sigreg_msg'].item(),\n",
    "#                     'shared/inv_loss_sender': losses['inv_loss_sender'].item(),\n",
    "#                 }\n",
    "\n",
    "#                 if self.verbose:\n",
    "#                     self.writer.write(to_log)\n",
    "            \n",
    "#             self.logger.info(\"Losses: %s\" % str({k: v.item() for k, v in losses.items()}))\n",
    "            \n",
    "#             s_jepa = self.lambda_ * (losses['sigreg_obs'] + losses['sigreg_msg']) + (1 - self.lambda_) * losses['inv_loss_sender']\n",
    "#             r_jepa =  self.lambda_ * losses['sigreg_img'] + (1 - self.lambda_) * losses['sim_loss_dynamics']\n",
    "#             task_loss = (self.W_H_PRED * losses['msg_pred_loss'] + \n",
    "#                          self.W_SIM_T * losses['sim_loss_t'] + \n",
    "#                          self.cfg.loss.idm.coeff * losses['idm_loss'])\n",
    "\n",
    "#             self.logger.info(f\"JEPA Losses: sender_jepa_loss: {s_jepa.item():.4f}, rec_jepa_loss: {r_jepa.item():.4f}, task_loss: {task_loss.item():.4f}\")\n",
    "\n",
    "#             pair_loss = s_jepa + r_jepa + task_loss\n",
    "#             num_pairs = len(self.agents) * (len(self.agents) - 1)\n",
    "#             scaled_loss = pair_loss / num_pairs #len(self.agents)\n",
    "            \n",
    "#             scaled_loss.backward()\n",
    "#             self.logger.info(f\"Agent: {rec}, agent_loss: {pair_loss.item():.4f}\")\n",
    "            \n",
    "#             batch_loss += scaled_loss\n",
    "\n",
    "#             num_valid = mask.sum().item()\n",
    "#             total_running_loss += pair_loss.item() * num_valid\n",
    "#             total_valid_steps += num_valid\n",
    "            \n",
    "#         loss = batch_loss\n",
    "#         self.optimizer.step()\n",
    "\n",
    "#         if batch_idx % 20 == 0:\n",
    "#             self.logger.info(f'Train Epoch: {epoch} [{batch_idx * len_obs}/{len(self.train_loader.dataset)} '\n",
    "#                   f'({100. * batch_idx / len(self.train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "#     final_epoch_loss = (total_running_loss / total_valid_steps) / len(self.agents) if total_valid_steps > 0 else 0.0\n",
    "#     self.logger.info(f'====> Epoch: {epoch} Average loss: {final_epoch_loss:.4f}')\n",
    "\n",
    "#     return final_epoch_loss\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2a9770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from mawm.models.utils import flatten_conv_output\n",
    "from einops import rearrange\n",
    "@patch\n",
    "def train_epoch(self: WMTrainer, epoch):\n",
    "    \n",
    "    total_running_loss = 0.0\n",
    "    total_valid_steps = 0\n",
    "    num_pairs = len(self.agents) * (len(self.agents) - 1) # Equals 2 for two agents\n",
    "\n",
    "    self.sampler.set_epoch(epoch) if epoch > 0 else None\n",
    "    sampling_prob = self.get_sampling_prob(epoch)\n",
    "\n",
    "    for batch_idx, data in enumerate(self.train_loader):\n",
    "        global_step = epoch * len(self.train_loader) + batch_idx\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        batch_log_accumulator = {}\n",
    "\n",
    "        for rec in self.agents:\n",
    "            for sender in self.agents:\n",
    "                if sender == rec: continue\n",
    "                \n",
    "                msg_hat, msg_target, h, proj_z, proj_h = self.sender_jepa(\n",
    "                    data[sender].values(), sampling_prob\n",
    "                )\n",
    "                \n",
    "                z0, z, act, mask_t, mask, len_obs = self.rec_jepa(\n",
    "                    data[rec].values(), h\n",
    "                )\n",
    "                \n",
    "                losses = self.criterion(global_step, z0, z, act, msg_target, msg_hat, proj_h, proj_z, mask_t)\n",
    "                self.logger.info(\"Losses: %s\" % str({k: v.item() for k, v in losses.items()}))\n",
    "                \n",
    "                s_jepa = self.lambda_ * (losses['sigreg_obs'] + losses['sigreg_msg']) + (1 - self.lambda_) * losses['inv_loss_sender']\n",
    "                r_jepa = self.lambda_ * losses['sigreg_img'] + (1 - self.lambda_) * losses['sim_loss_dynamics']\n",
    "                task_loss = (self.W_H_PRED * losses['msg_pred_loss'] + \n",
    "                             self.W_SIM_T * losses['sim_loss_t'] + \n",
    "                             self.cfg.loss.idm.coeff * losses['idm_loss'])\n",
    "\n",
    "                pair_loss = s_jepa + r_jepa + task_loss\n",
    "\n",
    "                self.logger.info(f\"JEPA Losses: sender_jepa_loss: {s_jepa.item():.4f}, rec_jepa_loss: {r_jepa.item():.4f}, task_loss: {task_loss.item():.4f}\")\n",
    "                \n",
    "                scaled_loss = pair_loss / num_pairs\n",
    "                scaled_loss.backward()\n",
    "\n",
    "                num_valid = mask.sum().item()\n",
    "                total_running_loss += pair_loss.item() * num_valid\n",
    "                total_valid_steps += num_valid\n",
    "\n",
    "                if self.verbose:\n",
    "                    for k, v in losses.items():\n",
    "                        batch_log_accumulator[f'{rec}_as_rec/{k}'] = v.item()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # if batch_idx % 2 == 0:\n",
    "        if self.verbose:\n",
    "            self.writer.write(batch_log_accumulator)\n",
    "            \n",
    "            processed_samples = batch_idx * len_obs \n",
    "            self.logger.info(f'Train Epoch: {epoch} [{processed_samples}/{len(self.train_loader.dataset)} '\n",
    "                             f'({100. * batch_idx / len(self.train_loader):.0f}%)]\\t'\n",
    "                             f'Pair Avg Loss: {pair_loss.item():.64f}')\n",
    "\n",
    "    final_epoch_loss = (total_running_loss / total_valid_steps) if total_valid_steps > 0 else 0.0\n",
    "    return final_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0041e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import wandb\n",
    "CHECKPOINT_FREQ = 1\n",
    "@patch\n",
    "def fit(self: WMTrainer):\n",
    "    self.jepa.train()\n",
    "    self.obs_enc.train()\n",
    "    self.msg_enc.train()\n",
    "    self.comm_module.train()\n",
    "    self.proj.train()\n",
    "\n",
    "    latest_file = \"latest.pt\"\n",
    "    folder = self.dmpc_dir\n",
    "    latest_path = os.path.join(folder, latest_file)\n",
    "    \n",
    "    loss_meter = AverageMeter()\n",
    "    lst_dfs = []\n",
    "\n",
    "    for epoch in range(1, self.cfg.epochs + 1):\n",
    "        self.logger.info(\"Epoch %d\" % (epoch))\n",
    "        lr = self.scheduler.adjust_learning_rate(epoch)\n",
    "        train_loss = self.train_epoch(epoch)\n",
    "        loss_meter.update(train_loss)\n",
    "        \n",
    "        def save_checkpoint(epoch, path):\n",
    "            if not self.verbose:\n",
    "                return\n",
    "            \n",
    "            def get_state(m):\n",
    "                return m.module.state_dict() if hasattr(m, 'module') else m.state_dict()\n",
    "            \n",
    "            save_dict = {\n",
    "                'epoch': epoch,\n",
    "                'jepa': get_state(self.jepa),\n",
    "                'obs_enc': get_state(self.obs_enc),\n",
    "                'msg_enc': get_state(self.msg_enc),\n",
    "                'comm_module': get_state(self.comm_module),\n",
    "                'proj': get_state(self.proj),\n",
    "                'train_loss': train_loss,\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "                \"lr\": lr,\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                torch.save(save_dict, path)\n",
    "                self.logger.info(f\"Successfully saved checkpoint to {path}\")\n",
    "            except Exception as e:\n",
    "                self.logger.info(f\"Encountered exception when saving checkpoint: {e}\")\n",
    "        \n",
    "        self.logger.info(\"avg. loss %.3f\" % loss_meter.avg)\n",
    "\n",
    "        df = pd.DataFrame.from_records([{\"epoch\": epoch ,\"train_loss\": train_loss}], index= \"epoch\")\n",
    "        lst_dfs.append(df)\n",
    "\n",
    "        if epoch % CHECKPOINT_FREQ == 0 or epoch == (self.cfg.epochs - 1):\n",
    "            save_checkpoint(epoch + 1, latest_path)\n",
    "            if self.cfg.save_every_freq > 0 and epoch % self.cfg.save_every_freq == 0:\n",
    "                save_every_file = f\"e{epoch}.pt\"\n",
    "                save_every_path = os.path.join(folder, save_every_file)\n",
    "                save_checkpoint(epoch + 1, save_every_path)\n",
    "\n",
    "        to_log = {\n",
    "            \"train_loss\": train_loss, \n",
    "        }\n",
    "\n",
    "        if self.verbose:\n",
    "            self.writer.write(to_log)\n",
    "\n",
    "    df_res = pd.concat(lst_dfs)\n",
    "    df_reset = df_res.reset_index()\n",
    "    if self.verbose:\n",
    "        self.writer.write({'Train Loss Table': wandb.Table(dataframe= df_reset)})\n",
    "        self.writer.finish()\n",
    "    return df_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69feff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marlgrid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
