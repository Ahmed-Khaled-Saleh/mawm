{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb656fc",
   "metadata": {},
   "source": [
    "# World Model trainer\n",
    "\n",
    "> This module implements LeJepa training procedure with three predictors and two input modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bbb56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp trainers.findgoal_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd36cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import *\n",
    "from fastcore.utils import *\n",
    "from torchvision.utils import save_image\n",
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb85e38",
   "metadata": {},
   "source": [
    "## WorldModel Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from mawm.models.utils import save_checkpoint\n",
    "from mawm.loggers.base import AverageMeter\n",
    "from mawm.losses.sigreg import SIGReg, SIGRegDistributed\n",
    "from mawm.losses.idm import IDMLoss    \n",
    "from mawm.models.utils import flatten_conv_output\n",
    "from einops import rearrange\n",
    "\n",
    "class WMTrainer:\n",
    "    def __init__(self, cfg, model, train_loader, sampler,\n",
    "                 optimizer=None, device=None,earlystopping=None, \n",
    "                 scheduler=None, writer= None, verbose= None, logger= None):\n",
    "        \n",
    "        self.cfg = cfg\n",
    "        self.device = device\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.sampler = sampler\n",
    "\n",
    "        self.model = model['jepa']\n",
    "        self.msg_enc = model['msg_enc']\n",
    "        self.comm_module = model['comm_module']\n",
    "        self.proj = model['proj']\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.earlystopping = earlystopping\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "        self.writer = writer\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "        self.sigreg = SIGReg().to(self.device)\n",
    "        self.disSigReg = SIGRegDistributed().to(self.device)\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        self.idm = IDMLoss(cfg.loss.idm, (32, 15, 15), device= self.device)\n",
    "\n",
    "        # self.anchor_head = nn.Linear(self.cfg.model.jepa.latent_dim, 2).to(self.device)\n",
    "        self.lambda_ = self.cfg.loss.lambda_\n",
    "        self.W_H_PRED = self.cfg.loss.W_H_PRED\n",
    "        self.W_SIM_T = self.cfg.loss.W_SIM_T\n",
    "\n",
    "        self.agents = [f\"agent_{i}\" for i in range(len(self.cfg.env.agents))]\n",
    "\n",
    "        self.dmpc_dir = os.path.join(self.cfg.log_dir, self.cfg.log_subdir, self.cfg.now)\n",
    "        if not os.path.exists(self.dmpc_dir):\n",
    "            os.makedirs(self.dmpc_dir , exist_ok=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59d561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 8, 5, 7, 7]), torch.Size([16, 8, 7, 7]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "import torch\n",
    "logits = torch.randn(16, 8, 5, 7, 7)\n",
    "targets = torch.randint(0, 5, (16, 8, 7, 7))\n",
    "logits.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9796df66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9733)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "loss(logits.flatten(0,1), targets.flatten(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c89df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def criterion(self: WMTrainer, global_step, Z0, Z, actions, msg_target, msg_hat, proj_h, proj_z, mask_t):\n",
    "\n",
    "    flat_encodings = flatten_conv_output(Z0) # [T, B, c`, h`, w`] => [T, B, d]\n",
    "    sigreg_img = self.disSigReg(flat_encodings[:1], global_step= global_step)\n",
    "\n",
    "    transition_mask = mask_t[1:] * mask_t[:-1]\n",
    "    diff = (Z0[1:] - Z[1:]).pow(2).mean(dim=(2, 3, 4)) # (T-1, B)\n",
    "    sim_loss = (diff * transition_mask).sum() / transition_mask.sum().clamp_min(1)\n",
    "\n",
    "    if self.cfg.loss.vicreg.sim_coeff_t:\n",
    "        diff_t = ( Z0[1:] -  Z0[:-1]).pow(2).mean(dim=(2, 3, 4))# (T-1, B)\n",
    "        sim_loss_t = (diff_t * transition_mask).sum() / transition_mask.sum().clamp_min(1)\n",
    "    else:\n",
    "        sim_loss_t = torch.zeros([1], device=self.device)\n",
    "    \n",
    "    idm_loss = self.idm(embeddings= Z0, predictions= Z, actions= actions)\n",
    "\n",
    "    msg_pred_loss = self.cross_entropy(msg_hat.flatten(0,1), msg_target.flatten(0,1))\n",
    "\n",
    "    sigreg_msg = self.disSigReg(proj_h, global_step= global_step)\n",
    "    sigreg_obs = self.disSigReg(proj_z, global_step= global_step)\n",
    "\n",
    "    inv_loss_sender = (proj_z - proj_h).square().mean(dim= -1)  # [T, B, d= 128] => [T, B]\n",
    "    inv_loss_sender = (inv_loss_sender * transition_mask).sum() / transition_mask.sum().clamp_min(1) \n",
    "\n",
    "    return {\n",
    "        'sigreg_img': sigreg_img,\n",
    "        'sigreg_msg': sigreg_msg,\n",
    "        'sigreg_obs': sigreg_obs,\n",
    "        'sim_loss_dynamics': sim_loss,\n",
    "        'sim_loss_t': sim_loss_t,\n",
    "        'inv_loss_sender': inv_loss_sender,\n",
    "        'msg_pred_loss': msg_pred_loss,\n",
    "        'idm_loss': idm_loss\n",
    "    }\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c67e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# @patch\n",
    "# def criterion(\n",
    "#     self: WMTrainer,\n",
    "#     global_step,\n",
    "#     Z0,            # encoded latents [T, B, C, H, W] (stop-grad)\n",
    "#     Z_hat,         # predicted latents [T, B, C, H, W]\n",
    "#     msg_target,\n",
    "#     msg_hat,\n",
    "#     proj_h,\n",
    "#     proj_z,\n",
    "#     mask_t,\n",
    "#     actions=None,          # [T-1, B, A]  (for action-sep)\n",
    "#     anchor_target=None     # optional (dx, dy) or moved flag\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Combined JEPA loss with:\n",
    "#     - delta dynamics (B)\n",
    "#     - action separation (A)\n",
    "#     - weak control anchor (C)\n",
    "#     \"\"\"\n",
    "\n",
    "#     losses = {}\n",
    "\n",
    "#     # ---------------------------------------------------------\n",
    "#     # 1. SIGReg on image / message / obs (UNCHANGED)\n",
    "#     # ---------------------------------------------------------\n",
    "#     flat_encodings = flatten_conv_output(Z0)  # [T, B, d]\n",
    "#     losses['sigreg_img'] = self.disSigReg(flat_encodings[:1], global_step)\n",
    "#     losses['sigreg_msg'] = self.disSigReg(proj_h, global_step)\n",
    "#     losses['sigreg_obs'] = self.disSigReg(proj_z, global_step)\n",
    "\n",
    "#     # ---------------------------------------------------------\n",
    "#     # 2. Transition mask\n",
    "#     # ---------------------------------------------------------\n",
    "#     transition_mask = mask_t[1:] * mask_t[:-1]   # [T-1, B]\n",
    "\n",
    "#     # ---------------------------------------------------------\n",
    "#     # 3. DELTA DYNAMICS LOSS (Option B)\n",
    "#     # ---------------------------------------------------------\n",
    "#     delta_hat = Z_hat[1:] - Z_hat[:-1]\n",
    "#     delta_true = Z0[1:] - Z0[:-1]\n",
    "\n",
    "#     diff_delta = (delta_hat - delta_true).pow(2).mean(dim=(2, 3, 4))\n",
    "#     losses['sim_loss_dynamics'] = (\n",
    "#         (diff_delta * transition_mask).sum()\n",
    "#         / transition_mask.sum().clamp_min(1)\n",
    "#     )\n",
    "\n",
    "#     # ---------------------------------------------------------\n",
    "#     # 4. GATED TIME-SMOOTHNESS (important!)\n",
    "#     # ---------------------------------------------------------\n",
    "#     if self.cfg.loss.vicreg.sim_coeff_t:\n",
    "#         # penalize only unexplained change\n",
    "#         resid = (Z0[1:] - Z0[:-1]) - delta_hat.detach()\n",
    "#         diff_t = resid.pow(2).mean(dim=(2, 3, 4))\n",
    "\n",
    "#         losses['sim_loss_t'] = (\n",
    "#             (diff_t * transition_mask).sum()\n",
    "#             / transition_mask.sum().clamp_min(1)\n",
    "#         )\n",
    "#     else:\n",
    "#         losses['sim_loss_t'] = torch.zeros(1, device=self.device)\n",
    "\n",
    "#     # ---------------------------------------------------------\n",
    "#     # 5. MESSAGE PREDICTION (UNCHANGED)\n",
    "#     # ---------------------------------------------------------\n",
    "#     losses['msg_pred_loss'] = self.cross_entropy(\n",
    "#         msg_hat.flatten(0, 1),\n",
    "#         msg_target.flatten(0, 1)\n",
    "#     )\n",
    "\n",
    "#     # ---------------------------------------------------------\n",
    "#     # 6. SENDER / RECEIVER INVARIANCE (UNCHANGED)\n",
    "#     # ---------------------------------------------------------\n",
    "#     inv_loss_sender = (proj_z - proj_h).square().mean(dim=-1)\n",
    "#     losses['inv_loss_sender'] = (\n",
    "#         (inv_loss_sender * transition_mask).sum()\n",
    "#         / transition_mask.sum().clamp_min(1)\n",
    "#     )\n",
    "\n",
    "#     # ---------------------------------------------------------\n",
    "#     # # 7. ACTION SEPARATION LOSS (Option A)\n",
    "#     # # ---------------------------------------------------------\n",
    "#     # if actions is not None:\n",
    "#     #     # sample a second action (shuffle within batch)\n",
    "#     #     perm = torch.randperm(actions.shape[1])\n",
    "#     #     actions_alt = actions[:, perm]\n",
    "\n",
    "#     #     delta_a1 = self.predict_delta(Z_hat[:-1], actions)\n",
    "#     #     delta_a2 = self.predict_delta(Z_hat[:-1], actions_alt)\n",
    "\n",
    "#     #     act_sep = (delta_a1 - delta_a2).pow(2).mean(dim=(2, 3, 4))\n",
    "#     #     losses['action_separation'] = -(\n",
    "#     #         (act_sep * transition_mask).sum()\n",
    "#     #         / transition_mask.sum().clamp_min(1)\n",
    "#     #     )\n",
    "#     # else:\n",
    "#     #     losses['action_separation'] = torch.zeros(1, device=self.device)\n",
    "\n",
    "#     # ---------------------------------------------------------\n",
    "#     # 8. WEAK CONTROL ANCHOR (Option C)\n",
    "#     # ---------------------------------------------------------\n",
    "#     # if anchor_target is not None:\n",
    "#     #     latents= flatten_conv_output(Z0)  # [T, B, d]\n",
    "#     #     latents = rearrange(latents, 't b d -> (t b) d')  # [(T*B), d]\n",
    "#     #     anchor_pred = self.anchor_head(latents[:-1])\n",
    "#     #     anchor_loss = self.anchor_loss(anchor_pred, anchor_target)\n",
    "\n",
    "#     #     losses['anchor_loss'] = (\n",
    "#     #         (anchor_loss * transition_mask).sum()\n",
    "#     #         / transition_mask.sum().clamp_min(1)\n",
    "#     #     )\n",
    "#     # else:\n",
    "#     #     losses['anchor_loss'] = torch.zeros(1, device=self.device)\n",
    "\n",
    "#     return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed8ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from mawm.models.utils import flatten_conv_output\n",
    "from einops import rearrange\n",
    "@patch\n",
    "def train_epoch(self: WMTrainer, epoch):\n",
    "    self.model.train()\n",
    "    self.msg_enc.train()\n",
    "    self.comm_module.train()\n",
    "    self.proj.train()\n",
    "    \n",
    "    total_running_loss = 0.0\n",
    "    total_valid_steps = 0\n",
    "\n",
    "    self.logger.info(f\"Device used: {self.device}\")\n",
    "    self.sampler.set_epoch(epoch)\n",
    "    for batch_idx, data in enumerate(self.train_loader):\n",
    "        global_step = epoch * len(self.train_loader) + batch_idx\n",
    "        self.optimizer.zero_grad()\n",
    "        batch_loss = 0\n",
    "\n",
    "        ## Receiver End\n",
    "        for agent_id in self.agents:\n",
    "            obs, pos, _, _, act, _, dones = data[agent_id].values()\n",
    "            mask = (~dones.bool()).float().to(self.device) # [B, T, d=1]\n",
    "            mask = rearrange(mask, 'b t d-> b (t d)', d=1)\n",
    "            mask_t = rearrange(mask, 'b t -> t b')\n",
    "            \n",
    "            agent_loss = 0\n",
    "\n",
    "            if mask.sum() == 0:\n",
    "                continue \n",
    "\n",
    "            obs = obs.to(self.device)\n",
    "            pos = pos.to(self.device)\n",
    "            act = act.to(self.device)\n",
    "\n",
    "            self.logger.info(f\"device used for main agent data: {mask_t.device} {obs.device}, {act.device}\")\n",
    "\n",
    "            #### SENDER End\n",
    "            for sender in self.agents:\n",
    "                if sender != agent_id:\n",
    "                    obs_sender, pos_sender, msg, msg_target, _,_, _ = data[sender].values()\n",
    "\n",
    "                    obs_sender = obs_sender.to(self.device)\n",
    "                    pos_sender = pos_sender.to(self.device)\n",
    "                    msg = msg.to(self.device)\n",
    "                    msg_target = msg_target.to(self.device)\n",
    "\n",
    "                    self.logger.info(f\"device used for other agent data: {obs_sender.device}, {msg.device}\")\n",
    "            \n",
    "            ### Reciever JEPA\n",
    "            h = self.msg_enc(msg) # [B, T, C, H, W] => [B, T, dim=32]\n",
    "            Z0, Z = self.model(x= obs, #[B, T, c, h, w] =>  [T, B, c, h, w]\n",
    "                               pos= pos, \n",
    "                               actions= act, \n",
    "                               msgs= h, \n",
    "                               T= act.size(1)-1)\n",
    "            \n",
    "            # delta_a1 = self.model.dynamics\n",
    "            \n",
    "            ## Sender JEPA\n",
    "            if hasattr(self.model, 'module'):\n",
    "                z_sender = self.model.module.backbone(obs_sender, position = pos_sender)  #[B, T, c, h, w] => [B, T, c`, h`, w`]\n",
    "            else:\n",
    "                z_sender = self.model.backbone(obs_sender, position = pos_sender)  #[B, T, c, h, w] => [B, T, c`, h`, w`]\n",
    "                \n",
    "            proj_z, proj_h = self.proj(z_sender, h) #[B, T, c`, h`, w`],[B, T, dim=32] => [T, B, d= 128], [T, B, d= 128]\n",
    "            msg_hat = self.comm_module(z_sender)  # [B, T, c`, h`, w`] => [B, T, C=5, H=7, W=7]\n",
    "            \n",
    "            \n",
    "            losses = self.criterion(global_step, Z0, Z, act, msg_target, msg_hat, proj_h, proj_z, mask_t)\n",
    "          \n",
    "            if self.verbose:\n",
    "                self.writer.write({\n",
    "                    f'{agent_id}/train/sigreg_img': losses['sigreg_img'].item(),\n",
    "                    f'{agent_id}/train/sigreg_msg': losses['sigreg_msg'].item(),\n",
    "                    f'{agent_id}/train/sigreg_obs': losses['sigreg_obs'].item(),\n",
    "                    f'{agent_id}/train/sim_loss': losses['sim_loss_dynamics'].item(),\n",
    "                    f'{agent_id}/train/sim_loss_t': losses['sim_loss_t'].item(),\n",
    "                    f'{agent_id}/train/msg_pred_loss': losses['msg_pred_loss'].item(),\n",
    "                    f'{agent_id}/train/inv_loss_sender': losses['inv_loss_sender'].item(),\n",
    "                    f'{agent_id}/train/idm_loss': losses['idm_loss'].item()\n",
    "                })\n",
    "    #             with torch.no_grad():\n",
    "    # dz = (Z[1:] - Z[:-1]).pow(2).mean(dim=(2,3,4))\n",
    "    # print(dz.mean(), dz.std())\n",
    "\n",
    "                \n",
    "            self.logger.info(\"Losses: %s\" % str({k: v.item() for k, v in losses.items()}))\n",
    "            \n",
    "            sender_jepa_loss = self.lambda_ * (losses['sigreg_obs'] + losses['sigreg_msg']) + 25.0 * losses['inv_loss_sender']\n",
    "            rec_jepa_loss = self.lambda_ * losses['sim_loss_dynamics'] + 5.0 * losses['sigreg_img']\n",
    "            comm_mod_loss = self.W_H_PRED * losses['msg_pred_loss']\n",
    "            smothness_loss = self.W_SIM_T * losses['sim_loss_t']\n",
    "            idm_loss = self.cfg.loss.idm.coeff * losses['idm_loss']\n",
    "\n",
    "            self.logger.info(f\"JEPA Losses: sender_jepa_loss: {sender_jepa_loss.item():.4f}, rec_jepa_loss: {rec_jepa_loss.item():.4f}, sim_loss_t: {losses['sim_loss_t'].item():.4f}, idm_loss: {losses['idm_loss'].item():.4f}\")\n",
    "\n",
    "            agent_loss = sender_jepa_loss + rec_jepa_loss + smothness_loss + comm_mod_loss + idm_loss\n",
    "            scaled_loss = agent_loss / len(self.agents)\n",
    "            scaled_loss.backward()\n",
    "            self.logger.info(f\"Agent: {agent_id}, agent_loss: {agent_loss.item():.4f}\")\n",
    "            \n",
    "            batch_loss += scaled_loss\n",
    "\n",
    "            num_valid = mask.sum().item()\n",
    "            total_running_loss += agent_loss.item() * num_valid\n",
    "            total_valid_steps += num_valid\n",
    "            \n",
    "        loss = batch_loss\n",
    "        # loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            self.logger.info(f'Train Epoch: {epoch} [{batch_idx * len(obs)}/{len(self.train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(self.train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "    final_epoch_loss = (total_running_loss / total_valid_steps) / len(self.agents) if total_valid_steps > 0 else 0.0\n",
    "    self.logger.info(f'====> Epoch: {epoch} Average loss: {final_epoch_loss:.4f}')\n",
    "\n",
    "    return final_epoch_loss\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0041e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import wandb\n",
    "CHECKPOINT_FREQ = 1\n",
    "@patch\n",
    "def fit(self: WMTrainer):\n",
    "\n",
    "    latest_file = \"latest.pt\"\n",
    "    folder = self.dmpc_dir\n",
    "    latest_path = os.path.join(folder, latest_file)\n",
    "    \n",
    "    loss_meter = AverageMeter()\n",
    "    lst_dfs = []\n",
    "\n",
    "    for epoch in range(1, self.cfg.epochs + 1):\n",
    "        self.logger.info(\"Epoch %d\" % (epoch))\n",
    "        lr = self.scheduler.adjust_learning_rate(epoch)\n",
    "        train_loss = self.train_epoch(epoch)\n",
    "        loss_meter.update(train_loss)\n",
    "        \n",
    "        def save_checkpoint(epoch, path):\n",
    "            if not self.verbose:\n",
    "                return\n",
    "            \n",
    "            def get_state(m):\n",
    "                return m.module.state_dict() if hasattr(m, 'module') else m.state_dict()\n",
    "            \n",
    "            save_dict = {\n",
    "                'epoch': epoch,\n",
    "                'jepa': self.model.state_dict(),#get_state(self.model),\n",
    "                'msg_enc':self.msg_enc.state_dict(),#get_state(self.msg_enc),\n",
    "                'comm_module': self.comm_module.state_dict(),#get_state(self.comm_module),\n",
    "                'proj': self.proj.state_dict(),#get_state(self.proj),\n",
    "                'train_loss': train_loss,\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "                \"lr\": lr,\n",
    "            }\n",
    "            try:\n",
    "                torch.save(save_dict, path)\n",
    "                self.logger.info(f\"Successfully saved checkpoint to {path}\")\n",
    "            except Exception as e:\n",
    "                self.logger.info(f\"Encountered exception when saving checkpoint: {e}\")\n",
    "        \n",
    "        self.logger.info(\"avg. loss %.3f\" % loss_meter.avg)\n",
    "\n",
    "        df = pd.DataFrame.from_records([{\"epoch\": epoch ,\"train_loss\": train_loss}], index= \"epoch\")\n",
    "        lst_dfs.append(df)\n",
    "\n",
    "        if epoch % CHECKPOINT_FREQ == 0 or epoch == (self.cfg.epochs - 1):\n",
    "            save_checkpoint(epoch + 1, latest_path)\n",
    "            if self.cfg.save_every_freq > 0 and epoch % self.cfg.save_every_freq == 0:\n",
    "                save_every_file = f\"e{epoch}.pt\"\n",
    "                save_every_path = os.path.join(folder, save_every_file)\n",
    "                save_checkpoint(epoch + 1, save_every_path)\n",
    "\n",
    "        to_log = {\n",
    "            \"train_loss\": train_loss, \n",
    "        }\n",
    "\n",
    "        if self.verbose:\n",
    "            self.writer.write(to_log)\n",
    "\n",
    "    df_res = pd.concat(lst_dfs)\n",
    "    df_reset = df_res.reset_index()\n",
    "    if self.verbose:\n",
    "        self.writer.write({'Train Loss Table': wandb.Table(dataframe= df_reset)})\n",
    "        self.writer.finish()\n",
    "    return df_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69feff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
