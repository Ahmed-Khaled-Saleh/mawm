{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb656fc",
   "metadata": {},
   "source": [
    "# Beam Search Module\n",
    "\n",
    "> This module handles all aspects of the VAE, including encoding, decoding, and latent space representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bbb56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp search.beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd36cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import *\n",
    "from fastcore.utils import *\n",
    "from torchvision.utils import save_image\n",
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91661aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # --- Utility Class for Beam Search Node ---\n",
    "# class BeamNode:\n",
    "#     def __init__(self, sequence, score, state):\n",
    "#         self.sequence = sequence  # List of token IDs (the sequence built so far)\n",
    "#         self.score = score        # Total log-probability of the sequence\n",
    "#         self.state = state        # Decoder hidden state/memory after the last token\n",
    "\n",
    "#     def __lt__(self, other):\n",
    "#         # Comparison operator for min-heap/sorting\n",
    "#         return self.score < other.score\n",
    "\n",
    "# def beam_search_decode(\n",
    "#     model,\n",
    "#     encoder_output,\n",
    "#     initial_decoder_state,\n",
    "#     SOS_IDX,\n",
    "#     EOS_IDX,\n",
    "#     K,\n",
    "#     max_len\n",
    "# ):\n",
    "#     # 1. Initialization\n",
    "#     device = encoder_output.device\n",
    "    \n",
    "#     # The beam starts with a single candidate: the <SOS> token.\n",
    "#     # We use a list to hold the BeamNode objects.\n",
    "#     initial_token = torch.tensor([SOS_IDX], device=device).unsqueeze(0) # (1, 1) tensor\n",
    "#     initial_score = 0.0 # Log probability of <SOS> is 0\n",
    "    \n",
    "#     # The model should handle mapping the encoder_output to the initial state\n",
    "#     # for the first decoding step.\n",
    "    \n",
    "#     # NOTE: In a real implementation, you need to handle batching. This simple example \n",
    "#     # assumes a single input sentence for clarity. For batching, K beams must be created\n",
    "#     # for *each* item in the batch.\n",
    "\n",
    "#     beam = [\n",
    "#         BeamNode(\n",
    "#             sequence=[SOS_IDX],\n",
    "#             score=initial_score,\n",
    "#             state=initial_decoder_state # Initial state, often the last encoder hidden state\n",
    "#         )\n",
    "#     ]\n",
    "    \n",
    "#     # Store finished sequences to keep track of the final results\n",
    "#     finished_hypotheses = []\n",
    "\n",
    "#     # 2. Main Decoding Loop\n",
    "#     for t in range(max_len):\n",
    "#         candidates = []\n",
    "        \n",
    "#         # Prepare data for batched forward pass on the decoder\n",
    "#         # Only take the *last* token from each current beam for the next step's input\n",
    "#         current_tokens = torch.tensor([n.sequence[-1] for n in beam], device=device).unsqueeze(1) # (K, 1)\n",
    "        \n",
    "#         # Stack the states of the current beams for a batched forward pass\n",
    "#         # This part depends heavily on your model's architecture (RNN state tuple or Transformer memory)\n",
    "#         # Assuming RNN: hidden state is (h, c)\n",
    "#         if isinstance(beam[0].state, tuple):\n",
    "#              current_states = tuple(torch.stack([n.state[i] for n in beam], dim=1) for i in range(len(beam[0].state))) # (num_layers, K, hidden_size)\n",
    "#         else:\n",
    "#              current_states = torch.stack([n.state for n in beam], dim=1) # (num_layers, K, hidden_size)\n",
    "\n",
    "#         # DECODER STEP: Compute next token log-probabilities\n",
    "#         # output_logits: (K, 1, vocab_size) -> (K, vocab_size)\n",
    "#         # next_states: State for next step, typically (num_layers, K, hidden_size)\n",
    "#         output_logits, next_states = model.decode(current_tokens, encoder_output, current_states)\n",
    "#         log_probs = F.log_softmax(output_logits.squeeze(1), dim=-1) # (K, vocab_size)\n",
    "\n",
    "#         # Iterate over each of the K beams and their potential next steps\n",
    "#         for i, node in enumerate(beam):\n",
    "#             # Select the top K next tokens for this *specific* beam\n",
    "#             # We select K, as K*K will be pruned to K overall later\n",
    "#             topk_log_probs, topk_indices = log_probs[i].topk(K) \n",
    "            \n",
    "#             # The next state is specific to the current beam (index i)\n",
    "#             if isinstance(next_states, tuple):\n",
    "#                  new_state = tuple(s[:, i, :].unsqueeze(1) for s in next_states) # Select state i: (num_layers, 1, hidden_size)\n",
    "#             else:\n",
    "#                  new_state = next_states[:, i, :].unsqueeze(1) # Select state i: (num_layers, 1, hidden_size)\n",
    "            \n",
    "#             # Create K new candidate nodes\n",
    "#             for log_prob, idx in zip(topk_log_probs.tolist(), topk_indices.tolist()):\n",
    "#                 new_sequence = node.sequence + [idx]\n",
    "#                 new_score = node.score + log_prob # Log probabilities are additive\n",
    "                \n",
    "#                 new_node = BeamNode(new_sequence, new_score, new_state)\n",
    "#                 candidates.append(new_node)\n",
    "        \n",
    "#         # 3. Pruning and Filtering\n",
    "#         # Sort all K*K candidates by score and select the overall top K\n",
    "#         candidates.sort(key=lambda x: x.score, reverse=True)\n",
    "#         beam = candidates[:K]\n",
    "\n",
    "#         # Check for finished hypotheses and remove them from the active beam\n",
    "#         new_beam = []\n",
    "#         for node in beam:\n",
    "#             if node.sequence[-1] == EOS_IDX or t == max_len - 1:\n",
    "#                 # Sequence is finished or reached max length, add to final list\n",
    "#                 # Use length normalization here for final scoring (optional but recommended)\n",
    "#                 # length_penalty = ((5 + len(node.sequence)) / 6)**0.7\n",
    "#                 # node.score /= length_penalty \n",
    "#                 finished_hypotheses.append(node)\n",
    "#             else:\n",
    "#                 # Keep active in the beam for the next step\n",
    "#                 new_beam.append(node)\n",
    "        \n",
    "#         beam = new_beam\n",
    "        \n",
    "#         if not beam:\n",
    "#             # All hypotheses finished\n",
    "#             break\n",
    "            \n",
    "#     # 4. Final Result\n",
    "#     # Combine the remaining active beams with the finished ones\n",
    "#     finished_hypotheses.extend(beam)\n",
    "    \n",
    "#     # Sort final candidates and return the top one (or top N)\n",
    "#     finished_hypotheses.sort(key=lambda x: x.score, reverse=True)\n",
    "    \n",
    "#     # Return the sequence (tokens) of the best hypothesis\n",
    "#     return finished_hypotheses[0].sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fa280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from MAWM.core import Program, PRIMITIVE_TEMPLATES\n",
    "from MAWM.models.program_embedder import batchify_programs\n",
    "from MAWM.models.program_encoder import ProgramEncoder\n",
    "from MAWM.models.program_synthizer import Proposer\n",
    "\n",
    "@torch.no_grad()\n",
    "def neural_guided_beam_search(\n",
    "    z,\n",
    "    program_embedder: nn.Module,\n",
    "    proposer: nn.Module,\n",
    "    program_encoder: nn.Module,\n",
    "    score_fn,\n",
    "    MAX_PARAMS=3,\n",
    "    beam_width=5,\n",
    "    topk=6,\n",
    "    max_prog_len=5,\n",
    "    grid_size=7,\n",
    "    lambdas= (0.25, 0.5, 0.25),\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    neural-guided beam search.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- normalize z shape ---\n",
    "    z = z.to(device)\n",
    "    if z.dim() == 1:\n",
    "        z = z.unsqueeze(0)\n",
    "    lambda_1, lambda_2, lambda_3 = lambdas\n",
    "\n",
    "    sos_idx = proposer.sos_idx\n",
    "    EOS_IDX = len(PRIMITIVE_TEMPLATES)\n",
    "    zero_params = torch.full((MAX_PARAMS,), -1, device=device)\n",
    "\n",
    "    # initial program: [SOS]\n",
    "    init_prog = Program(tokens=[(sos_idx, zero_params.tolist())])\n",
    "    beam = [(init_prog, 0.0)]  # (program, score)\n",
    "\n",
    "    best_program = init_prog\n",
    "    best_score = -float(\"inf\")\n",
    "\n",
    "    # ---- start beam search ----\n",
    "    for depth in range(1, max_prog_len + 1):\n",
    "        alive = []\n",
    "        finished = []\n",
    "\n",
    "        for prog, score in beam:\n",
    "            last_prim = prog.tokens[-1][0]\n",
    "            if last_prim == EOS_IDX:\n",
    "                finished.append((prog, score))\n",
    "            else:\n",
    "                alive.append((prog, score))\n",
    "\n",
    "        if len(alive) == 0:\n",
    "            break\n",
    "        # ---- 1) build batch of prefixes ----\n",
    "        prefix_programs = [p for (p, _) in alive]\n",
    "        prev_idx_batch, prev_params_batch = batchify_programs(prefix_programs, padding_vals=[EOS_IDX, -1])\n",
    "\n",
    "        B = prev_idx_batch.shape[0]\n",
    "\n",
    "        p_vec = program_embedder(prev_idx_batch.to(device),\n",
    "                                 prev_params_batch.to(device))\n",
    "\n",
    "        # replicate z\n",
    "        z_batch = z.repeat(B, 1)\n",
    "\n",
    "        # proposer forward\n",
    "        def get_proposer(seq_len):\n",
    "            return Proposer(\n",
    "                    obs_dim= z.shape[-1],\n",
    "                    num_prims= len(PRIMITIVE_TEMPLATES),\n",
    "                    max_params= 2,\n",
    "                    seq_len= seq_len,\n",
    "                    prog_emb_dim_x= 32,\n",
    "                    prog_emb_dim_y= 32,\n",
    "                    prog_emb_dim_prims= 32,\n",
    "                )\n",
    "\n",
    "        proposer = get_proposer(prev_idx_batch.shape[-1])\n",
    "        prim_logits_batch, param_pred_batch = proposer.forward_step(z_batch, p_vec)\n",
    "\n",
    "        # ---- 2) expand each beam entry ----\n",
    "        expansions = []\n",
    "        for b_i, (prog_parent, parent_score) in enumerate(alive):\n",
    "            prim_logprobs = F.log_softmax(prim_logits_batch[b_i], dim=-1)\n",
    "            top_vals, top_idx = torch.topk(prim_logprobs, k=topk)\n",
    "\n",
    "            for k_i in range(topk):\n",
    "                prim_idx = int(top_idx[k_i].item())\n",
    "                prim_logp = float(top_vals[k_i].item())\n",
    "                if prim_idx == EOS_IDX:\n",
    "                    # add program as completed (no more expansions)\n",
    "                    new_prog = prog_parent.extend(prim_idx, [])\n",
    "                    expansions.append((new_prog, parent_score + prim_logp))\n",
    "                    # print(f\"Beam search found EOS at depth {depth} with program: {new_prog}\")\n",
    "                    continue\n",
    "\n",
    "                arity = PRIMITIVE_TEMPLATES[prim_idx][1]              \n",
    "\n",
    "                # ----- parameter instantiation -----\n",
    "                instantiations = []\n",
    "                if arity > 0:\n",
    "                    pred_params = param_pred_batch[b_i][:arity].cpu().numpy()\n",
    "                    for _ in range(3):\n",
    "                        if np.random.rand() < 0.7:\n",
    "                            inst = [\n",
    "                                float(round(float(p) * (grid_size - 1)))\n",
    "                                for p in pred_params\n",
    "                            ]\n",
    "                        else:\n",
    "                            inst = [\n",
    "                                float(np.random.randint(0, grid_size))\n",
    "                                for _ in range(arity)\n",
    "                            ]\n",
    "                        instantiations.append(inst)\n",
    "                else:\n",
    "                    instantiations.append([])\n",
    "\n",
    "                # ----- create new beam children -----\n",
    "                for inst_params in instantiations:\n",
    "                    new_prog = prog_parent.extend(prim_idx, inst_params)\n",
    "                    expansions.append((new_prog, parent_score + prim_logp))\n",
    "\n",
    "        if not expansions:\n",
    "            break\n",
    "\n",
    "        # ---- 3) Score all expanded programs ----\n",
    "        cand_programs = [p for (p, _) in expansions]\n",
    "        prim_ids_padded, params_padded = batchify_programs(cand_programs, padding_vals=[EOS_IDX, -1])\n",
    "\n",
    "        def get_pencoder(seq_len):\n",
    "            return ProgramEncoder(num_primitives= len(PRIMITIVE_TEMPLATES),\n",
    "                    param_cardinalities= [grid_size, grid_size],\n",
    "                    seq_len= seq_len,\n",
    "                    max_params_per_primitive= 2)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            program_encoder = get_pencoder(prim_ids_padded.shape[-1])\n",
    "            prog_emb_batch = program_encoder(prim_ids_padded, params_padded)\n",
    "\n",
    "        scores = score_fn(z, prog_emb_batch)\n",
    "\n",
    "        # ---- 4) attach scores and prune ----\n",
    "        candidates = []\n",
    "        for idx_c, (prog, base_score) in enumerate(expansions):\n",
    "            total = lambda_1 * base_score + lambda_2 * float(scores[idx_c].item()) - lambda_3 * len(prog.tokens) \n",
    "            candidates.append((prog, total))\n",
    "\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        # best_program, best_score = max(candidates, key=lambda x: x[1])\n",
    "\n",
    "        topk_from_alive = candidates#[:beam_width]\n",
    "        beam = topk_from_alive + finished\n",
    "\n",
    "        filtered_beam = []\n",
    "        for p in beam:\n",
    "            if len(p[0].tokens) == 1 and p[0].tokens[0][0] == -1:\n",
    "                continue\n",
    "            filtered_beam.append(p)\n",
    "        if len(filtered_beam) == 0:\n",
    "            break\n",
    "\n",
    "        best_program, best_score = max(filtered_beam, key=lambda x: x[1])\n",
    "        beam = sorted(beam, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "        # update alive + finished splits\n",
    "        alive  = [(p,s) for (p,s) in beam if not p.finished]\n",
    "        finished = [(p,s) for (p,s) in beam if p.finished]\n",
    "\n",
    "    return best_program, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99377140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from MAWM.models.program_encoder import ProgramPredictor, ProgramEncoder\n",
    "\n",
    "import torch\n",
    "def loss_fn(z_hat, z, loss_exp= 1):\n",
    "    return torch.mean(torch.abs(z_hat - z) ** loss_exp) / loss_exp\n",
    "\n",
    "def score_fn(z, m, predictor= None):\n",
    "    predictor = ProgramPredictor()\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        z_hat = predictor(m)\n",
    "    \n",
    "    return [-loss_fn(z_hat[i].unsqueeze(0), z) for i in range(z_hat.size(0))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952113fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from MAWM.models.program_embedder import ProgramEmbedder, PRIMITIVE_TEMPLATES\n",
    "from MAWM.models.program_encoder import ProgramEncoder\n",
    "from MAWM.core import *\n",
    "from MAWM.models.program_synthizer import Proposer\n",
    "import torch\n",
    "z = torch.randn(1, 32)\n",
    "device = 'cpu'\n",
    "grid_size= 7\n",
    "\n",
    "num_primitives = len(PRIMITIVE_TEMPLATES)\n",
    "p_embed = ProgramEmbedder(\n",
    "    num_primitives= num_primitives,\n",
    "    param_cardinalities= [7, 7],\n",
    "    max_params_per_primitive= 2,\n",
    "    d_name= 32,\n",
    "    d_param= 32,\n",
    ")\n",
    "\n",
    "p_encoder = ProgramEncoder(num_primitives, [grid_size, grid_size],2, seq_len=1)\n",
    "proposer = Proposer(obs_dim= 32,\n",
    "                    num_prims= num_primitives,\n",
    "                    max_params= 2,\n",
    "                    seq_len= 1,\n",
    "                    prog_emb_dim_x= 32,\n",
    "                    prog_emb_dim_y= 32,\n",
    "                    prog_emb_dim_prims= 32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6799b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst_res = []\n",
    "# for i in range(1000):\n",
    "#     z = torch.randn(1, 32).to(device)\n",
    "#     res = neural_guided_beam_search(z, p_embed, proposer, p_encoder, score_fn, beam_width=3, topk=4, max_prog_len= 10, lambdas= (0.7, 0.2, 0.01), device= device)\n",
    "#     lst_res.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77685453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OtherAgentDirection(-1, -1, -1) | CellEmpty(1.0, 0.0) | OtherAgentDirection(3.0,) | GoalAt(3.0, 3.0) | ItemAt(0.0, 1.0) | GoalAt(3.0, 3.0) | CellObstacle(3.0, 3.0) | CellGoal(3.0, 3.0) | GoalAt(3.0, 3.0) | CellAgent(2.0, 4.0) | OtherAgentNear()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# idx = np.random.choice(len(lst_res))\n",
    "# print(idx)\n",
    "# lst_res[idx][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b590d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.716"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum([len(lst_res[idx][0]) for idx in range(len(lst_res))]) / len(lst_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69feff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
