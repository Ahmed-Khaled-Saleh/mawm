{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b4bdaba",
   "metadata": {},
   "source": [
    "# Vision Encoder model\n",
    "\n",
    "> CNN for image feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b1ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.vision.__init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b4c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f9d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributions as td\n",
    "import torch.nn as nn\n",
    "from fastcore.utils import *\n",
    "from fastcore.all import *\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2364280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "ENCODER_LAYERS_CONFIG = {\n",
    "    # L1\n",
    "    \"a\": [(2, 32, 5, 1, 0), (32, 32, 4, 2, 0), (32, 32, 3, 1, 0), (32, 16, 1, 1, 0)],\n",
    "    \"b\": [(2, 16, 5, 1, 0), (16, 32, 4, 2, 0), (32, 32, 3, 1, 0), (32, 16, 1, 1, 0)],\n",
    "    \"c\": [(2, 16, 5, 1, 0), (16, 16, 4, 2, 0), (16, 16, 3, 1, 0)],\n",
    "    \"f\": [(2, 16, 5, 1, 0), (16, 16, 5, 2, 0), (16, 16, 5, 1, 2)],\n",
    "    \"g\": [(2, 32, 5, 1, 0), (32, 32, 5, 2, 0), (32, 32, 5, 1, 2), (32, 16, 1, 1, 0)],\n",
    "    \"h\": [(2, 16, 5, 1, 0), (16, 16, 5, 2, 0), (16, 16, 3, 1, 0)],\n",
    "    \"i\": [(2, 16, 5, 1, 0), (16, 16, 5, 2, 0), (16, 16, 3, 1, 1)],\n",
    "    \"i_fc\": [\n",
    "        (2, 16, 5, 1, 0),\n",
    "        (16, 16, 5, 2, 0),\n",
    "        (16, 16, 3, 1, 1),\n",
    "        (\"fc\", 13456, 512),\n",
    "    ],\n",
    "    \"i_b\": [(6, 16, 5, 1, 0), (16, 16, 5, 2, 0), (16, 16, 3, 1, 0)],\n",
    "    \"d4rl_a\": [\n",
    "        (6, 16, 5, 1, 0),\n",
    "        (16, 32, 5, 2, 0),\n",
    "        (32, 32, 3, 1, 0),\n",
    "        (32, 32, 3, 1, 1),\n",
    "        (32, 16, 1, 1, 0),\n",
    "    ],\n",
    "    \"d4rl_b\": [\n",
    "        (6, 16, 5, 1, 0),\n",
    "        (16, 32, 5, 2, 0),\n",
    "        (32, 32, 3, 1, 0),\n",
    "        (32, 32, 3, 1, 1),\n",
    "        (32, 32, 3, 1, 1),\n",
    "        (32, 16, 1, 1, 0),\n",
    "    ],\n",
    "    \"d4rl_c\": [\n",
    "        (6, 16, 5, 1, 0),\n",
    "        (16, 32, 5, 2, 0),\n",
    "        (32, 32, 3, 1, 0),\n",
    "        (32, 32, 3, 1, 1),\n",
    "        (32, 32, 3, 1, 1),\n",
    "    ],\n",
    "    \"j\": [(2, 32, 5, 1, 0), (32, 32, 5, 2, 0), (32, 32, 3, 1, 1), (32, 16, 1, 1, 0)],\n",
    "    \"k\": [(2, 16, 5, 1, 0), (16, 32, 5, 2, 0), (32, 32, 3, 1, 1), (32, 16, 1, 1, 0)],\n",
    "    # L2\n",
    "    \"d\": [(16, 16, 3, 1, 0), (16, 16, 3, 1, 0)],\n",
    "    \"e\": [\n",
    "        (\"pad\", (0, 1, 0, 1)),\n",
    "        (16, 16, 3, 1, 0),\n",
    "        (\"avg_pool\", 2, 2, 0),\n",
    "        (16, 16, 3, 1, 0),\n",
    "    ],\n",
    "    \"l2a\": [(16, 16, 5, 1, 2), (16, 16, 5, 2, 2), (16, 16, 3, 1, 1)],  # (8, 16, 15, 15)\n",
    "    \"l2b\": [(16, 16, 3, 1, 1), (16, 16, 3, 2, 1), (16, 16, 3, 1, 1)],  # (8, 16, 15, 15)\n",
    "    \"l2c\": [(16, 32, 5, 1, 2), (32, 32, 5, 2, 2), (32, 32, 3, 1, 1)],  # (8, 32, 15, 15)\n",
    "    \"l2d\": [(16, 32, 3, 1, 1), (32, 32, 3, 2, 1), (32, 32, 3, 1, 1)],  # (8, 32, 15, 15)\n",
    "    \"l2e\": [(16, 16, 3, 2, 1), (16, 16, 3, 1, 1)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e05351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 16, 5, 1, 0),\n",
       " (16, 32, 5, 2, 0),\n",
       " (32, 32, 3, 1, 0),\n",
       " (32, 32, 3, 1, 1),\n",
       " (32, 16, 1, 1, 0)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "ENCODER_LAYERS_CONFIG['d4rl_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PassThrough(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab1181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from mawm.models.vision.enums import BackboneOutput\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self, output_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(output_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.flatten(1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = BackboneOutput(encodings=out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e55886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from mawm.models.vision.enums import BackboneOutput\n",
    "from mawm.models.vision.base import SequenceBackbone\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MeNet5(SequenceBackbone):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        output_dim: int = 64,\n",
    "        input_channels: int = 1,\n",
    "        width_factor: int = 1,\n",
    "        conv_out_dim: int = 9 * 32,\n",
    "        backbone_norm: str = \"batch_norm\",\n",
    "        backbone_pool: str = \"backbone_pool\",\n",
    "        backbone_final_fc: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.width_factor = width_factor\n",
    "        self.conv_out_dim = conv_out_dim\n",
    "        self.backbone_final_fc = backbone_final_fc\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                input_channels, 16 * width_factor, kernel_size=5, stride=2, padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            (\n",
    "                nn.BatchNorm2d(16 * width_factor)\n",
    "                if backbone_norm == \"batch_norm\"\n",
    "                else nn.GroupNorm(4, 16 * width_factor)\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                16 * width_factor, 32 * width_factor, kernel_size=5, stride=2, padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            (\n",
    "                nn.BatchNorm2d(32 * width_factor)\n",
    "                if backbone_norm == \"batch_norm\"\n",
    "                else nn.GroupNorm(4, 32 * width_factor)\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                32 * width_factor, 32 * width_factor, kernel_size=3, stride=1, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            (\n",
    "                nn.BatchNorm2d(32 * width_factor)\n",
    "                if backbone_norm == \"batch_norm\"\n",
    "                else nn.GroupNorm(4, 32 * width_factor)\n",
    "            ),\n",
    "        )\n",
    "        if backbone_pool == \"avg_pool\":\n",
    "            self.pool = nn.AvgPool2d(2, stride=2)\n",
    "        else:\n",
    "            self.pool = nn.Conv2d(\n",
    "                in_channels=32 * width_factor, out_channels=32, kernel_size=1\n",
    "            )\n",
    "        sample_input = torch.randn(input_dim).unsqueeze(0)\n",
    "        sample_output = self.pool(self.layer1(sample_input)).reshape(1, -1)\n",
    "        if backbone_final_fc:\n",
    "            self.fc = nn.Linear(sample_output.shape[-1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)  # [bs,64,16,16]\n",
    "        out = self.pool(out)  # [bs, 32, 16, 16]\n",
    "        if self.backbone_final_fc:\n",
    "            out = out.reshape(out.size(0), -1)\n",
    "            out = self.fc(out)\n",
    "        out = BackboneOutput(encodings=out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77425d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ResizeConv2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        scale_factor,\n",
    "        mode=\"nearest\",\n",
    "        groups=1,\n",
    "        bias=False,\n",
    "        padding=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=1,\n",
    "            padding=padding,\n",
    "            groups=groups,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n",
    "        x = self.conv(x)\n",
    "        x = BackboneOutput(encodings=x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e197269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Canonical(nn.Module):\n",
    "    def __init__(self, output_dim: int = 64):\n",
    "        super().__init__()\n",
    "        res = int(np.sqrt(output_dim / 64))\n",
    "        assert (\n",
    "            res * res * 64 == output_dim\n",
    "        ), \"canonical backbone resolution error: cant fit desired output_dim\"\n",
    "\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((res, res)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(1)\n",
    "        x = BackboneOutput(encodings=x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75292fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from mawm.models.misc import  (\n",
    "    build_mlp,\n",
    "    Projector,\n",
    "    MLP,\n",
    "    build_norm1d,\n",
    "    PartialAffineLayerNorm,\n",
    ")\n",
    "\n",
    "class MLPEncoder(SequenceBackbone):\n",
    "    def __init__(self, cfg, input_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = MLP(\n",
    "            arch=cfg.backbone_subclass,\n",
    "            input_dim=input_dim,\n",
    "            norm=cfg.backbone_norm,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = BackboneOutput(encodings=x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f8a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ObposEncoder1(SequenceBackbone):\n",
    "    \"\"\"\n",
    "    Fused encoder for observation and pos state.\n",
    "    cat(obs, pos) --> encoder --> encodings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, obs_dim):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.encoder = MLP(\n",
    "            arch=config.backbone_subclass,\n",
    "            input_dim=obs_dim + config.pos_dim,\n",
    "            norm=config.backbone_norm,\n",
    "            activation=\"mish\",\n",
    "        )\n",
    "\n",
    "        out_dim = int(config.backbone_subclass.split(\"-\")[-1])\n",
    "\n",
    "        if config.final_ln:\n",
    "            self.final_ln = build_norm1d(config.backbone_norm, out_dim)\n",
    "        else:\n",
    "            self.final_ln = nn.Identity()\n",
    "\n",
    "    def forward(self, obs, pos):\n",
    "        x = torch.cat([obs, pos], dim=-1)\n",
    "        x = self.encoder(x)\n",
    "        x = self.final_ln(x)\n",
    "\n",
    "        return BackboneOutput(encodings=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a4854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ObposEncoder2(SequenceBackbone):\n",
    "    \"\"\"\n",
    "    Distangled encoder for observation and pos state.\n",
    "    obs --> obs_encoder --> obs_out\n",
    "    pos --> pos_encoder --> pos_out\n",
    "    encodings = cat(obs_out, pos_out)\n",
    "    return: encodings, obs_out, pos_out\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, obs_dim):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        obs_subclass, pos_subclass = config.backbone_subclass.split(\",\")\n",
    "\n",
    "        if obs_subclass == \"id\":\n",
    "            self.obs_encoder = nn.Identity()\n",
    "            obs_out_dim = obs_dim\n",
    "        else:\n",
    "            self.obs_encoder = build_mlp(\n",
    "                layers_dims=obs_subclass,\n",
    "                input_dim=obs_dim,\n",
    "                norm=config.backbone_norm,\n",
    "                activation=\"mish\",\n",
    "            )\n",
    "            obs_out_dim = int(obs_subclass.split(\"-\")[-1])\n",
    "\n",
    "        if pos_subclass == \"id\":\n",
    "            self.pos_encoder = nn.Identity()\n",
    "            pos_out_dim = config.pos_dim\n",
    "        else:\n",
    "            self.pos_encoder = build_mlp(\n",
    "                layers_dims=pos_subclass,\n",
    "                input_dim=config.pos_dim,\n",
    "                norm=config.backbone_norm,\n",
    "                activation=\"mish\",\n",
    "            )\n",
    "            pos_out_dim = int(pos_subclass.split(\"-\")[-1])\n",
    "\n",
    "        if config.final_ln:\n",
    "            self.final_ln = PartialAffineLayerNorm(\n",
    "                first_dim=obs_out_dim,\n",
    "                second_dim=pos_out_dim,\n",
    "                first_affine=(obs_subclass != \"id\"),\n",
    "                second_affine=(pos_subclass != \"id\"),\n",
    "            )\n",
    "        else:\n",
    "            self.final_ln = nn.Identity()\n",
    "\n",
    "    def forward(self, obs, pos):\n",
    "        obs_out = self.obs_encoder(obs)\n",
    "        pos_out = self.pos_encoder(pos)\n",
    "\n",
    "        next_state = torch.cat([obs_out, pos_out], dim=1)\n",
    "        next_state = self.final_ln(next_state)\n",
    "\n",
    "        return BackboneOutput(\n",
    "            encodings=next_state,\n",
    "            obs_component=obs_out,\n",
    "            pos_component=pos_out,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e968fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from mawm.models.utils import Expander2D, build_conv\n",
    "\n",
    "class MeNet6(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        input_dim: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.input_dim = input_dim\n",
    "        subclass = config.backbone_subclass\n",
    "        layers_config = ENCODER_LAYERS_CONFIG[subclass]\n",
    "\n",
    "        if \"l2\" in subclass:\n",
    "            # add prenormalization and relu layers?\n",
    "            pre_conv = nn.Sequential(nn.GroupNorm(4, layers_config[0][0]), nn.ReLU())\n",
    "        else:\n",
    "            pre_conv = nn.Identity()\n",
    "        conv_layers = build_conv(layers_config, (input_dim[0],))\n",
    "\n",
    "        self.layers = nn.Sequential(pre_conv, conv_layers)\n",
    "\n",
    "        if config.position_dim:\n",
    "            # infer output dim of encoder\n",
    "            sample_input = torch.randn(input_dim).unsqueeze(0) # [1, C, H, W]\n",
    "            sample_output = self.layers(sample_input)\n",
    "            encoder_output_dim = tuple(sample_output.shape[1:])\n",
    "\n",
    "            if (\n",
    "                self.config.position_encoder_arch\n",
    "                and self.config.position_encoder_arch != \"id\"\n",
    "            ):\n",
    "                layer_dims = [\n",
    "                    int(x) for x in self.config.position_encoder_arch.split(\"-\")\n",
    "                ]\n",
    "                layers = []\n",
    "                for i in range(len(layer_dims) - 1):\n",
    "                    layers.append(nn.Linear(layer_dims[i], layer_dims[i + 1]))\n",
    "                    layers.append(nn.ReLU())\n",
    "                # remove last ReLU\n",
    "                layers.pop()\n",
    "\n",
    "                self.position_encoder = nn.Sequential(\n",
    "                    *layers,\n",
    "                    Expander2D(w=encoder_output_dim[-2], h=encoder_output_dim[-1]),\n",
    "                )\n",
    "            else:\n",
    "                self.position_encoder = Expander2D(\n",
    "                    w=encoder_output_dim[-2], h=encoder_output_dim[-1]\n",
    "                )\n",
    "        \n",
    "    @property\n",
    "    def repr_dim(self):\n",
    "        with torch.no_grad():\n",
    "            sample_inp = torch.randn(self.input_dim).unsqueeze(0)  # [1, C, H, W]\n",
    "            sample_out = self.layers(sample_inp)\n",
    "            encoder_output_dim = tuple(sample_out.shape[1:])\n",
    "            if self.config.position_dim:\n",
    "                return (self.config.position_dim + encoder_output_dim[0], encoder_output_dim[1], encoder_output_dim[2])\n",
    "            return encoder_output_dim \n",
    "    \n",
    "    \n",
    "    def forward(self, x, position=None):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            x: [T, BS, *] or [BS, T, *]\n",
    "        output:\n",
    "            x: [T, BS, *] or [T, BS, *]\n",
    "        \"\"\"\n",
    "        time= False\n",
    "        if x.dim() == 2 or x.dim() == 4:\n",
    "            encoded_obs = self.layers(x) # # [BS, C, H, W]\n",
    "        else:\n",
    "            time= True\n",
    "            obs = x.flatten(0, 1) # [T*BS, C, H, W]\n",
    "            encoded_obs = self.layers(obs) # [T*BS, 16, 10, 10]\n",
    "\n",
    "        if position is not None:\n",
    "            if position.dim() == 3:\n",
    "                position = position.flatten(0, 1)  # [T*BS, pos_dim]\n",
    "            encoded_pos = self.position_encoder(position)\n",
    "            z = torch.cat([encoded_obs, encoded_pos], dim=1)\n",
    "        else:\n",
    "            encoded_pos = None\n",
    "            z = encoded_obs\n",
    "            \n",
    "        if time:\n",
    "            new_shape = x.shape[:2] + z.shape[1:]\n",
    "            z = z.reshape(new_shape)\n",
    "            return z\n",
    "        return z\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ce407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from mawm.models.vision.enums import BackboneConfig\n",
    "from mawm.models.vision.resnet import resnet18, resnet18ID\n",
    "from mawm.models.misc import build_mlp,Projector, MLP, build_norm1d, PartialAffineLayerNorm\n",
    "\n",
    "def build_backbone(\n",
    "    config: BackboneConfig,\n",
    "    input_dim,\n",
    "):\n",
    "    backbone, embedding = None, None\n",
    "    arch = config.arch\n",
    "\n",
    "    backbone = MeNet6(\n",
    "            config=config,\n",
    "            input_dim=input_dim,\n",
    "        )\n",
    "    \n",
    "    if config.backbone_mlp is not None:\n",
    "        backbone_mlp = Projector(config.backbone_mlp, embedding)\n",
    "        backbone = nn.Sequential(backbone, backbone_mlp)\n",
    "\n",
    "    backbone.input_dim = input_dim\n",
    "    sample_input = torch.randn(input_dim).unsqueeze(0)\n",
    "\n",
    "    if config.position_dim is not None:\n",
    "        sample_position_input = torch.randn(config.position_dim).unsqueeze(0)\n",
    "        sample_output = backbone(sample_input, position=sample_position_input)\n",
    "    else:\n",
    "        sample_output = backbone(sample_input)\n",
    "\n",
    "    output_dim = tuple(sample_output.encodings.shape[1:])\n",
    "    output_dim = output_dim[0] if len(output_dim) == 1 else output_dim\n",
    "    backbone.output_dim = output_dim\n",
    "\n",
    "    if sample_output.pos_component is not None:\n",
    "        output_obs_dim = tuple(sample_output.obs_component.shape[1:])\n",
    "        output_obs_dim = (\n",
    "            output_obs_dim[0] if len(output_obs_dim) == 1 else output_obs_dim\n",
    "        )\n",
    "        output_position_dim = tuple(sample_output.pos_component.shape[1:])\n",
    "        output_position_dim = (\n",
    "            output_position_dim[0] if len(output_position_dim) == 1 else output_position_dim\n",
    "        )\n",
    "    else:\n",
    "        output_obs_dim = output_dim\n",
    "        output_position_dim = 0\n",
    "\n",
    "    backbone.output_obs_dim = output_obs_dim\n",
    "    backbone.output_position_dim = output_position_dim\n",
    "\n",
    "    backbone.config = config\n",
    "\n",
    "    return backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b3ab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from omegaconf import OmegaConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e262888c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arch': 'MeNet6', 'backbone_subclass': 'd4rl_a', 'backbone_mlp': None, 'backbone_width_factor': 2, 'input_dim': 4, 'channels': 3, 'position_dim': 2, 'position_encoder_arch': 'id'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "cfg = OmegaConf.load(\"../cfgs/MPCJepa/mpc.yaml\")\n",
    "\n",
    "cfg.model.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c668905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "model = MeNet6(\n",
    "    config=cfg.model.backbone,\n",
    "    input_dim=(3, cfg.model.img_size, cfg.model.img_size),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d90012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MeNet6(\n",
       "  (layers): Sequential(\n",
       "    (0): Identity()\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (1): GroupNorm(4, 16, eps=1e-05, affine=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "      (4): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (7): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (8): ReLU()\n",
       "      (9): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (10): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (11): ReLU()\n",
       "      (12): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (position_encoder): Expander2D()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac9f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "T = 8\n",
    "BS = 16\n",
    "inp = torch.randn(BS, T, 3, 42, 42)\n",
    "pos = torch.randn(BS, T, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725062b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8, 18, 15, 15])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "out = model(inp, position=pos)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf170bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 15, 15)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "model.repr_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7494c7",
   "metadata": {},
   "source": [
    "### MSG Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f74aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "class SemanticEncoder(nn.Module):\n",
    "    def __init__(self, num_primitives=5, latent_dim=32):\n",
    "        self.latent_dim = latent_dim\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(num_primitives, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 3 * 3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = rearrange(x, 'b t c h w -> (b t) c h w')\n",
    "        x = self.net(x) # [B*T, latent_dim]\n",
    "        x = rearrange(x, '(b t) d -> b t d', b= B)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f40c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8, 32])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "model = SemanticEncoder(num_primitives=5, latent_dim=32)\n",
    "inp = torch.randn(16, 8, 5, 7, 7)\n",
    "out = model(inp)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8, 32])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "from mawm.models.utils import flatten_conv_output\n",
    "out_flat = flatten_conv_output(out)\n",
    "out_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6708002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8, 32, 15, 15])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "from mawm.models.utils import Expander2D\n",
    "inp = out_flat\n",
    "m = Expander2D(w= 15, h=15)\n",
    "out = m(inp)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3db35f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
