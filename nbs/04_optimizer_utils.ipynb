{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities for Optimization\n",
    "\n",
    "> This module handles all aspects of the world model, including state representation, environment dynamics, and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp optimizer.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import *\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from functools import partial\n",
    "from torch.optim import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Source - https://stackoverflow.com/a\n",
    "# Posted by isle_of_gods, modified by community. See post 'Timeline' for change history\n",
    "# Retrieved 2025-11-15, License - CC BY-SA 4.0\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EarlyStopping(object): # pylint: disable=R0902\n",
    "    \"\"\"\n",
    "    Gives a criterion to stop training when a given metric is not\n",
    "    improving anymore\n",
    "    Args:\n",
    "        mode (str): One of `min`, `max`. In `min` mode, training will\n",
    "            be stopped when the quantity monitored has stopped\n",
    "            decreasing; in `max` mode it will be stopped when the\n",
    "            quantity monitored has stopped increasing. Default: 'min'.\n",
    "        patience (int): Number of epochs with no improvement after\n",
    "            which training is stopped. For example, if\n",
    "            `patience = 2`, then we will ignore the first 2 epochs\n",
    "            with no improvement, and will only stop learning after the\n",
    "            3rd epoch if the loss still hasn't improved then.\n",
    "            Default: 10.\n",
    "        threshold (float): Threshold for measuring the new optimum,\n",
    "            to only focus on significant changes. Default: 1e-4.\n",
    "        threshold_mode (str): One of `rel`, `abs`. In `rel` mode,\n",
    "            dynamic_threshold = best * ( 1 + threshold ) in 'max'\n",
    "            mode or best * ( 1 - threshold ) in `min` mode.\n",
    "            In `abs` mode, dynamic_threshold = best + threshold in\n",
    "            `max` mode or best - threshold in `min` mode. Default: 'rel'.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mode='min', patience=10, threshold=1e-4, threshold_mode='rel'):\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "        self.threshold_mode = threshold_mode\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = None\n",
    "        self.mode_worse = None  # the worse value for the chosen mode\n",
    "        self.is_better = None\n",
    "        self.last_epoch = -1\n",
    "        self._init_is_better(mode=mode, threshold=threshold,\n",
    "                             threshold_mode=threshold_mode)\n",
    "        self._reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _reset(self: EarlyStopping):\n",
    "        \"\"\"Resets num_bad_epochs counter and cooldown counter.\"\"\"\n",
    "        self.best = self.mode_worse\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "@patch\n",
    "def step(self: EarlyStopping, metrics, epoch=None):\n",
    "    \"\"\" Updates early stopping state \"\"\"\n",
    "    current = metrics\n",
    "    if epoch is None:\n",
    "        epoch = self.last_epoch = self.last_epoch + 1\n",
    "    self.last_epoch = epoch\n",
    "\n",
    "    if self.is_better(current, self.best):\n",
    "        self.best = current\n",
    "        self.num_bad_epochs = 0\n",
    "    else:\n",
    "        self.num_bad_epochs += 1\n",
    "\n",
    "@property\n",
    "@patch\n",
    "def stop(self: EarlyStopping):\n",
    "    \"\"\" Should we stop learning? \"\"\"\n",
    "    return self.num_bad_epochs > self.patience\n",
    "\n",
    "\n",
    "@patch\n",
    "def _cmp(self: EarlyStopping, mode, threshold_mode, threshold, a, best): # pylint: disable=R0913, R0201\n",
    "    if mode == 'min' and threshold_mode == 'rel':\n",
    "        rel_epsilon = 1. - threshold\n",
    "        return a < best * rel_epsilon\n",
    "\n",
    "    elif mode == 'min' and threshold_mode == 'abs':\n",
    "        return a < best - threshold\n",
    "\n",
    "    elif mode == 'max' and threshold_mode == 'rel':\n",
    "        rel_epsilon = threshold + 1.\n",
    "        return a > best * rel_epsilon\n",
    "\n",
    "    return a > best + threshold\n",
    "\n",
    "@patch\n",
    "def _init_is_better(self: EarlyStopping, mode, threshold, threshold_mode):\n",
    "    if mode not in {'min', 'max'}:\n",
    "        raise ValueError('mode ' + mode + ' is unknown!')\n",
    "    if threshold_mode not in {'rel', 'abs'}:\n",
    "        raise ValueError('threshold mode ' + threshold_mode + ' is unknown!')\n",
    "\n",
    "    if mode == 'min':\n",
    "        self.mode_worse = float('inf')\n",
    "    else:  # mode == 'max':\n",
    "        self.mode_worse = (-float('inf'))\n",
    "\n",
    "    self.is_better = partial(self._cmp, mode, threshold_mode, threshold)\n",
    "\n",
    "@patch\n",
    "def state_dict(self: EarlyStopping):\n",
    "    \"\"\" Returns early stopping state \"\"\"\n",
    "    return {key: value for key, value in self.__dict__.items() if key != 'is_better'}\n",
    "\n",
    "@patch\n",
    "def load_state_dict(self: EarlyStopping, state_dict):\n",
    "    \"\"\" Loads early stopping state \"\"\"\n",
    "    self.__dict__.update(state_dict)\n",
    "    self._init_is_better(mode=self.mode, threshold=self.threshold,\n",
    "                            threshold_mode=self.threshold_mode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ReduceLROnPlateau(object): # pylint: disable=R0902\n",
    "    \"\"\"Reduce learning rate when a metric has stopped improving.\n",
    "    Models often benefit from reducing the learning rate by a factor\n",
    "    of 2-10 once learning stagnates. This scheduler reads a metrics\n",
    "    quantity and if no improvement is seen for a 'patience' number\n",
    "    of epochs, the learning rate is reduced.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        mode (str): One of `min`, `max`. In `min` mode, lr will\n",
    "            be reduced when the quantity monitored has stopped\n",
    "            decreasing; in `max` mode it will be reduced when the\n",
    "            quantity monitored has stopped increasing. Default: 'min'.\n",
    "        factor (float): Factor by which the learning rate will be\n",
    "            reduced. new_lr = lr * factor. Default: 0.1.\n",
    "        patience (int): Number of epochs with no improvement after\n",
    "            which learning rate will be reduced. For example, if\n",
    "            `patience = 2`, then we will ignore the first 2 epochs\n",
    "            with no improvement, and will only decrease the LR after the\n",
    "            3rd epoch if the loss still hasn't improved then.\n",
    "            Default: 10.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "        threshold (float): Threshold for measuring the new optimum,\n",
    "            to only focus on significant changes. Default: 1e-4.\n",
    "        threshold_mode (str): One of `rel`, `abs`. In `rel` mode,\n",
    "            dynamic_threshold = best * ( 1 + threshold ) in 'max'\n",
    "            mode or best * ( 1 - threshold ) in `min` mode.\n",
    "            In `abs` mode, dynamic_threshold = best + threshold in\n",
    "            `max` mode or best - threshold in `min` mode. Default: 'rel'.\n",
    "        cooldown (int): Number of epochs to wait before resuming\n",
    "            normal operation after lr has been reduced. Default: 0.\n",
    "        min_lr (float or list): A scalar or a list of scalars. A\n",
    "            lower bound on the learning rate of all param groups\n",
    "            or each group respectively. Default: 0.\n",
    "        eps (float): Minimal decay applied to lr. If the difference\n",
    "            between new and old lr is smaller than eps, the update is\n",
    "            ignored. Default: 1e-8.\n",
    "\n",
    "    Example:\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        >>> scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "        >>> for epoch in range(10):\n",
    "        >>>     train(...)\n",
    "        >>>     val_loss = validate(...)\n",
    "        >>>     # Note that step should be called after validate()\n",
    "        >>>     scheduler.step(val_loss)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, mode='min', factor=0.1, patience=10, # pylint: disable=R0913\n",
    "                 verbose=False, threshold=1e-4, threshold_mode='rel',\n",
    "                 cooldown=0, min_lr=0, eps=1e-8):\n",
    "\n",
    "        if factor >= 1.0:\n",
    "            raise ValueError('Factor should be < 1.0.')\n",
    "        self.factor = factor\n",
    "\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(min_lr, (list, tuple)):\n",
    "            if len(min_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} min_lrs, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(min_lr)))\n",
    "            self.min_lrs = list(min_lr)\n",
    "        else:\n",
    "            self.min_lrs = [min_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.cooldown = cooldown\n",
    "        self.cooldown_counter = 0\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "        self.threshold_mode = threshold_mode\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = None\n",
    "        self.mode_worse = None  # the worse value for the chosen mode\n",
    "        self.is_better = None\n",
    "        self.eps = eps\n",
    "        self.last_epoch = -1\n",
    "        self._init_is_better(mode=mode, threshold=threshold,\n",
    "                             threshold_mode=threshold_mode)\n",
    "        self._reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _reset(self: ReduceLROnPlateau):\n",
    "    \"\"\"Resets num_bad_epochs counter and cooldown counter.\"\"\"\n",
    "    self.best = self.mode_worse\n",
    "    self.cooldown_counter = 0\n",
    "    self.num_bad_epochs = 0\n",
    "\n",
    "@patch\n",
    "def step(self: ReduceLROnPlateau, metrics, epoch=None):\n",
    "    \"\"\" Updates scheduler state \"\"\"\n",
    "    current = metrics\n",
    "    if epoch is None:\n",
    "        epoch = self.last_epoch = self.last_epoch + 1\n",
    "    self.last_epoch = epoch\n",
    "\n",
    "    if self.is_better(current, self.best):\n",
    "        self.best = current\n",
    "        self.num_bad_epochs = 0\n",
    "    else:\n",
    "        self.num_bad_epochs += 1\n",
    "\n",
    "    if self.in_cooldown:\n",
    "        self.cooldown_counter -= 1\n",
    "        self.num_bad_epochs = 0  # ignore any bad epochs in cooldown\n",
    "\n",
    "    if self.num_bad_epochs > self.patience:\n",
    "        self._reduce_lr(epoch)\n",
    "        self.cooldown_counter = self.cooldown\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "@patch\n",
    "def _reduce_lr(self: ReduceLROnPlateau, epoch):\n",
    "    for i, param_group in enumerate(self.optimizer.param_groups):\n",
    "        old_lr = float(param_group['lr'])\n",
    "        new_lr = max(old_lr * self.factor, self.min_lrs[i])\n",
    "        if old_lr - new_lr > self.eps:\n",
    "            param_group['lr'] = new_lr\n",
    "            if self.verbose:\n",
    "                print('Epoch {:5d}: reducing learning rate'\n",
    "                        ' of group {} to {:.4e}.'.format(epoch, i, new_lr))\n",
    "\n",
    "@property\n",
    "@patch\n",
    "def in_cooldown(self: ReduceLROnPlateau):\n",
    "    \"\"\" Are we on CD? \"\"\"\n",
    "    return self.cooldown_counter > 0\n",
    "\n",
    "@patch\n",
    "def _cmp(self: ReduceLROnPlateau, mode, threshold_mode, threshold, a, best): # pylint: disable=R0913,R0201\n",
    "    if mode == 'min' and threshold_mode == 'rel':\n",
    "        rel_epsilon = 1. - threshold\n",
    "        return a < best * rel_epsilon\n",
    "\n",
    "    elif mode == 'min' and threshold_mode == 'abs':\n",
    "        return a < best - threshold\n",
    "\n",
    "    elif mode == 'max' and threshold_mode == 'rel':\n",
    "        rel_epsilon = threshold + 1.\n",
    "        return a > best * rel_epsilon\n",
    "\n",
    "    return a > best + threshold\n",
    "\n",
    "@patch\n",
    "def _init_is_better(self: ReduceLROnPlateau, mode, threshold, threshold_mode):\n",
    "    if mode not in {'min', 'max'}:\n",
    "        raise ValueError('mode ' + mode + ' is unknown!')\n",
    "    if threshold_mode not in {'rel', 'abs'}:\n",
    "        raise ValueError('threshold mode ' + threshold_mode + ' is unknown!')\n",
    "\n",
    "    if mode == 'min':\n",
    "        self.mode_worse = float('inf')\n",
    "    else:  # mode == 'max':\n",
    "        self.mode_worse = (-float('inf'))\n",
    "\n",
    "    self.is_better = partial(self._cmp, mode, threshold_mode, threshold)\n",
    "\n",
    "@patch\n",
    "def state_dict(self: ReduceLROnPlateau):\n",
    "    \"\"\" Returns scheduler state \"\"\"\n",
    "    return {key: value for key, value in self.__dict__.items()\n",
    "            if key not in {'optimizer', 'is_better'}}\n",
    "\n",
    "@patch\n",
    "def load_state_dict(self: ReduceLROnPlateau, state_dict):\n",
    "    \"\"\" Loads scheduler state \"\"\"\n",
    "    self.__dict__.update(state_dict)\n",
    "    self._init_is_better(mode=self.mode, threshold=self.threshold,\n",
    "                            threshold_mode=self.threshold_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
