{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models utilities module\n",
    "\n",
    "> This module handles all aspects of the world model, including state representation, environment dynamics, and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import *\n",
    "from fastcore.utils import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CellEmpty': 0, 'CellObstacle': 1, 'CellItem': 2, 'CellGoal': 3, 'CellAgent': 4, 'GoalAt': 5, 'ItemAt': 6, 'Near': 7, 'SeeGoal': 8, 'CanMove': 9, 'OtherAgentAt': 10, 'OtherAgentNear': 11, 'OtherAgentDirection': 12}\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from mawm.models.utils import *\n",
    "\n",
    "\n",
    "def build_projector(arch: str, embedding: int):\n",
    "    if arch == \"id\":\n",
    "        return nn.Identity(), embedding\n",
    "    else:\n",
    "        f = [embedding] + list(map(int, arch.split(\"-\")))\n",
    "        return build_mlp(f), f[-1]\n",
    "\n",
    "\n",
    "def build_norm1d(norm: str, dim: int):\n",
    "    if norm == \"batch_norm\":\n",
    "        return torch.nn.BatchNorm1d(dim)\n",
    "    elif norm == \"layer_norm\":\n",
    "        return torch.nn.LayerNorm(dim)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown norm {norm}\")\n",
    "\n",
    "\n",
    "def build_activation(activation: str):\n",
    "    if activation == \"relu\":\n",
    "        return nn.ReLU(True)\n",
    "    elif activation == \"mish\":\n",
    "        return nn.Mish(True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown activation {activation}\")\n",
    "\n",
    "\n",
    "class PartialAffineLayerNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        first_dim: int,\n",
    "        second_dim: int,\n",
    "        first_affine: bool = True,\n",
    "        second_affine: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.first_dim = first_dim\n",
    "        self.second_dim = second_dim\n",
    "\n",
    "        if first_affine:\n",
    "            self.first_ln = nn.LayerNorm(first_dim, elementwise_affine=True)\n",
    "        else:\n",
    "            self.first_ln = nn.LayerNorm(first_dim, elementwise_affine=False)\n",
    "\n",
    "        if second_affine:\n",
    "            self.second_ln = nn.LayerNorm(second_dim, elementwise_affine=True)\n",
    "        else:\n",
    "            self.second_ln = nn.LayerNorm(second_dim, elementwise_affine=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        first = self.first_ln(x[..., : self.first_dim])\n",
    "        second = self.second_ln(x[..., self.first_dim :])\n",
    "        out = torch.cat([first, second], dim=-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "def build_mlp(\n",
    "    layers_dims: Union[List[int], str],\n",
    "    input_dim: int = None,\n",
    "    output_shape: int = None,\n",
    "    norm=\"batch_norm\",\n",
    "    activation=\"relu\",\n",
    "    pre_actnorm=False,\n",
    "    post_norm=False,\n",
    "):\n",
    "    if isinstance(layers_dims, str):\n",
    "        layers_dims = (\n",
    "            list(map(int, layers_dims.split(\"-\"))) if layers_dims != \"\" else []\n",
    "        )\n",
    "\n",
    "    if input_dim is not None:\n",
    "        layers_dims = [input_dim] + layers_dims\n",
    "\n",
    "    if output_shape is not None:\n",
    "        layers_dims = layers_dims + [output_shape]\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    if pre_actnorm:\n",
    "        if norm is not None:\n",
    "            layers.append(build_norm1d(norm, layers_dims[0]))\n",
    "        if activation is not None:\n",
    "            layers.append(build_activation(activation))\n",
    "\n",
    "    for i in range(len(layers_dims) - 2):\n",
    "        layers.append(nn.Linear(layers_dims[i], layers_dims[i + 1]))\n",
    "        if norm is not None:\n",
    "            layers.append(build_norm1d(norm, layers_dims[i + 1]))\n",
    "        if activation is not None:\n",
    "            layers.append(build_activation(activation))\n",
    "\n",
    "    layers.append(nn.Linear(layers_dims[-2], layers_dims[-1]))\n",
    "\n",
    "    if post_norm:\n",
    "        layers.append(build_norm1d(norm, layers_dims[-1]))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        arch: str,\n",
    "        input_dim: int = None,\n",
    "        output_shape: int = None,\n",
    "        norm=None,\n",
    "        activation=\"relu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = build_mlp(\n",
    "            layers_dims=arch,\n",
    "            input_dim=input_dim,\n",
    "            output_shape=output_shape,\n",
    "            norm=norm,\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "PROBER_CONV_LAYERS_CONFIG = {\n",
    "    \"a\": [\n",
    "        (-1, 16, 3, 1, 1),\n",
    "        (\"max_pool\", 2, 2, 0),\n",
    "        (16, 8, 1, 1, 0),\n",
    "        (\"max_pool\", 2, 2, 0),\n",
    "        (\"fc\", -1, 2),\n",
    "    ],\n",
    "    \"b\": [\n",
    "        (-1, 32, 3, 1, 1),\n",
    "        (\"max_pool\", 2, 2, 0),\n",
    "        (32, 32, 3, 1, 1),\n",
    "        (\"max_pool\", 2, 2, 0),\n",
    "        (32, 32, 3, 1, 1),\n",
    "        (\"fc\", -1, 2),\n",
    "    ],\n",
    "    \"c\": [\n",
    "        (-1, 32, 3, 1, 1),\n",
    "        (\"max_pool\", 2, 2, 0),\n",
    "        (32, 32, 3, 1, 1),\n",
    "        (\"max_pool\", 2, 2, 0),\n",
    "        (32, 32, 3, 1, 1),\n",
    "        (32, 32, 3, 1, 1),\n",
    "        (\"fc\", -1, 2),\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "class Prober(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding: int,\n",
    "        arch: str,\n",
    "        output_shape: int,\n",
    "        input_dim=None,\n",
    "        arch_subclass: str = \"a\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.output_shape = output_shape\n",
    "        self.arch = arch\n",
    "\n",
    "        if arch == \"conv\":\n",
    "            self.prober = build_conv(\n",
    "                PROBER_CONV_LAYERS_CONFIG[arch_subclass], input_dim=input_dim\n",
    "            )\n",
    "        else:\n",
    "            arch_list = list(map(int, arch.split(\"-\"))) if arch != \"\" else []\n",
    "            f = [embedding] + arch_list + [self.output_shape]\n",
    "            layers = []\n",
    "            for i in range(len(f) - 2):\n",
    "                layers.append(torch.nn.Linear(f[i], f[i + 1]))\n",
    "                # layers.append(torch.nn.BatchNorm1d(f[i + 1]))\n",
    "                layers.append(torch.nn.ReLU(True))\n",
    "            layers.append(torch.nn.Linear(f[-2], f[-1]))\n",
    "            self.prober = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, e):\n",
    "        if self.arch == \"conv\":\n",
    "            output = self.prober(e)\n",
    "        else:\n",
    "            e = flatten_conv_output(e)\n",
    "            output = self.prober(e)\n",
    "\n",
    "        # output = output.view(*output.shape[:-1], *self.output_shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Projector(torch.nn.Module):\n",
    "    def __init__(self, arch: str, embedding: int, random: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.arch = arch\n",
    "        self.embedding = embedding\n",
    "        self.random = random\n",
    "\n",
    "        self.model, self.output_dim = build_projector(arch, embedding)\n",
    "\n",
    "        if self.random:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def maybe_reinit(self):\n",
    "        if self.random and self.arch != \"id\":\n",
    "            for param in self.parameters():\n",
    "                torch.nn.init.xavier_uniform_(param)\n",
    "                print(\"initialized\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch import nn\n",
    "from einops import rearrange\n",
    "class JepaProjector(nn.Module):\n",
    "    def __init__(self, z_input_dim=4050, c_input_dim=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.z_projector = nn.Sequential(\n",
    "                nn.Linear(z_input_dim, 2048),\n",
    "                nn.BatchNorm1d(2048),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(2048, 128) # 128 is the 'Shared Latent Space'\n",
    "            )\n",
    "\n",
    "        self.msg_projector = nn.Sequential(\n",
    "            nn.Linear(c_input_dim, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 128) # 128 is the 'Shared Latent Space'\n",
    "            )\n",
    "        \n",
    "    def forward(self, z_sender, C):\n",
    "        B, T, D = C.shape\n",
    "        z_sender = rearrange(z_sender, 'b t c h w -> (t b) (c h w)')\n",
    "        proj_z = self.z_projector(z_sender) # [(T*B, dim=128]\n",
    "        proj_z = rearrange(proj_z, '(t b) d -> t b d', b= B)\n",
    "\n",
    "        C = rearrange(rearrange(C, 'b t d -> t b d'), 't b d -> (t b) d')\n",
    "        print(C.shape)\n",
    "        proj_c = self.msg_projector(C) # [(T*B, dim=128]\n",
    "        proj_c = rearrange(proj_c, '(t b) d -> t b d', b= B)\n",
    "        \n",
    "        return  proj_z[:-1], proj_c[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 15, 15)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "from mawm.models.jepa import JEPA\n",
    "from omegaconf import OmegaConf\n",
    "cfg = OmegaConf.load(\"../cfgs/MPCJepa/mpc.yaml\")\n",
    "\n",
    "model = JEPA(cfg.model,input_dim=(3, 42, 42), action_dim= 5)\n",
    "model.backbone.repr_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#| hide\n",
    "from mawm.models.vision import SemanticEncoder\n",
    "msg_enc = SemanticEncoder(latent_dim = 32)\n",
    "msg_enc.latent_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 16, 128]), torch.Size([7, 16, 128]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "T = 8\n",
    "B = 16\n",
    "d_c = 32\n",
    "C = torch.randn(B, T, d_c)\n",
    "z = torch.randn(B, T, 16, 15, 15)\n",
    "from functools import reduce\n",
    "prod = reduce(lambda x, y: x * y, model.backbone.repr_dim)\n",
    "proj = JepaProjector(z_input_dim=prod, c_input_dim=msg_enc.latent_dim)\n",
    "out_z, out_c = proj(z, C)\n",
    "out_z.shape, out_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from omegaconf import OmegaConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arch': 'MeNet6', 'backbone_subclass': 'd4rl_a', 'backbone_mlp': None, 'backbone_width_factor': 2, 'input_dim': 4, 'channels': 3, 'position_dim': 0, 'position_encoder_arch': 'id'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "cfg = OmegaConf.load(\"../cfgs/MPCJepa/mpc.yaml\")\n",
    "\n",
    "cfg.model.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "T = 8\n",
    "BS = 16\n",
    "inp = torch.randn(BS, T, 3, 42, 42)\n",
    "pos = torch.randn(BS, T, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8, 16, 15, 15])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "z = model.backbone(inp, position=None)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A message predictor is a predictor for the message from the latent of the observation. So the input shape is :\n",
    "$$\\mathbf{z} \\in \\mathbb{R}^{B \\times C \\times H \\times W}$$\n",
    "where C is the number of channels of the latent representation of the observation.\n",
    "\n",
    "Now the problem is that, the observation encoder outputs two different shapes depending on whether or not the position is passed. It it's passed, the output shape is:\n",
    "$$\\mathbf{z} \\in \\mathbb{R}^{B \\times 18 \\times 15 \\times 15}$$\n",
    "where H and W are the height and width of the latent representation.\n",
    "Otherwise, the output shape is:\n",
    "\n",
    "$$\\mathbf{z} \\in \\mathbb{R}^{B \\times 16 \\times 15 \\times 15}$$\n",
    "where H and W are the height and width of the latent representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class MsgPred(nn.Module):\n",
    "    def __init__(self, h_dim=32, in_channels=16):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Input: (B, 16, 15, 15)\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, stride=2), # -> (B, 32, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2), # -> (B, 32, 3, 3)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 3 * 3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, h_dim) # Output: 32\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # z shape: (B, T, 16, 15, 15)\n",
    "        if z.dim() == 5:\n",
    "            B, T, C, H, W = z.shape\n",
    "            z = rearrange(z, 'b t c h w -> (b t) c h w')\n",
    "            out = self.net(z)\n",
    "            out = rearrange(out, '(b t) d -> b t d', b= B)\n",
    "            return out\n",
    "        return self.net(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 8, 32])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "from einops import rearrange\n",
    "inp = torch.randn(64, 8, 16, 15, 15)\n",
    "model = MsgPred(h_dim=32, in_channels=16)\n",
    "out = model(inp)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class ObsPred(nn.Module):\n",
    "    def __init__(self, h_dim=32, out_channels= 18):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(h_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16 * 7 * 7),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.upsample = nn.Sequential(\n",
    "            # Input: (B, 16, 7, 7)\n",
    "            nn.ConvTranspose2d(16, 32, kernel_size=3, stride=2), # -> (B, 32, 15, 15)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, out_channels, kernel_size=1) # -> (B, 16, 15, 15)\n",
    "        )\n",
    "\n",
    "    def forward(self, h):\n",
    "        # h shape: (B, T, 32)\n",
    "        B, T, d = h.shape\n",
    "        h = rearrange(h, 'b t d -> (b t) d')\n",
    "        z = self.fc(h)\n",
    "        z = z.view(-1, 16, 7, 7)\n",
    "        z = self.upsample(z)\n",
    "        z = rearrange(z, '(b t) c h w -> b t c h w', b= B)\n",
    "        return z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8, 18, 15, 15])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "from einops import rearrange\n",
    "model = ObsPred(h_dim=32)\n",
    "inp = torch.randn(16, 8, 32)\n",
    "out = model(inp)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
