{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models utilities module\n",
    "\n",
    "> This module handles all aspects of the world model, including state representation, environment dynamics, and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import *\n",
    "from fastcore.utils import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from mawm.models.utils import *\n",
    "\n",
    "\n",
    "def build_projector(arch: str, embedding: int):\n",
    "    if arch == \"id\":\n",
    "        return nn.Identity(), embedding\n",
    "    else:\n",
    "        f = [embedding] + list(map(int, arch.split(\"-\")))\n",
    "        return build_mlp(f), f[-1]\n",
    "\n",
    "\n",
    "def build_norm1d(norm: str, dim: int):\n",
    "    if norm == \"batch_norm\":\n",
    "        return torch.nn.BatchNorm1d(dim)\n",
    "    elif norm == \"layer_norm\":\n",
    "        return torch.nn.LayerNorm(dim)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown norm {norm}\")\n",
    "\n",
    "\n",
    "def build_activation(activation: str):\n",
    "    if activation == \"relu\":\n",
    "        return nn.ReLU(True)\n",
    "    elif activation == \"mish\":\n",
    "        return nn.Mish(True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown activation {activation}\")\n",
    "\n",
    "\n",
    "class PartialAffineLayerNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        first_dim: int,\n",
    "        second_dim: int,\n",
    "        first_affine: bool = True,\n",
    "        second_affine: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.first_dim = first_dim\n",
    "        self.second_dim = second_dim\n",
    "\n",
    "        if first_affine:\n",
    "            self.first_ln = nn.LayerNorm(first_dim, elementwise_affine=True)\n",
    "        else:\n",
    "            self.first_ln = nn.LayerNorm(first_dim, elementwise_affine=False)\n",
    "\n",
    "        if second_affine:\n",
    "            self.second_ln = nn.LayerNorm(second_dim, elementwise_affine=True)\n",
    "        else:\n",
    "            self.second_ln = nn.LayerNorm(second_dim, elementwise_affine=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        first = self.first_ln(x[..., : self.first_dim])\n",
    "        second = self.second_ln(x[..., self.first_dim :])\n",
    "        out = torch.cat([first, second], dim=-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "def build_mlp(\n",
    "    layers_dims: Union[List[int], str],\n",
    "    input_dim: int = None,\n",
    "    output_shape: int = None,\n",
    "    norm=\"batch_norm\",\n",
    "    activation=\"relu\",\n",
    "    pre_actnorm=False,\n",
    "    post_norm=False,\n",
    "):\n",
    "    if isinstance(layers_dims, str):\n",
    "        layers_dims = (\n",
    "            list(map(int, layers_dims.split(\"-\"))) if layers_dims != \"\" else []\n",
    "        )\n",
    "\n",
    "    if input_dim is not None:\n",
    "        layers_dims = [input_dim] + layers_dims\n",
    "\n",
    "    if output_shape is not None:\n",
    "        layers_dims = layers_dims + [output_shape]\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    if pre_actnorm:\n",
    "        if norm is not None:\n",
    "            layers.append(build_norm1d(norm, layers_dims[0]))\n",
    "        if activation is not None:\n",
    "            layers.append(build_activation(activation))\n",
    "\n",
    "    for i in range(len(layers_dims) - 2):\n",
    "        layers.append(nn.Linear(layers_dims[i], layers_dims[i + 1]))\n",
    "        if norm is not None:\n",
    "            layers.append(build_norm1d(norm, layers_dims[i + 1]))\n",
    "        if activation is not None:\n",
    "            layers.append(build_activation(activation))\n",
    "\n",
    "    layers.append(nn.Linear(layers_dims[-2], layers_dims[-1]))\n",
    "\n",
    "    if post_norm:\n",
    "        layers.append(build_norm1d(norm, layers_dims[-1]))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        arch: str,\n",
    "        input_dim: int = None,\n",
    "        output_shape: int = None,\n",
    "        norm=None,\n",
    "        activation=\"relu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = build_mlp(\n",
    "            layers_dims=arch,\n",
    "            input_dim=input_dim,\n",
    "            output_shape=output_shape,\n",
    "            norm=norm,\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "PROBER_CONV_LAYERS_CONFIG = {\n",
    "    \"a\": [\n",
    "        (-1, 16, 3, 1, 1),\n",
    "        (\"max_pool\", 2, 2, 0),\n",
    "        (16, 8, 1, 1, 0),\n",
    "        (\"max_pool\", 2, 2, 0),\n",
    "        (\"fc\", -1, 2),\n",
    "    ],\n",
    "    \"b\": [\n",
    "        (-1, 32, 3, 1, 1),\n",
    "        (\"max_pool\", 2, 2, 0),\n",
    "        (32, 32, 3, 1, 1),\n",
    "        (\"max_pool\", 2, 2, 0),\n",
    "        (32, 32, 3, 1, 1),\n",
    "        (\"fc\", -1, 2),\n",
    "    ],\n",
    "    \"c\": [\n",
    "        (-1, 32, 3, 1, 1),\n",
    "        (\"max_pool\", 2, 2, 0),\n",
    "        (32, 32, 3, 1, 1),\n",
    "        (\"max_pool\", 2, 2, 0),\n",
    "        (32, 32, 3, 1, 1),\n",
    "        (32, 32, 3, 1, 1),\n",
    "        (\"fc\", -1, 2),\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "class Prober(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding: int,\n",
    "        arch: str,\n",
    "        output_shape: int,\n",
    "        input_dim=None,\n",
    "        arch_subclass: str = \"a\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.output_shape = output_shape\n",
    "        self.arch = arch\n",
    "\n",
    "        if arch == \"conv\":\n",
    "            self.prober = build_conv(\n",
    "                PROBER_CONV_LAYERS_CONFIG[arch_subclass], input_dim=input_dim\n",
    "            )\n",
    "        else:\n",
    "            arch_list = list(map(int, arch.split(\"-\"))) if arch != \"\" else []\n",
    "            f = [embedding] + arch_list + [self.output_shape]\n",
    "            layers = []\n",
    "            for i in range(len(f) - 2):\n",
    "                layers.append(torch.nn.Linear(f[i], f[i + 1]))\n",
    "                # layers.append(torch.nn.BatchNorm1d(f[i + 1]))\n",
    "                layers.append(torch.nn.ReLU(True))\n",
    "            layers.append(torch.nn.Linear(f[-2], f[-1]))\n",
    "            self.prober = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, e):\n",
    "        if self.arch == \"conv\":\n",
    "            output = self.prober(e)\n",
    "        else:\n",
    "            e = flatten_conv_output(e)\n",
    "            output = self.prober(e)\n",
    "\n",
    "        # output = output.view(*output.shape[:-1], *self.output_shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Projector(torch.nn.Module):\n",
    "    def __init__(self, arch: str, embedding: int, random: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.arch = arch\n",
    "        self.embedding = embedding\n",
    "        self.random = random\n",
    "\n",
    "        self.model, self.output_dim = build_projector(arch, embedding)\n",
    "\n",
    "        if self.random:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def maybe_reinit(self):\n",
    "        if self.random and self.arch != \"id\":\n",
    "            for param in self.parameters():\n",
    "                torch.nn.init.xavier_uniform_(param)\n",
    "                print(\"initialized\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
