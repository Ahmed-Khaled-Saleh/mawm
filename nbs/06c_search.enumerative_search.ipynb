{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb656fc",
   "metadata": {},
   "source": [
    "# Beam Search Module\n",
    "\n",
    "> This module handles all aspects of the VAE, including encoding, decoding, and latent space representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bbb56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp search.enumerative_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd36cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import *\n",
    "from fastcore.utils import *\n",
    "from torchvision.utils import save_image\n",
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03e05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CellEmpty': 0, 'CellObstacle': 1, 'CellItem': 2, 'CellGoal': 3, 'CellAgent': 4, 'GoalAt': 5, 'ItemAt': 6, 'Near': 7, 'SeeGoal': 8, 'CanMove': 9, 'OtherAgentAt': 10, 'OtherAgentNear': 11, 'OtherAgentDirection': 12}\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from mawm.core import Program, PRIMITIVE_TEMPLATES\n",
    "from mawm.models.program_embedder import ProgramEmbedder\n",
    "from mawm.models.program_encoder import ProgramEncoder\n",
    "from mawm.models.program_synthizer import Proposer\n",
    "\n",
    "device = 'cpu'\n",
    "grid_size= 7\n",
    "\n",
    "num_primitives = len(PRIMITIVE_TEMPLATES)\n",
    "p_embed = ProgramEmbedder(\n",
    "    num_primitives= num_primitives,\n",
    "    param_cardinalities= [7, 7],\n",
    "    max_params_per_primitive= 2,\n",
    "    d_name= 32,\n",
    "    d_param= 32,\n",
    ").to(device)\n",
    "\n",
    "p_encoder = ProgramEncoder(num_primitives, [grid_size, grid_size],2, seq_len=5)\n",
    "proposer = Proposer(obs_dim= 32,\n",
    "                    num_prims= num_primitives,\n",
    "                    max_params= 2,\n",
    "                    seq_len= 5,\n",
    "                    prog_emb_dim_x= 32,\n",
    "                    prog_emb_dim_y= 32,\n",
    "                    prog_emb_dim_prims= 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619adb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 5\n",
    "PAD_PRIM = len(PRIMITIVE_TEMPLATES)  # index for padding primitive\n",
    "PAD_PARAM = -1  # value for padding parameters\n",
    "MAX_PARAMS = 2  # maximum number of parameters per primitive\n",
    "\n",
    "def program_to_indices(program):\n",
    "    tokens = program.tokens[:SEQ_LEN]\n",
    "\n",
    "    prims = [t[0] for t in tokens]\n",
    "    params = [list(t[1])[:MAX_PARAMS] for t in tokens]\n",
    "\n",
    "    # pad params\n",
    "    for p in params:\n",
    "        while len(p) < MAX_PARAMS:\n",
    "            p.append(PAD_PARAM)\n",
    "\n",
    "    # pad program length\n",
    "    pad_len = SEQ_LEN - len(tokens)\n",
    "    prims += [PAD_PRIM] * pad_len\n",
    "    params += [[PAD_PARAM]*MAX_PARAMS for _ in range(pad_len)]\n",
    "\n",
    "    return (\n",
    "        torch.tensor(prims).unsqueeze(0),\n",
    "        torch.tensor(params).unsqueeze(0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a03d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_programs(programs, seq_len=SEQ_LEN, max_params=MAX_PARAMS):\n",
    "    prim_list = []\n",
    "    param_list = []\n",
    "\n",
    "    for p in programs:\n",
    "        prim_ids, param_ids = program_to_indices(p)\n",
    "        prim_list.append(prim_ids[0])       # (SEQ_LEN)\n",
    "        param_list.append(param_ids[0])     # (SEQ_LEN, max_params)\n",
    "\n",
    "    prim_batch = torch.stack(prim_list, dim=0)      # (B, SEQ_LEN)\n",
    "    param_batch = torch.stack(param_list, dim=0)    # (B, SEQ_LEN, max_params)\n",
    "\n",
    "    return prim_batch, param_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea42b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sos_idx = proposer.sos_idx\n",
    "# EOS_IDX = len(PRIMITIVE_TEMPLATES)\n",
    "# zero_params = torch.full((MAX_PARAMS,), -1, device=device)\n",
    "\n",
    "# init_prog = Program(tokens=[(sos_idx, zero_params.tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34d2394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]), torch.Size([2, 5, 2]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a, b = batchify_programs([init_prog, init_prog])\n",
    "# a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dab0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fa280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def program_to_key(prog):\n",
    "    # prog.tokens: list[(prim_idx, [params])]\n",
    "    flat = []\n",
    "    for p, ps in prog.tokens:\n",
    "        flat.append(int(p))\n",
    "        for v in ps:\n",
    "            flat.append(int(v))\n",
    "    return tuple(flat)\n",
    "\n",
    "@torch.no_grad()\n",
    "def enumerative_search(\n",
    "    z, program_embedder, proposer, program_encoder, score_fn,\n",
    "    max_prog_len=5, grid_size=7, device=\"cuda\",\n",
    "    init_top_params=3, top_prims=6, top_params=2,\n",
    "    frontier_size=5000, eval_batch_size=512,\n",
    "    lambda_prop=0.5, lambda_score=1.0, lambda_len=0.25, max_params=2,\n",
    "    seed=None\n",
    "):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    z = z.to(device)\n",
    "    if z.dim() == 1:\n",
    "        z = z.unsqueeze(0)\n",
    "\n",
    "    num_prims = len(PRIMITIVE_TEMPLATES)\n",
    "\n",
    "    # caches\n",
    "    score_cache = {}    # key -> float\n",
    "    emb_cache = {}      # key -> tensor (optional)\n",
    "\n",
    "    # priority queue: (-priority, counter, Program)\n",
    "    counter = 0\n",
    "    heap = []\n",
    "    best_prog = None\n",
    "    best_score = -1e9\n",
    "\n",
    "    # helper: evaluate a batch of programs (list of Program)\n",
    "    def eval_batch(programs):\n",
    "        # filter those not in cache\n",
    "        to_eval = []\n",
    "        keys = []\n",
    "        for p in programs:\n",
    "            k = program_to_key(p)\n",
    "            if k not in score_cache:\n",
    "                to_eval.append(p)\n",
    "                keys.append(k)\n",
    "        if len(to_eval) == 0:\n",
    "            return\n",
    "        prim_ids, params_padded = batchify_programs(to_eval, seq_len=max_prog_len, max_params=max_params)\n",
    "        prim_ids = prim_ids.to(torch.long).to(device)\n",
    "        params_padded = params_padded.to(torch.long).to(device)\n",
    "        emb = program_encoder(prim_ids, params_padded)\n",
    "        scores_t = score_fn(z, emb).detach().cpu().numpy().squeeze()\n",
    "        for k, s in zip(keys, scores_t):\n",
    "            score_cache[k] = float(s)\n",
    "\n",
    "    # 1) initialize single-token programs\n",
    "    init_programs = []\n",
    "    # for arity 0: just add; for arity >0: ask proposer for init params\n",
    "    for prim_idx in range(num_prims):\n",
    "        arity = PRIMITIVE_TEMPLATES[prim_idx][1]\n",
    "        if arity == 0:\n",
    "            init_programs.append(Program(tokens=[(prim_idx, [])]))\n",
    "        else:\n",
    "            # build a fake empty prefix program for proposer call; you can use (sos) or empty\n",
    "            prefix = Program(tokens=[(-1, [-1]*max_params)])  # or an empty representation your batchify handles\n",
    "            pids, pparams = batchify_programs([prefix], seq_len=max_prog_len, max_params=max_params)\n",
    "            pvec = program_embedder(pids.to(device), pparams.to(device))\n",
    "            zrep = z.repeat(1,1)\n",
    "            prim_logits, param_logits = proposer.forward_step(zrep, pvec)\n",
    "            # quantize param_logits to grid values, get top-k combos\n",
    "            param_preds = param_logits[0].cpu().numpy()  # shape (max_params,)\n",
    "            # simple deterministic: center = round\n",
    "            center_vals = [int(round(float(x) * (grid_size-1))) for x in param_preds[:arity]]\n",
    "            combos = [center_vals]\n",
    "            # add small variations\n",
    "            for _ in range(init_top_params-1):\n",
    "                inst = [max(0, min(grid_size-1, v + np.random.randint(-1,2))) for v in center_vals]\n",
    "                combos.append(inst)\n",
    "            for inst in combos:\n",
    "                init_programs.append(Program(tokens=[(prim_idx, inst)]))\n",
    "\n",
    "    # evaluate init set in batch\n",
    "    eval_batch(init_programs)\n",
    "\n",
    "    # push to heap with priority\n",
    "    for p in init_programs:\n",
    "        k = program_to_key(p)\n",
    "        s = score_cache[k]\n",
    "        # optionally add proposer primitive logprob (cheap for single token, can skip)\n",
    "        priority = lambda_score * s - lambda_len * len(p.tokens)\n",
    "        heapq.heappush(heap, (-priority, counter, p)); counter += 1\n",
    "\n",
    "    # main loop\n",
    "    expansions_done = 0\n",
    "    batch_buffer = []\n",
    "    while heap and expansions_done < 20000:  # or other budget\n",
    "        _, _, parent = heapq.heappop(heap)\n",
    "        # optionally stop early if priority small\n",
    "        parent_key = program_to_key(parent)\n",
    "        parent_score = score_cache.get(parent_key, -1e9)\n",
    "        if parent_score > best_score:\n",
    "            best_score = parent_score\n",
    "            best_prog = parent\n",
    "\n",
    "        if len(parent.tokens) >= max_prog_len:\n",
    "            continue\n",
    "\n",
    "        # use proposer to get top primitives & params for this parent\n",
    "        # build prefix for proposer: batchify single parent\n",
    "        prims_to_try = list(range(num_prims))\n",
    "        param_insts_per_prim = {p: [[ ]] for p in prims_to_try}\n",
    "        if proposer is not None:\n",
    "            pids, pparams = batchify_programs([parent], seq_len=max_prog_len, max_params=max_params)\n",
    "            pvec = program_embedder(pids.to(device), pparams.to(device))\n",
    "            zrep = z.repeat(1,1)\n",
    "            prim_logits, param_logits = proposer.forward_step(zrep, pvec)\n",
    "            prim_logprobs = torch.log_softmax(prim_logits[0], dim=-1).cpu().numpy()\n",
    "            top_prim_idx = np.argsort(prim_logprobs)[-top_prims:][::-1]\n",
    "            prims_to_try = [int(x) for x in top_prim_idx if int(x) < num_prims]\n",
    "            # params: quantize param_logits\n",
    "            param_preds = param_logits[0].cpu().numpy()\n",
    "            for p in prims_to_try:\n",
    "                ar = PRIMITIVE_TEMPLATES[p][1]\n",
    "                insts = []\n",
    "                if ar == 0:\n",
    "                    insts = [[]]\n",
    "                else:\n",
    "                    center = [int(round(float(x) * (grid_size-1))) for x in param_preds[:ar]]\n",
    "                    # generate small set of param instantiations around center\n",
    "                    insts = [center]\n",
    "                    # add +-1 neighbors\n",
    "                    for _ in range(top_params-1):\n",
    "                        insts.append([max(0, min(grid_size-1, c + np.random.randint(-1,2))) for c in center])\n",
    "                param_insts_per_prim[p] = insts\n",
    "\n",
    "        # produce children\n",
    "        for prim in prims_to_try:\n",
    "            for inst in param_insts_per_prim[prim]:\n",
    "                child = parent.extend(int(prim), inst)\n",
    "                # print(child.tokens)\n",
    "                k = program_to_key(child)\n",
    "                if k in score_cache:\n",
    "                    sc = score_cache[k]\n",
    "                    priority = lambda_score * sc - lambda_len * len(child.tokens)\n",
    "                    heapq.heappush(heap, (-priority, counter, child)); counter += 1\n",
    "                else:\n",
    "                    batch_buffer.append(child)\n",
    "\n",
    "        # batch eval when enough\n",
    "        if len(batch_buffer) >= eval_batch_size:\n",
    "            eval_batch(batch_buffer)\n",
    "            for cb in batch_buffer:\n",
    "                k = program_to_key(cb)\n",
    "                sc = score_cache[k]\n",
    "                priority = lambda_score * sc - lambda_len * len(cb.tokens)\n",
    "                heapq.heappush(heap, (-priority, counter, cb)); counter += 1\n",
    "            batch_buffer = []\n",
    "\n",
    "        expansions_done += 1\n",
    "        # maintain frontier size\n",
    "        if len(heap) > frontier_size:\n",
    "            # drop worst to keep size small (inefficient to remove many from heap; you can rebuild)\n",
    "            heap = heapq.nsmallest(frontier_size, heap)\n",
    "            heapq.heapify(heap)\n",
    "\n",
    "    # final flush\n",
    "    if batch_buffer:\n",
    "        eval_batch(batch_buffer)\n",
    "        for cb in batch_buffer:\n",
    "            k = program_to_key(cb)\n",
    "            sc = score_cache[k]\n",
    "            priority = lambda_score * sc - lambda_len * len(cb.tokens)\n",
    "            heapq.heappush(heap, (-priority, counter, cb)); counter += 1\n",
    "        batch_buffer = []\n",
    "    print(score_cache)\n",
    "    return best_prog, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99377140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from mawm.models.program_encoder import ProgramPredictor, ProgramEncoder\n",
    "\n",
    "import torch\n",
    "def loss_fn(z_hat, z, loss_exp= 1):\n",
    "    return torch.mean(torch.abs(z_hat - z) ** loss_exp) / loss_exp\n",
    "\n",
    "def score_fn(z, m, predictor= None):\n",
    "    predictor = ProgramPredictor()\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        z_hat = predictor(m)\n",
    "    \n",
    "    return torch.tensor([-loss_fn(z_hat[i].unsqueeze(0), z) for i in range(z_hat.size(0))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a86b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = ProgramPredictor()\n",
    "# set predictor parameters to stop gradients\n",
    "for param in predictor.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952113fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# from mawm.models.program_embedder import ProgramEmbedder, PRIMITIVE_TEMPLATES\n",
    "# from mawm.models.program_encoder import ProgramEncoder\n",
    "# from mawm.core import *\n",
    "# from mawm.models.program_synthizer import Proposer\n",
    "# import torch\n",
    "# z = torch.randn(1, 32)\n",
    "# device = 'cpu'\n",
    "# grid_size= 7\n",
    "\n",
    "# num_primitives = len(PRIMITIVE_TEMPLATES)\n",
    "# p_embed = ProgramEmbedder(\n",
    "#     num_primitives= num_primitives,\n",
    "#     param_cardinalities= [7, 7],\n",
    "#     max_params_per_primitive= 2,\n",
    "#     d_name= 32,\n",
    "#     d_param= 32,\n",
    "# )\n",
    "\n",
    "# p_encoder = ProgramEncoder(num_primitives, [grid_size, grid_size],2, seq_len=5)\n",
    "# proposer = Proposer(obs_dim= 32,\n",
    "#                     num_prims= num_primitives,\n",
    "#                     max_params= 2,\n",
    "#                     seq_len= 5,\n",
    "#                     prog_emb_dim_x= 32,\n",
    "#                     prog_emb_dim_y= 32,\n",
    "#                     prog_emb_dim_prims= 32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ff6b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 3, 3): -0.5741857290267944, (0, 2, 3): -0.5741221904754639, (1, 3, 3): -0.5742213129997253, (1, 4, 2): -0.5742852687835693, (1, 3, 2): -0.5742353200912476, (2, 3, 3): -0.5742419958114624, (2, 2, 2): -0.5742534399032593, (2, 4, 3): -0.5743171572685242, (3, 3, 3): -0.5742272734642029, (3, 4, 3): -0.5743175745010376, (3, 4, 4): -0.5742443203926086, (4, 3, 3): -0.5742278099060059, (4, 2, 3): -0.5741777420043945, (5, 3, 3): -0.57429438829422, (5, 2, 3): -0.5742396116256714, (6, 3, 3): -0.574146568775177, (6, 4, 4): -0.574253261089325, (6, 2, 4): -0.5741258859634399, (7,): -0.5740537643432617, (8,): -0.574178159236908, (9, 3): -0.5741127133369446, (9, 4): -0.5741903781890869, (9, 2): -0.574112057685852, (10, 3, 3): -0.5742272734642029, (10, 4, 3): -0.5742970108985901, (10, 3, 2): -0.5742251873016357, (11,): -0.5742096900939941, (12, 3): -0.5741569995880127, (12, 2): -0.5740978717803955, (7, 0, 3, 3): -0.5876527428627014, (7, 0, 4, 4): -0.5876098871231079, (7, 9, 3): -0.5876118540763855, (7, 9, 4): -0.5876248478889465, (7, 4, 3, 3): -0.5876571536064148, (7, 4, 4, 2): -0.587652862071991, (7, 5, 3, 3): -0.5876009464263916, (7, 6, 3, 3): -0.5876225829124451, (12, 2, 0, 3, 3): -0.5876355171203613, (12, 2, 0, 3, 2): -0.5876466631889343, (12, 2, 9, 3): -0.5876125693321228, (12, 2, 4, 3, 3): -0.587666392326355, (12, 2, 4, 3, 4): -0.5876678228378296, (12, 2, 5, 3, 3): -0.5876126885414124, (12, 2, 5, 3, 4): -0.5876109004020691, (12, 2, 6, 3, 3): -0.5876208543777466, (12, 2, 6, 4, 3): -0.5876007080078125, (9, 2, 0, 3, 3): -0.5876596570014954, (9, 2, 0, 4, 2): -0.5876260995864868, (9, 2, 9, 3): -0.5876300930976868, (9, 2, 9, 2): -0.5876084566116333, (9, 2, 4, 3, 3): -0.5877029299736023, (9, 2, 4, 3, 2): -0.587693989276886, (9, 2, 5, 3, 3): -0.5875905752182007, (9, 2, 5, 3, 2): -0.5875819325447083, (9, 2, 6, 3, 3): -0.5876631736755371, (9, 2, 6, 2, 4): -0.5876513123512268, (9, 3, 0, 3, 3): -0.5877148509025574, (9, 3, 9, 3): -0.5876772999763489, (9, 3, 9, 2): -0.5876237154006958, (9, 3, 4, 3, 3): -0.5877119302749634, (9, 3, 4, 2, 3): -0.5876443982124329, (9, 3, 5, 3, 3): -0.5876677632331848, (9, 3, 5, 3, 4): -0.5876186490058899, (9, 3, 6, 3, 3): -0.5876798033714294, (9, 3, 6, 2, 4): -0.587617039680481, (0, 2, 3, 0, 3, 3): -0.5878154635429382, (0, 2, 3, 0, 4, 2): -0.587719738483429, (0, 2, 3, 9, 3): -0.587790846824646, (0, 2, 3, 9, 2): -0.5877746343612671, (0, 2, 3, 4, 3, 3): -0.5878019332885742, (0, 2, 3, 4, 2, 3): -0.5878172516822815, (0, 2, 3, 5, 3, 3): -0.5877382755279541, (0, 2, 3, 5, 2, 3): -0.587717592716217, (0, 2, 3, 6, 3, 3): -0.5877842903137207, (0, 2, 3, 6, 4, 2): -0.5876904129981995, (6, 2, 4, 0, 3, 3): -0.587628185749054, (6, 2, 4, 0, 3, 2): -0.5876255631446838, (6, 2, 4, 9, 3): -0.587656557559967, (6, 2, 4, 9, 4): -0.5876255035400391, (6, 2, 4, 4, 3, 3): -0.5876412391662598, (6, 2, 4, 5, 3, 3): -0.5876662135124207, (6, 2, 4, 5, 4, 3): -0.5876331329345703, (6, 2, 4, 6, 3, 3): -0.5876681804656982, (6, 2, 4, 6, 4, 2): -0.587614119052887, (6, 3, 3, 0, 3, 3): -0.5876714587211609, (6, 3, 3, 0, 2, 4): -0.5877292156219482, (6, 3, 3, 9, 3): -0.5876542925834656, (6, 3, 3, 4, 3, 3): -0.5877038836479187, (6, 3, 3, 4, 4, 3): -0.5876805782318115, (6, 3, 3, 5, 3, 3): -0.5876713395118713, (6, 3, 3, 5, 4, 3): -0.5877649784088135, (6, 3, 3, 6, 3, 3): -0.5877348780632019, (6, 3, 3, 6, 3, 4): -0.5876951813697815, (12, 3, 0, 3, 3): -0.5877131223678589, (12, 3, 0, 2, 3): -0.587670087814331, (12, 3, 9, 3): -0.5876237750053406, (12, 3, 9, 4): -0.5876487493515015, (12, 3, 4, 3, 3): -0.5876718163490295, (12, 3, 5, 3, 3): -0.5876641869544983, (12, 3, 6, 3, 3): -0.5876702070236206, (12, 3, 6, 3, 4): -0.5875844955444336, (12, 3, 0, 4, 4): -0.5876052379608154, (12, 3, 4, 2, 2): -0.5876483917236328, (12, 3, 5, 3, 4): -0.5876213312149048, (12, 3, 6, 4, 3): -0.5876822471618652, (4, 2, 3, 0, 3, 3): -0.5877555012702942, (4, 2, 3, 0, 4, 3): -0.5876802206039429, (4, 2, 3, 9, 3): -0.5877254605293274, (4, 2, 3, 9, 4): -0.5876360535621643, (4, 2, 3, 4, 3, 3): -0.5877458453178406, (4, 2, 3, 4, 2, 2): -0.5877063870429993, (4, 2, 3, 5, 3, 3): -0.5877090692520142, (4, 2, 3, 5, 2, 3): -0.5876993536949158, (4, 2, 3, 6, 3, 3): -0.587756872177124, (4, 2, 3, 6, 2, 2): -0.5876546502113342, (8, 0, 3, 3): -0.587671160697937, (8, 0, 4, 4): -0.5875823497772217, (8, 9, 3): -0.5875900387763977, (8, 9, 4): -0.5875999331474304, (8, 4, 3, 3): -0.5876240730285645, (8, 4, 2, 4): -0.5876060724258423, (8, 5, 3, 3): -0.5876208543777466, (8, 6, 3, 3): -0.5876433849334717, (8, 6, 3, 2): -0.5876048803329468, (0, 3, 3, 0, 3, 3): -0.5878073573112488, (0, 3, 3, 0, 4, 4): -0.5878089666366577, (0, 3, 3, 9, 3): -0.5877915024757385, (0, 3, 3, 9, 4): -0.5878109335899353, (0, 3, 3, 4, 3, 3): -0.5877987146377563, (0, 3, 3, 4, 2, 3): -0.587813675403595, (0, 3, 3, 5, 3, 3): -0.5877519845962524, (0, 3, 3, 6, 3, 3): -0.5877882242202759, (0, 3, 3, 6, 4, 4): -0.5878099799156189, (0, 3, 3, 0, 2, 4): -0.5878227949142456, (0, 3, 3, 4, 4, 3): -0.5877613425254822, (0, 3, 3, 6, 4, 2): -0.5877753496170044, (9, 4, 0, 3, 3): -0.5876323580741882, (9, 4, 0, 4, 3): -0.58762127161026, (9, 4, 9, 3): -0.5876007080078125, (9, 4, 4, 3, 3): -0.5876485109329224, (9, 4, 4, 4, 2): -0.5875958204269409, (9, 4, 5, 3, 3): -0.5876408815383911, (9, 4, 5, 4, 2): -0.5876091122627258, (9, 4, 6, 3, 3): -0.5876243114471436, (9, 4, 6, 3, 4): -0.587570309638977, (11, 0, 3, 3): -0.5877185463905334, (11, 0, 3, 4): -0.5876854062080383, (11, 9, 3): -0.5876689553260803, (11, 9, 4): -0.587656557559967, (11, 4, 3, 3): -0.587672770023346, (11, 4, 3, 2): -0.58763587474823, (11, 5, 3, 3): -0.5876807570457458, (11, 5, 3, 4): -0.5876455903053284, (11, 6, 3, 3): -0.5877408981323242, (11, 6, 3, 2): -0.5876825451850891, (1, 3, 3, 0, 3, 3): -0.5877452492713928, (1, 3, 3, 0, 2, 4): -0.5877675414085388, (1, 3, 3, 9, 3): -0.5877091288566589, (1, 3, 3, 4, 3, 3): -0.5877166390419006, (1, 3, 3, 4, 4, 3): -0.587765097618103, (1, 3, 3, 5, 3, 3): -0.5876743197441101, (1, 3, 3, 5, 2, 4): -0.5876927375793457, (1, 3, 3, 6, 3, 3): -0.5877563953399658, (10, 3, 2, 0, 3, 3): -0.5877439379692078, (10, 3, 2, 9, 3): -0.5876808166503906, (10, 3, 2, 4, 3, 3): -0.5877048969268799, (10, 3, 2, 4, 2, 3): -0.5876975059509277, (10, 3, 2, 5, 3, 3): -0.5876496434211731, (10, 3, 2, 5, 3, 4): -0.5876455307006836, (10, 3, 2, 6, 3, 3): -0.5877386331558228, (10, 3, 2, 6, 3, 4): -0.587723970413208, (3, 3, 3, 0, 3, 3): -0.5877898335456848, (3, 3, 3, 0, 3, 2): -0.5877997875213623, (3, 3, 3, 9, 3): -0.5877019762992859, (3, 3, 3, 9, 4): -0.5877375602722168, (3, 3, 3, 4, 3, 3): -0.5877289175987244, (3, 3, 3, 4, 2, 4): -0.5877150893211365, (3, 3, 3, 5, 3, 3): -0.587695837020874, (3, 3, 3, 5, 4, 4): -0.5877053737640381, (3, 3, 3, 6, 3, 3): -0.58773273229599, (3, 3, 3, 6, 2, 2): -0.5877220630645752, (10, 3, 3, 0, 3, 3): -0.5877586603164673, (10, 3, 3, 0, 3, 4): -0.587759792804718, (10, 3, 3, 9, 3): -0.5877074599266052, (10, 3, 3, 9, 4): -0.5877454876899719, (10, 3, 3, 4, 3, 3): -0.5877305865287781, (10, 3, 3, 4, 4, 3): -0.5877586603164673, (10, 3, 3, 5, 3, 3): -0.5876753330230713, (10, 3, 3, 5, 4, 3): -0.5878352522850037, (10, 3, 3, 6, 3, 3): -0.5877299308776855, (4, 3, 3, 0, 3, 3): -0.5877777934074402, (4, 3, 3, 0, 2, 2): -0.5877112150192261, (4, 3, 3, 9, 3): -0.5877315998077393, (4, 3, 3, 4, 3, 3): -0.5877460241317749, (4, 3, 3, 5, 3, 3): -0.5877479314804077, (4, 3, 3, 5, 2, 2): -0.5877137780189514, (4, 3, 3, 6, 3, 3): -0.5877922177314758, (4, 3, 3, 6, 2, 4): -0.5877853631973267, (4, 3, 3, 0, 3, 2): -0.5877330303192139, (4, 3, 3, 9, 2): -0.5877284407615662, (4, 3, 3, 4, 3, 4): -0.5877937078475952, (4, 3, 3, 6, 3, 4): -0.5877849459648132, (1, 3, 2, 0, 3, 3): -0.5877659320831299, (1, 3, 2, 9, 3): -0.5877065062522888, (1, 3, 2, 4, 3, 3): -0.5877254605293274, (1, 3, 2, 4, 4, 2): -0.5877491235733032, (1, 3, 2, 5, 3, 3): -0.587700605392456, (1, 3, 2, 5, 2, 4): -0.5876455307006836, (1, 3, 2, 6, 3, 3): -0.5877737998962402, (1, 3, 2, 6, 3, 2): -0.5877387523651123, (5, 2, 3, 0, 3, 3): -0.5877693891525269, (5, 2, 3, 0, 2, 3): -0.58783358335495, (5, 2, 3, 9, 3): -0.5877594947814941, (5, 2, 3, 9, 2): -0.587742269039154, (5, 2, 3, 4, 3, 3): -0.5878209471702576, (5, 2, 3, 4, 3, 2): -0.5877690315246582, (5, 2, 3, 5, 3, 3): -0.5876985788345337, (5, 2, 3, 5, 2, 4): -0.5877346396446228, (5, 2, 3, 6, 3, 3): -0.5877490043640137, (5, 2, 3, 6, 2, 4): -0.5877947211265564, (5, 2, 3, 4, 2, 3): -0.5878223776817322, (5, 2, 3, 6, 2, 2): -0.5876808762550354, (2, 3, 3, 0, 3, 3): -0.5877734422683716, (2, 3, 3, 0, 3, 2): -0.5877566337585449, (2, 3, 3, 9, 3): -0.5877183079719543, (2, 3, 3, 4, 3, 3): -0.5877352356910706, (2, 3, 3, 4, 2, 2): -0.5877319574356079, (2, 3, 3, 5, 3, 3): -0.5877025127410889, (2, 3, 3, 5, 3, 2): -0.5876744985580444, (2, 3, 3, 6, 3, 3): -0.5877381563186646, (2, 3, 3, 6, 3, 2): -0.5877192616462708, (3, 4, 4, 0, 3, 3): -0.5876390933990479, (3, 4, 4, 0, 4, 3): -0.5876001119613647, (3, 4, 4, 9, 3): -0.5875924825668335, (3, 4, 4, 4, 3, 3): -0.5876178741455078, (3, 4, 4, 4, 4, 2): -0.5876204371452332, (3, 4, 4, 5, 3, 3): -0.5876125693321228, (3, 4, 4, 5, 2, 3): -0.5876250863075256, (3, 4, 4, 6, 3, 3): -0.5876149535179138, (3, 4, 4, 6, 2, 2): -0.5876305103302002, (6, 4, 4, 0, 3, 3): -0.5876162052154541, (6, 4, 4, 0, 4, 4): -0.5875300765037537, (6, 4, 4, 9, 3): -0.5875920653343201, (6, 4, 4, 4, 3, 3): -0.5875829458236694, (6, 4, 4, 4, 4, 3): -0.5875533223152161, (6, 4, 4, 5, 3, 3): -0.5876250863075256, (6, 4, 4, 5, 2, 4): -0.587517499923706, (6, 4, 4, 6, 3, 3): -0.5876324772834778, (2, 2, 2, 0, 3, 3): -0.5877345204353333, (2, 2, 2, 9, 3): -0.5876678228378296, (2, 2, 2, 9, 4): -0.5876158475875854, (2, 2, 2, 4, 3, 3): -0.5877010226249695, (2, 2, 2, 4, 2, 3): -0.5877180695533752, (2, 2, 2, 5, 3, 3): -0.5876626968383789, (2, 2, 2, 5, 4, 4): -0.587640106678009, (2, 2, 2, 6, 3, 3): -0.5877096652984619, (2, 2, 2, 6, 2, 3): -0.5876944661140442, (1, 4, 2, 0, 3, 3): -0.587681770324707, (1, 4, 2, 0, 4, 3): -0.5876898765563965, (1, 4, 2, 9, 3): -0.5876659750938416, (1, 4, 2, 9, 4): -0.5876724720001221, (1, 4, 2, 4, 3, 3): -0.5876502990722656, (1, 4, 2, 4, 2, 4): -0.5876007080078125, (1, 4, 2, 5, 3, 3): -0.5876250863075256, (1, 4, 2, 5, 4, 3): -0.5877293348312378, (1, 4, 2, 6, 3, 3): -0.5876815319061279, (5, 3, 3, 0, 3, 3): -0.5877915024757385, (5, 3, 3, 0, 4, 3): -0.5878068208694458, (5, 3, 3, 9, 3): -0.5877610445022583, (5, 3, 3, 9, 2): -0.5877863764762878, (5, 3, 3, 4, 3, 3): -0.5878169536590576, (5, 3, 3, 4, 2, 3): -0.5878297090530396, (5, 3, 3, 5, 3, 3): -0.58775395154953, (5, 3, 3, 5, 4, 4): -0.5877869129180908, (5, 3, 3, 6, 3, 3): -0.5877905488014221, (5, 3, 3, 6, 3, 4): -0.5877962112426758, (10, 4, 3, 0, 3, 3): -0.5876756310462952, (10, 4, 3, 0, 4, 4): -0.5875948071479797, (10, 4, 3, 9, 3): -0.5876220464706421, (10, 4, 3, 4, 3, 3): -0.5876478552818298, (10, 4, 3, 4, 3, 4): -0.5876168608665466, (10, 4, 3, 5, 3, 3): -0.587604820728302, (10, 4, 3, 5, 4, 2): -0.5876442193984985, (10, 4, 3, 6, 3, 3): -0.5876405239105225, (10, 4, 3, 6, 3, 2): -0.5876293182373047, (2, 4, 3, 0, 3, 3): -0.5877189636230469, (2, 4, 3, 0, 4, 3): -0.5876149535179138, (2, 4, 3, 4, 3, 3): -0.5876701474189758, (2, 4, 3, 4, 2, 3): -0.5876736640930176, (2, 4, 3, 9, 3): -0.5875967741012573, (2, 4, 3, 9, 2): -0.5875902771949768, (2, 4, 3, 5, 3, 3): -0.5876606702804565, (2, 4, 3, 5, 3, 4): -0.5876191854476929, (2, 4, 3, 6, 3, 3): -0.5876357555389404, (2, 4, 3, 6, 4, 4): -0.5876496434211731, (3, 4, 3, 0, 3, 3): -0.5877182483673096, (3, 4, 3, 0, 3, 2): -0.5877116918563843, (3, 4, 3, 4, 3, 3): -0.5876690745353699, (3, 4, 3, 4, 3, 2): -0.5876944661140442, (3, 4, 3, 9, 3): -0.5876153111457825, (3, 4, 3, 9, 4): -0.5876095294952393, (3, 4, 3, 5, 3, 3): -0.5876095294952393, (3, 4, 3, 5, 3, 2): -0.58763188123703, (3, 4, 3, 6, 3, 3): -0.5876372456550598, (3, 4, 3, 6, 4, 4): -0.5876092314720154}\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# z = torch.randn(32, 32).to(device)\n",
    "# res = enumerative_search(\n",
    "#     z[0], p_embed, proposer, p_encoder, score_fn,\n",
    "#     max_prog_len=5, grid_size=7, device=\"cpu\",\n",
    "#     init_top_params=3, top_prims=6, top_params=2,\n",
    "#     frontier_size=5000, eval_batch_size=512,\n",
    "#     lambda_prop=0.7, lambda_score=0.2, lambda_len=0.01, max_params=2,\n",
    "#     seed=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50738b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Near(), -0.5740537643432617)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69feff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
