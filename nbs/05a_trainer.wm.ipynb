{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb656fc",
   "metadata": {},
   "source": [
    "# World Model trainer\n",
    "\n",
    "> This module implements LeJepa training procedure with three predictors and two input modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bbb56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp trainers.wm_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd36cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import *\n",
    "from fastcore.utils import *\n",
    "from torchvision.utils import save_image\n",
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb85e38",
   "metadata": {},
   "source": [
    "## WorldModel Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from mawm.trainers.trainer import Trainer\n",
    "from mawm.models.utils import save_checkpoint\n",
    "from mawm.logger.base import AverageMeter\n",
    "from mawm.losses.sigreg import SIGReg, SIGRegDistributed\n",
    "from mawm.losses.vicreg import VICReg\n",
    "from mawm.models.utils import flatten_conv_output\n",
    "\n",
    "class WMTrainer(Trainer):\n",
    "    def __init__(self, cfg, model, train_loader, sampler,\n",
    "                 optimizer=None, device=None,earlystopping=None, \n",
    "                 scheduler=None, writer= None, verbose= None, logger= None):\n",
    "        \n",
    "        self.cfg = cfg\n",
    "        self.device = device\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.sampler = sampler\n",
    "\n",
    "        self.model = model['jepa']\n",
    "        self.msg_encoder = model['msg_encoder']\n",
    "        self.msg_predictor = model['msg_predictor']\n",
    "        self.obs_predictor = model['obs_predictor']\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.earlystopping = earlystopping\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "        self.writer = writer\n",
    "        # self.loss_meter = AverageMeter()\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "        self.sigreg = SIGReg().to(self.device)\n",
    "        self.disSigReg = SIGRegDistributed().to(self.device)\n",
    "\n",
    "        # self.vicreg = VICReg(self.cfg).to(self.device)\n",
    "        self.lambda_ = self.cfg.loss.lambda_\n",
    "        self.W_H_PRED = self.cfg.loss.W_H_PRED\n",
    "        self.W_SIM_T = self.cfg.loss.W_SIM_T\n",
    "\n",
    "        self.agents = [f\"agent_{i}\" for i in range(len(self.cfg.env.agents))]\n",
    "\n",
    "        self.dmpc_dir = os.path.join(self.cfg.log_dir, 'dmpc_marlrid', self.cfg.now)\n",
    "        if not os.path.exists(self.dmpc_dir):\n",
    "            os.makedirs(self.dmpc_dir , exist_ok=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c89df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def criterion(self: WMTrainer, global_step, Z0, Z, h, h_hat, mask_t, mask):\n",
    "\n",
    "    flat_encodings = flatten_conv_output(Z0) # [T, B, c`, h`, w`] => [T, B, d]\n",
    "    sigreg_img = self.disSigReg(flat_encodings[:1], global_step= global_step)\n",
    "    sigreg_msg = self.disSigReg(h, global_step= global_step)\n",
    "    transition_mask = mask_t[1:] * mask_t[:-1]\n",
    "\n",
    "    diff = (Z0[1:] - Z[1:]).pow(2).mean(dim=(2, 3, 4)) # (T-1, B)\n",
    "    sim_loss = (diff * transition_mask).sum() / transition_mask.sum().clamp_min(1)\n",
    "\n",
    "    if self.cfg.loss.vicreg.sim_coeff_t:\n",
    "        diff_t = ( Z0[1:] -  Z0[:-1]).pow(2).mean(dim=(2, 3, 4))# (T-1, B)\n",
    "        sim_loss_t = (diff_t * transition_mask).sum() / transition_mask.sum().clamp_min(1)\n",
    "    else:\n",
    "        sim_loss_t = torch.zeros([1], device=self.device)\n",
    "\n",
    "    # z_sender_flat = flatten_conv_output(z_sender)  # [B, T, c`, h`, w`] => [B, T,d]\n",
    "    # z_sender_hat = flatten_conv_output(z_sender_hat)  # [B, T, d]\n",
    "\n",
    "    # z_pred_loss = (z_sender_flat - z_sender_hat).square().mean(dim= -1)  # [B, T, d] => [B, T]\n",
    "    # z_pred_loss = (z_pred_loss * mask).sum() / mask.sum().clamp_min(1) \n",
    "\n",
    "    h_pred_loss = (h - h_hat).square().mean(dim= -1)  # [B, T, dim=32] => [B, T]\n",
    "    h_pred_loss = (h_pred_loss * mask).sum() / mask.sum().clamp_min(1) \n",
    "\n",
    "    return {\n",
    "        'sigreg_img': sigreg_img,\n",
    "        'sigreg_msg': sigreg_msg,\n",
    "        'sim_loss': sim_loss,\n",
    "        'sim_loss_t': sim_loss_t,\n",
    "        # 'z_pred_loss': z_pred_loss,\n",
    "        'h_pred_loss': h_pred_loss\n",
    "    }\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed8ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from mawm.models.utils import flatten_conv_output\n",
    "from einops import rearrange\n",
    "@patch\n",
    "def train_epoch(self: WMTrainer, epoch):\n",
    "    self.model.train()\n",
    "    \n",
    "    total_running_loss = 0.0\n",
    "    total_valid_steps = 0\n",
    "\n",
    "    self.logger.info(f\"Device used: {self.device}\")\n",
    "    self.sampler.set_epoch(epoch)\n",
    "    for batch_idx, data in enumerate(self.train_loader):\n",
    "        global_step = epoch * len(self.train_loader) + batch_idx\n",
    "        self.optimizer.zero_grad()\n",
    "        batch_loss = 0\n",
    "\n",
    "        for agent_id in self.agents:\n",
    "            obs, _, _, act, _, dones = data[agent_id].values()\n",
    "            mask = (~dones.bool()).float().to(self.device) # [B, T, d=1]\n",
    "            mask = rearrange(mask, 'b t d-> b (t d)')  # [T, B]\n",
    "            mask_t = rearrange(mask, 'b t -> t b')  # [T, B]\n",
    "            \n",
    "            agent_loss = 0\n",
    "\n",
    "            if mask.sum() == 0:\n",
    "                continue \n",
    "\n",
    "            obs = obs.to(self.device)\n",
    "            # pos = pos.to(self.device)\n",
    "            act = act.to(self.device)\n",
    "\n",
    "            self.logger.info(f\"device used for main agent data: {mask_t.device} {obs.device}, {act.device}\")\n",
    "\n",
    "            for other_agent in self.agents:\n",
    "                if other_agent != agent_id:\n",
    "                    obs_sender, _, msg, _, _,_ = data[other_agent].values()\n",
    "\n",
    "                    obs_sender = obs_sender.to(self.device)\n",
    "                    # pos_sender = pos_sender.to(self.device)\n",
    "                    msg = msg.to(self.device)\n",
    "                    self.logger.info(f\"device used for other agent data: {obs_sender.device}, {msg.device}\")\n",
    "            \n",
    "            h = self.msg_encoder(msg) # [B, T, C, H, W] => [B, T, dim=32]\n",
    "\n",
    "            Z0, Z = self.model(x= obs, pos= None, actions= act, msgs= h, T= act.size(1)-1)#[B, T, c, h, w] =>  [T, B, c, h, w]\n",
    "            # vicreg_loss = self.vicreg(Z0, Z, mask= mask_t)\n",
    "            # self.logger.info(\"Vicreg losses: %s\" % str({k: v.item() for k, v in vicreg_loss.items()}))\n",
    "            \n",
    "            if hasattr(self.model, 'module'):\n",
    "                z_sender = self.model.module.backbone(obs_sender, position = None)  #[B, T, c, h, w] => [B, T, c`, h`, w`]\n",
    "            else:\n",
    "                z_sender = self.model.backbone(obs_sender, position = None)  #[B, T, c, h, w] => [B, T, c`, h`, w`]\n",
    "                \n",
    "            # z_sender_hat = self.obs_predictor(h) # [B, T, d=32] => [B, T, C, H, W]\n",
    "            h_hat = self.msg_predictor(z_sender) # [B, T, d] => [B, T, dim=32]\n",
    "            \n",
    "            # losses = self.criterion(global_step, Z0, Z, h, h_hat, mask_t, mask, z_sender, z_sender_hat)\n",
    "            losses = self.criterion(global_step, Z0, Z, h, h_hat, mask_t, mask)\n",
    "\n",
    "\n",
    "            self.writer.write({\n",
    "                f'{agent_id}/train/sigreg_img': losses['sigreg_img'].item(),\n",
    "                f'{agent_id}/train/sigreg_msg': losses['sigreg_msg'].item(),\n",
    "                f'{agent_id}/train/sim_loss': losses['sim_loss'].item(),\n",
    "                f'{agent_id}/train/sim_loss_t': losses['sim_loss_t'].item(),\n",
    "                # f'{agent_id}/train/z_pred_loss': losses['z_pred_loss'].item(),\n",
    "                f'{agent_id}/train/h_pred_loss': losses['h_pred_loss'].item(),\n",
    "            })\n",
    "            \n",
    "            self.logger.info(\"Losses: %s\" % str({k: v.item() for k, v in losses.items()}))\n",
    "            \n",
    "            # jepa_1_loss = (1 - self.lambda_) * losses['sim_loss'] + self.lambda_ * losses['sigreg_img']\n",
    "            # jepa_2_loss = (1 - self.lambda_) * losses['z_pred_loss'] + self.lambda_ * losses['sigreg_msg']\n",
    "            # jepa_3_loss = self.W_H_PRED * losses['h_pred_loss']\n",
    "\n",
    "            jepa_1_loss = (1 - self.lambda_) * losses['sim_loss'] + self.lambda_ * losses['sigreg_img']\n",
    "            jepa_2_loss = (1 - self.lambda_) * losses['h_pred_loss'] + self.lambda_ * losses['sigreg_msg']\n",
    "            # jepa_3_loss = self.W_H_PRED * losses['h_pred_loss']\n",
    "\n",
    "            self.logger.info(f\"JEPA Losses: jepa_1_loss: {jepa_1_loss.item():.4f}, jepa_2_loss: {jepa_2_loss.item():.4f}, sim_loss_t: {losses['sim_loss_t'].item():.4f}\")\n",
    "\n",
    "            agent_loss = jepa_1_loss + jepa_2_loss + self.W_SIM_T * losses['sim_loss_t']\n",
    "            self.logger.info(f\"Agent: {agent_id}, agent_loss: {agent_loss.item():.4f}\")\n",
    "            \n",
    "            # agent_loss = self.lambda_ * z_pred_loss + (1 - self.lambda_) * h_pred_loss + vicreg_loss['total_loss']\n",
    "            # self.logger.info(f\"Agent: {agent_id}, z_pred_loss: {z_alignment.item():.4f}, h_pred_loss: {h_alignment.item():.4f}, vicreg_loss: {vicreg_loss['total_loss'].item():.4f}, agent_loss: {agent_loss.item():.4f}\")\n",
    "            batch_loss += agent_loss\n",
    "\n",
    "            num_valid = mask.sum().item()\n",
    "            total_running_loss += agent_loss.item() * num_valid\n",
    "            total_valid_steps += num_valid\n",
    "            \n",
    "        loss = batch_loss / len(self.agents)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "       \n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            self.logger.info(f'Train Epoch: {epoch} [{batch_idx * len(obs)}/{len(self.train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(self.train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "    # divide by len(self.agents) to match the 'loss' scale used in training\n",
    "    final_epoch_loss = (total_running_loss / total_valid_steps) / len(self.agents)\n",
    "    self.logger.info(f'====> Epoch: {epoch} Average loss: {final_epoch_loss:.4f}')\n",
    "\n",
    "    return final_epoch_loss\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72ef3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# from einops import rearrange\n",
    "# @patch\n",
    "# def eval_epoch(self: WMTrainer):\n",
    "#     self.model.eval()\n",
    "\n",
    "#     total_running_loss = 0.0\n",
    "#     total_valid_steps = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch_idx, data in enumerate(self.train_loader):\n",
    "#             batch_loss = 0\n",
    "            \n",
    "#             for agent_id in self.agents:\n",
    "#                 obs, pos, _, act, _, dones = data[agent_id].values()\n",
    "#                 mask = (~dones.bool()).float().to(self.device) # [B, T, d=1]\n",
    "#                 mask = rearrange(mask, 'b t d-> b (t d)')  # [T, B]\n",
    "#                 mask_t = rearrange(mask, 'b t -> t b')  # [T, B]\n",
    "                    \n",
    "#                 agent_loss = 0\n",
    "                \n",
    "#                 if mask.sum() == 0: # CHECK: mask is determined per the reciever agent\n",
    "#                     continue  # entire batch is terminals\n",
    "\n",
    "#                 obs = obs.to(self.device)\n",
    "#                 pos = pos.to(self.device)\n",
    "#                 act = act.to(self.device)\n",
    "\n",
    "#                 for other_agent in self.agents:\n",
    "#                     if other_agent != agent_id:\n",
    "#                         obs_sender, pos_sender, msg, _, _,_ = data[other_agent].values()\n",
    "\n",
    "#                         obs_sender = obs_sender.to(self.device)\n",
    "#                         pos_sender = pos_sender.to(self.device)\n",
    "#                         msg = msg.to(self.device)\n",
    "                \n",
    "#                 h = self.msg_encoder(msg) # [B, T, C, H, W] => [B, T, dim=32]\n",
    "\n",
    "#                 Z0, Z = self.model(x= obs, pos= pos, actions= act, msgs= h, T= act.size(1)-1)#[B, T, c, h, w] =>  [T, B, c, h, w]\n",
    "#                 vicreg_loss = self.vicreg(Z0, Z, mask= mask_t)\n",
    "                \n",
    "#                 z_sender = self.model.backbone(obs_sender, position = pos_sender)  #[B, T, c, h, w] => [B, T, c`, h`, w`]\n",
    "#                 z_sender_hat = self.obs_predictor(h)\n",
    "                \n",
    "#                 z_sender = self.model.backbone(obs_sender, position = pos_sender)  #[B, T, c, h, w] => [B, T, c`, h`, w`]\n",
    "#                 z_sender_hat = self.obs_predictor(h)\n",
    "\n",
    "#                 z_sender_flat = flatten_conv_output(z_sender)  # [B, T, c`, h`, w`] => [B, T,d]\n",
    "#                 z_sender_hat = flatten_conv_output(z_sender_hat)  # [B, T, d]\n",
    "\n",
    "#                 z_pred_loss = (z_sender_flat - z_sender_hat).square().mean(dim= -1)  # [B, T, d] => [B, T]\n",
    "#                 z_pred_loss = (z_pred_loss * mask).sum() / mask.sum().clamp_min(1) \n",
    "\n",
    "#                 h_hat = self.msg_predictor(z_sender[:, :, :-2]) # [B, T, d] => [B, T, dim=32]\n",
    "#                 h_pred_loss = (h - h_hat).square().mean(dim= -1)  # [B, T, dim=32] => [B, T]\n",
    "#                 h_pred_loss = (h_pred_loss * mask).sum() / mask.sum().clamp_min(1) \n",
    "                \n",
    "#                 agent_loss = self.lambda_ * z_pred_loss + (1 - self.lambda_) * h_pred_loss + vicreg_loss['total_loss']\n",
    "#                 batch_loss += agent_loss\n",
    "\n",
    "#                 num_valid = mask.sum().item()\n",
    "#                 total_running_loss += agent_loss.item() * num_valid\n",
    "#                 total_valid_steps += num_valid\n",
    "\n",
    "#     final_epoch_loss = (total_running_loss / total_valid_steps) / len(self.agents)\n",
    "#     print(f'====>  Test set loss: {final_epoch_loss:.4f}')\n",
    "#     return final_epoch_loss\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0041e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import wandb\n",
    "CHECKPOINT_FREQ = 1\n",
    "@patch\n",
    "def fit(self: WMTrainer):\n",
    "\n",
    "    \n",
    "    latest_file = \"latest.pt\"\n",
    "    folder = self.dmpc_dir\n",
    "    latest_path = os.path.join(folder, latest_file)\n",
    "    \n",
    "    loss_meter = AverageMeter()\n",
    "    lst_dfs = []\n",
    "\n",
    "    for epoch in range(1, self.cfg.epochs + 1):\n",
    "        self.logger.info(\"Epoch %d\" % (epoch))\n",
    "        lr = self.scheduler.adjust_learning_rate(epoch)\n",
    "        train_loss = self.train_epoch(epoch)\n",
    "        loss_meter.update(train_loss)\n",
    "        \n",
    "        def save_checkpoint(epoch, path):\n",
    "            if not self.verbose:\n",
    "                return\n",
    "            \n",
    "            def get_state(m):\n",
    "                return m.module.state_dict() if hasattr(m, 'module') else m.state_dict()\n",
    "            \n",
    "            save_dict = {\n",
    "                'epoch': epoch,\n",
    "                'jepa': get_state(self.model),\n",
    "                'msg_encoder': get_state(self.msg_encoder),\n",
    "                \"msg_predictor\": get_state(self.msg_predictor),\n",
    "                \"obs_predictor\": get_state(self.obs_predictor),\n",
    "                'train_loss': train_loss,\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "                \"lr\": lr,\n",
    "            }\n",
    "            try:\n",
    "                torch.save(save_dict, path)\n",
    "                self.logger.info(f\"Successfully saved checkpoint to {path}\")\n",
    "            except Exception as e:\n",
    "                self.logger.info(f\"Encountered exception when saving checkpoint: {e}\")\n",
    "        \n",
    "        self.logger.info(\"avg. loss %.3f\" % loss_meter.avg)\n",
    "\n",
    "        df = pd.DataFrame.from_records([{\"epoch\": epoch ,\"train_loss\": train_loss}], index= \"epoch\")\n",
    "        lst_dfs.append(df)\n",
    "\n",
    "        if epoch % CHECKPOINT_FREQ == 0 or epoch == (self.cfg.epochs - 1):\n",
    "            save_checkpoint(epoch + 1, latest_path)\n",
    "            if self.cfg.save_every_freq > 0 and epoch % self.cfg.save_every_freq == 0:\n",
    "                save_every_file = f\"e{epoch}.pt\"\n",
    "                save_every_path = os.path.join(folder, save_every_file)\n",
    "                save_checkpoint(epoch + 1, save_every_path)\n",
    "\n",
    "        to_log = {\n",
    "            \"train_loss\": train_loss, \n",
    "        }\n",
    "\n",
    "        if self.verbose:\n",
    "            self.writer.write(to_log)\n",
    "\n",
    "    df_res = pd.concat(lst_dfs)\n",
    "    df_reset = df_res.reset_index()\n",
    "    if self.verbose:\n",
    "        self.writer.write({'Train Loss Table': wandb.Table(dataframe= df_reset)})\n",
    "\n",
    "    self.writer.finish()\n",
    "    return df_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69feff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
