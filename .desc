
In a multi-agent RL scenario with the CTDE paradigm, two agents collaborate to solve a task. At each time step $t$:

- Agent i (Sender): Observes $o_t^{i}$, encoded as latent $z_{t}^{i} \in \mathbb{R}^{d_z}$ 
 - Agent j (Receiver): Observes $o_t^j$, takes action $a_t^{i}$, gets reward $r_t^{i}$.\\

Note: Agents $i$ and $j$ are sending and receiving simultaneously, and we use parameter sharing, so they both have the same models and are both sending and receiving.\\


The idea is that, this $z$ is a continuous thought, that can be internalized by the agent itself, but in order for other agent to understand it, we need a discrete symbolic program that describes this in a form that is compatible with the discrete nature of communication.

Models:

------------------------------------------------------------------------

1- VAE Encoder (pre-trained, frozen): $\operatorname{Enc}: o \rightarrow z$ 

- Maps the raw observation to a vector representation.\\

2- Program Encoder: $\phi: P \rightarrow m \in \mathbb{R}^{d_m}$:

- Maps the symbolic program P to a continuous message $m$ to be used in other agent's models for conditioning.\\

3-World Model: $$f_\theta: (z_t^{j}, a_t^{j}, m_t^i) \rightarrow \hat{z}_{t+1}^{j}$$ 

-Predicts the next state given the current state, action, and message.

- Trained using the same method of V-JEPA 2 action conditioned, but on images instead of videos. \\

4- Reward Model: $$g_\psi: (z_t^{j}, a_t^{i}) \rightarrow \text{Bernoulli}(p)$$
- Predicts reward distribution. (Note: Reward is Sparse, we only get 1 if both agents reached the goal and 0 otherwise).\\

5- Proposer: $$Q_\phi: z_t^{i} \rightarrow P_t^{i}$$

- Generates a program from the sender's observation, which provides a symbolic, discrete program message that we can communicate with the other agent to serve as a symbolic description of the latent observation of the sender agent. 

-----------------------------------------------------------------


Programs $P = [(p_1, v_1), (p_2, v_2), ..., (p_L, v_L)] $

Each token: primitive $p_i \in \{1, ..., N_{\text{prim}}\}$ with parameters $v_i \in \mathbb{R}^{d_v} $

The goal is to learn models that allow communication to improve state prediction:$$\min_{\theta, \psi, \phi} \mathcal{L}_{\text{total}} =  \lambda_{\text{world}} \mathcal{L}_{\text{world}} + \lambda_{\text{reward}} \mathcal{L}_{\text{reward}} + \lambda_{\text{Info}} \mathcal{L}_{\text{Info}}$$

where $\mathcal{L}$ for agent $j$, for example, is:

$$\mathcal{L}_{\text{world}}= \lambda_z ||f_\theta(z_t^{j},a_t^{j},\phi(P_t^{*j}))-z_{t+1}^{j}||$$

And $\mathcal{L}_{\text{reward}}$ is:
$$ \mathcal{L}_{\text {reward }}=-\mathbb{E}\left[\log g_\psi\left(r_t^{j} \mid z_t^j, a_t^{j}, \phi\left(P^{*j}\right)\right)\right]$$


We also have an auxiliary loss to ensure that the program is as informative as possible about the latent observation:\\
$$\mathcal{L}_{\text{Info}} = I(P ; z) \geq \mathbb{E}_{(z, P) \sim p(z, P)}\left[\log \frac{g(z^{j}, P^{j})}{\frac{1}{K} \sum_{k=1}^K g\left(z^{j}, P_k^{j}\right)}\right]$$

where $g(z^{j}, P^{j})$ is a learned compatibility function. For example:
$$ \exp(MLP([z; \phi(P)]))$$

-----------------------------------------------------------------

The training procedure goes like this\\

Step 1: Wake Phase (E-step): Program Inference:\\

- Run Beam search for each tuple $(z_t^{j}, a_t^{j}, z_{t+1}^{j}, z_t^{i})$ with the following parameters:

- Start: Empty program with no primitives.

- Transition function: Proposer model $Q_\phi: z_t^{i} \rightarrow P_t^{i}$.

 Score function: World model loss + reward loss: $\text{score} = \lambda_{\text{world}} \mathcal{L}_{\text{world}} + \lambda_{\text{reward}} \mathcal{L}_{\text{reward}}$.\\

$\text{Beam} = 5$, $\text{max\_len} = 15$\\

- Obtain program $P$ such that: $P^{*}=\arg \min_P - \text{Score}$\\
- Run this and append obtained programs to get a program for each latent observation in the dataset.

Step 2: World model/reward/InfoCons model.


Sleep Phase:

Given inferred programs $\{P_i^*\}$ from the wake phase, train the proposer to predict inferred programs via teacher forcing:



$\phi_{\text {proposer }} \leftarrow \arg \min _\phi \frac{1}{B} \sum_{i=1}^B \sum_{l=1}^{L_i}\left[\mathcal{L}_{\mathrm{CE}}\left(p_l \mid z_{1, i}^t, P_{i,<l}^*\right)\right.$

------------------\\


Pseudocode:

\begin{verbatim}
    def train_one_iteration(batch_transitions, 
                       world_model, reward_model, program_encoder, proposer,
                       optimizer_models, optimizer_proposer):
    """
    One complete wake-sleep iteration
    """
    
    # ========== WAKE PHASE ==========
    print("Wake phase: synthesizing programs...")
    
    z_senders = []
    z_receivers = []
    actions = []
    z_next_receivers = []
    rewards = []
    
    for (z_s, z_r, a, z_next_r, r) in batch_transitions:
        z_senders.append(z_s)
        z_receivers.append(z_r)
        actions.append(a)
        z_next_receivers.append(z_next_r)
        rewards.append(r)
    
    # Synthesize programs via beam search
    programs = []
    for i in range(len(batch_transitions)):
        P_star = beam_search(
            z_sender=z_senders[i],
            z_receiver=z_receivers[i],
            action=actions[i],
            z_next_receiver=z_next_receivers[i],
            reward=rewards[i],
            world_model=world_model,
            reward_model=reward_model,
            program_encoder=program_encoder,
            proposer=proposer,
            beam_width=5
        )
        programs.append(P_star)
    
    # ========== UPDATE MODELS (θ, ψ, φ_enc) ==========
    print("Updating world model, reward model, program encoder...")
    
    # Encode programs
    messages = [program_encoder(P) for P in programs]
    
    # World model loss
    L_world = 0
    for i in range(len(batch_transitions)):
        z_pred = world_model(z_receivers[i], actions[i], messages[i])
        L_world += torch.norm(z_pred - z_next_receivers[i]) ** 2
    L_world /= len(batch_transitions)
    
    # Reward model loss
    L_reward = 0
    for i in range(len(batch_transitions)):
        r_pred = reward_model(z_receivers[i], actions[i], messages[i])
        if rewards[i] == 1:
            L_reward += -torch.log(r_pred + 1e-8)
        else:
            L_reward += -torch.log(1 - r_pred + 1e-8)
    L_reward /= len(batch_transitions)
    
    # Mutual information loss (optional)
    L_info = compute_infonce_loss(z_senders, messages)
    
    # Combined loss
    L_models = λ_z * L_world + λ_r * L_reward + λ_i * L_info
    
    optimizer_models.zero_grad()
    L_models.backward()
    optimizer_models.step()
    
    # ========== SLEEP PHASE ==========
    print("Sleep phase: training proposer...")
    
    # Convert to tensors
    z_senders_tensor = torch.stack(z_senders)
    
    # Train proposer via teacher forcing
    L_sleep = sleep_phase_batched(
        z_senders_tensor, 
        programs, 
        proposer, 
        optimizer_proposer
    )
    
    print(f"Losses - World: {L_world:.4f}, Reward: {L_reward:.4f}, "
          f"Info: {L_info:.4f}, Sleep: {L_sleep:.4f}")
    
    return L_models.item(), L_sleep
\end{verbatim}



