"""This module implements LeJepa training procedure."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/05d_trainer_lejepa.ipynb.

# %% auto 0
__all__ = ['LejepaTrainer']

# %% ../../nbs/05d_trainer_lejepa.ipynb 3
from fastcore import *
from fastcore.utils import *
from torchvision.utils import save_image
import torch
import os
from torch import nn
import pandas as pd

# %% ../../nbs/05d_trainer_lejepa.ipynb 19
from .trainer import Trainer
from ..models.utils import save_checkpoint
from ..optimizer.losses import SIGReg
class LejepaTrainer(Trainer):
    def __init__(self, cfg, model, train_loader, val_loader=None, 
                 criterion=None, optimizer=None, device=None,
                 earlystopping=None, scheduler=None, writer= None):
        
        super().__init__(cfg, model, train_loader, val_loader, criterion, optimizer, device)
        self.earlystopping = earlystopping
        self.scheduler = scheduler
        self.writer = writer
        self.sigreg = SIGReg().to(self.device)
        self.lambda_ = self.cfg.lambda_

        self.lejepa_dir = os.path.join(self.cfg.log_dir, 'lejepa_marlrid')
        if not os.path.exists(self.lejepa_dir):
            os.mkdir(self.lejepa_dir)
            os.mkdir(os.path.join(self.lejepa_dir, 'samples'))

    

# %% ../../nbs/05d_trainer_lejepa.ipynb 20
@patch
def train_epoch(self: LejepaTrainer, epoch):
    self.model.train()
    self.train_loader.dataset.load_next_buffer()
    
    train_loss = 0
    actual_len = 0
    for batch_idx, data in enumerate(self.train_loader):
        # import pdb; pdb.set_trace()
        obs, dones, agent_id = data
        mask = ~dones.bool()     # keep only where done is False

        if mask.sum() == 0:
            continue  # entire batch is terminals

        obs = obs[mask]          # filter observations
        obs = obs.to(self.device)

        self.optimizer.zero_grad()
        proj = self.model(obs)
        inv_loss = (proj.mean(0) - proj).square().mean()
        sigreg_loss = self.sigreg(proj)
        loss = (1- self.lambda_) * inv_loss + self.lambda_ * sigreg_loss
        
        loss.backward()
        train_loss += loss.item()
        self.optimizer.step()
        if batch_idx % 20 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(obs), len(self.train_loader.dataset),
                100. * batch_idx / len(self.train_loader),
                loss.item() / len(obs)))
        actual_len += len(obs)

    print('====> Epoch: {} Average loss: {:.4f}'.format(
        epoch, train_loss / actual_len))

    return train_loss / actual_len
       

# %% ../../nbs/05d_trainer_lejepa.ipynb 21
@patch
def eval_epoch(self: LejepaTrainer):
    self.model.eval()
    self.val_loader.dataset.load_next_buffer()
    test_loss = 0
    actual_len = 0
    with torch.no_grad():
        for data in self.val_loader:
            obs, dones, agent_id = data
            mask = ~dones.bool()     # keep only where done is False

            if mask.sum() == 0:
                continue  # entire batch is terminals

            obs = obs[mask]          # filter observations
            obs = obs.to(self.device)
            proj = self.model(obs)
            inv_loss = (proj.mean(0) - proj).square().mean()
            sigreg_loss = self.sigreg(proj)
            loss = (1- self .lambda_) * inv_loss + self.lambda_ * sigreg_loss
            test_loss += loss.item()
            actual_len += len(obs)

    test_loss /= actual_len
    print('====> Test set loss: {:.4f}'.format(test_loss))
    return test_loss


# %% ../../nbs/05d_trainer_lejepa.ipynb 22
import wandb

@patch
def fit(self: LejepaTrainer):
    cur_best = None
    lst_dfs = []

    for epoch in range(1, self.cfg.epochs + 1):
        train_loss = self.train_epoch(epoch)
        test_loss = self.eval_epoch()
        # self.scheduler.step(test_loss)

        # checkpointing
        best_filename = os.path.join(self.lejepa_dir, 'best.pth')
        filename = os.path.join(self.lejepa_dir, 'checkpoint.pth')

        is_best = not cur_best or test_loss < cur_best
        if is_best:
            cur_best = test_loss

        save_checkpoint({
            'epoch': epoch,
            'state_dict': self.model.state_dict(),
            'precision': test_loss,
            'optimizer': self.optimizer.state_dict(),
            # 'scheduler': self.scheduler.state_dict(),
            # 'earlystopping': self.earlystopping.state_dict()
        }, is_best, filename, best_filename)

        # if self.earlystopping.early_stop(test_loss):             
        #     break
       

        to_log = {
            "train_loss": train_loss, 
            "test_loss": test_loss,
        }

        self.writer.write(to_log)
        df = pd.DataFrame.from_records([{"epoch": epoch ,"train_loss": train_loss, "test_loss":test_loss}], index= "epoch")
        lst_dfs.append(df)

    df_res = pd.concat(lst_dfs)
    df_reset = df_res.reset_index()
    self.writer.write({'Train-Val Loss Table': wandb.Table(dataframe= df_reset)})

    self.writer.finish()
    return df_reset
