"""a ConvNet module for percpetion."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/02aa_models.encoder.ipynb.

# %% auto 0
__all__ = ['VisionEncoder']

# %% ../../nbs/02aa_models.encoder.ipynb 3
from fastcore import *
from fastcore.utils import *

# %% ../../nbs/02aa_models.encoder.ipynb 4
import torch
import torch.nn as nn
import torch.nn.functional as F

class VisionEncoder(nn.Module):
    def __init__(self, in_channels=3, latent_dim=128):
        super().__init__()

        self.latent_dim = latent_dim
        hidden_dims = [32, 64, 128, 256] 

        # -----------------------
        #        Encoder
        # -----------------------
        # Path: 42x42 -> 21x21 -> 11x11 -> 6x6 -> 3x3
        modules = []
        for h_dim in hidden_dims:
            modules.append(
                nn.Sequential(
                    nn.Conv2d(in_channels, h_dim, kernel_size=3,
                              stride=2, padding=1),
                    nn.BatchNorm2d(h_dim),
                    nn.LeakyReLU()
                )
            )
            in_channels = h_dim

        self.encoder = nn.Sequential(*modules)

        # Final shape is 256 * 3 * 3 = 2304
        self.flattened_dim = hidden_dims[-1] * 3 * 3 

        self.proj  = nn.Linear(self.flattened_dim, latent_dim)
        # self.fc_var = nn.Linear(self.flattened_dim, latent_dim)

    def forward(self, x):
        N, V = x.shape[:2]
        x = self.encoder(x.flatten(0, 1))
        x = torch.flatten(x, start_dim=1)
        proj = self.proj(x)
        return proj.reshape(N, V, -1).transpose(0, 1)

