"""World model (Predictor)."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/02c_models_world_model.ipynb.

# %% auto 0
__all__ = ['MLPPredictor', 'WorldModelAttention']

# %% ../../nbs/02c_models_world_model.ipynb 3
from fastcore import *
from fastcore.utils import *

# %% ../../nbs/02c_models_world_model.ipynb 4
import torch
import torch.nn as nn

# %% ../../nbs/02c_models_world_model.ipynb 5
class MLPPredictor(nn.Module):
    def __init__(self, latent_dim=32, action_dim=1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(latent_dim + action_dim, 128),
            nn.ReLU(),
            nn.Linear(128, latent_dim)
        )

    def forward(self, z, a):
        return self.net(torch.cat([z, a], dim=-1))


# %% ../../nbs/02c_models_world_model.ipynb 7
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# Reusing the Multi-Head Attention logic from our previous conversation, 
# ensuring all components for Agent I are in this single module.
class WorldModelAttention(nn.Module):
    """
    World Model with Multi-Head Cross-Attention.
    
    Handles message reception:
    - Training: Accepts continuous message vector (h_t^j).
    - Execution: Accepts discrete symbol index (m_hard) and performs embedding lookup.
    """

    def __init__(self, 
                 state_dim: int, 
                 action_dim: int, 
                 vocab_size: int, 
                 embed_dim: int, # Dimension of the actual message vector
                 predictor_embed_dim: int = 64,
                 total_attention_dim: int = 128,
                 num_heads: int = 3):
        """
        Initializes the World Model and the Symbol Embedding Table.

        Args:
            state_dim (int): Dimension of z_t^i and z_{t+1}^i.
            action_dim (int): Dimension of raw action a_t^i.
            vocab_size (int): Size of the shared symbol vocabulary |V|.
            embed_dim (int): Dimension of the message vector (h_t^j or h_received).
            ... MHA and MLP parameters ...
        """
        super().__init__()
        
        # Critical shared component: Agent I's version of the Symbol Embedding Matrix
        self.symbol_embeddings = nn.Embedding(vocab_size, embed_dim)
        
        # --- Multi-Head Attention Setup ---
        self.num_heads = num_heads
        self.total_attention_dim = total_attention_dim
        self.head_dim = total_attention_dim // num_heads
        self.predictor_embed_dim = predictor_embed_dim

        # MHA Projections: Key/Value input size is the message embedding dimension (embed_dim)
        self.query_proj = nn.Linear(state_dim, total_attention_dim, bias=False)
        self.key_proj = nn.Linear(embed_dim, total_attention_dim, bias=False)
        self.value_proj = nn.Linear(embed_dim, total_attention_dim, bias=False)
        self.output_proj = nn.Linear(total_attention_dim, total_attention_dim)

        # --- Action Encoding Layer ---
        self.action_encoder = nn.Linear(action_dim, predictor_embed_dim, bias=True)

        # --- Final Prediction Network (MLP) ---
        mlp_input_dim = state_dim + total_attention_dim + predictor_embed_dim
        
        self.prediction_mlp = nn.Sequential(
            nn.Linear(mlp_input_dim, state_dim * 2),
            nn.ReLU(),
            nn.Linear(state_dim * 2, state_dim) 
        )

    


# %% ../../nbs/02c_models_world_model.ipynb 8
# --- Utility methods for MHA (as before) ---
@patch
def _split_heads(self: WorldModelAttention, x: torch.Tensor):
    new_shape = x.size()[:-1] + (self.num_heads, self.head_dim)
    x = x.view(*new_shape)
    return x.unsqueeze(2) # Add sequence length L=1: (B, num_heads, 1, head_dim)

@patch
def _combine_heads(self: WorldModelAttention, x: torch.Tensor):
    x = x.squeeze(2) 
    new_shape = x.size()[:-2] + (self.total_attention_dim,)
    return x.view(*new_shape)

# --- Core Attention Logic ---

@patch
def _multi_head_attention(self: WorldModelAttention, Q_input: torch.Tensor, KV_input: torch.Tensor):
    """ Computes Multi-Head Attention given the Q and KV sources. """
    
    # Compute Q, K, V Projections
    Q_proj = self.query_proj(Q_input)   # Q_input is z_t^i
    K_proj = self.key_proj(KV_input)    # KV_input is the continuous message (h_t^j or h_received)
    V_proj = self.value_proj(KV_input)  

    # Split into Multiple Heads
    Q = self._split_heads(Q_proj) # (B, num_heads, 1, head_dim)
    K = self._split_heads(K_proj) 
    V = self._split_heads(V_proj) 
    
    # Scaled Dot-Product
    K_T = K.transpose(-1, -2)
    scores = torch.matmul(Q, K_T) / math.sqrt(self.head_dim)
    
    weights = F.softmax(scores, dim=-1) 
    context_head_output = torch.matmul(weights, V)

    # Combine Heads and Final Projection
    context_vector_combined = self._combine_heads(context_head_output)
    context_vector = self.output_proj(context_vector_combined) # (B, total_attention_dim)
    
    return context_vector


# %% ../../nbs/02c_models_world_model.ipynb 9
@patch
def forward(self: WorldModelAttention, 
            z_t_i, 
            message_input, 
            action_t,
            is_training):
    """
    Predicts the next state z_{t+1}^i.

    Args:
        z_t_i (torch.Tensor): Agent i's current state. Shape: (Batch, state_dim)
        message_input (torch.Tensor): 
            - TRAINING: Continuous vector h_t^j. Shape: (Batch, embed_dim)
            - EXECUTION: Discrete symbol index m_hard. Shape: (Batch)
        action_t (torch.Tensor): Agent i's action. Shape: (Batch, action_dim)
        is_training (bool): Flag to switch message processing mode.
    """
    
    # --- 1. Message Processing (The Key Difference) ---
    if is_training:
        # Training: Use the continuous message directly
        h_message = message_input # (B, embed_dim)
    else:
        # Execution: Perform non-differentiable lookup on the received index
        # message_input is the discrete index m_hard (LongTensor)
        h_message = self.symbol_embeddings(message_input.long()) # (B, embed_dim)
    
    
    # --- 2. Multi-Head Attention ---
    # Q comes from z_t_i, K/V comes from the processed message h_message
    context_vector = self._multi_head_attention(
        Q_input=z_t_i,
        KV_input=h_message
    )

    # --- 3. Encode Action ---
    a = self.action_encoder(action_t) # (B, predictor_embed_dim)

    # --- 4. Final Prediction ---
    combined_features = torch.cat([z_t_i, context_vector, a], dim=1) 
    z_t_plus_1_i = self.prediction_mlp(combined_features) 

    return z_t_plus_1_i

