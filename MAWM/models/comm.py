"""Model architecture that handles communicaion."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/02b_models_comm.ipynb.

# %% auto 0
__all__ = []

# %% ../../nbs/02b_models_comm.ipynb 3
from fastcore import *
from fastcore.utils import *

# %% ../../nbs/02b_models_comm.ipynb 5
@patch
def forward(self: CommunicationModuleGRU, z_t_j: torch.Tensor, h_prev_j: torch.Tensor, is_training: bool):
    """
    Calculates the new message context and determines the output based on mode.

    Args:
        z_t_j (torch.Tensor): The encoded observation at time t. Shape: (Batch, observation_dim)
        h_prev_j (torch.Tensor): The previous embedded message context (h_{t-1}^j). 
                                    Shape: (Batch, message_context_dim)
        is_training (bool): If True, returns h_t_j (continuous). 
                            If False, returns the discrete index m_hard.

    Returns:
        - If is_training=True: torch.Tensor (h_t^j), Shape: (Batch, message_context_dim)
        - If is_training=False: torch.Tensor (m_hard), Shape: (Batch)
    """
    B = z_t_j.size(0)

    # 1. Update Recurrent Message Context (h_t^j)
    
    # Input reshape: (B, observation_dim) -> (B, 1, observation_dim)
    z_t_j_seq = z_t_j.unsqueeze(1) 

    # Hidden state reshape: (B, message_context_dim) -> (1, B, message_context_dim)
    h_prev_j_gru = h_prev_j.unsqueeze(0).contiguous()

    # Output h_t_j_gru: (1, B, message_context_dim)
    _, h_t_j_gru = self.gru(z_t_j_seq, h_prev_j_gru)
    
    # Final continuous message context: (B, message_context_dim)
    h_t_j = h_t_j_gru.squeeze(0)

    # 2. Determine Output Based on Mode
    if is_training:
        # --- TRAINING PATH: Pass Continuous Context ---
        # Gradients flow through h_t_j to the GRU, optimizing for usefulness.
        return h_t_j
    else:
        # --- EXECUTION PATH: Hard Discretization ---
        # 2a. Compute Logits
        logits = self.logit_proj(h_t_j) # Shape: (Batch, vocab_size)

        # 2b. Hard Sampling (Argmax): Select the best symbol index
        discrete_token_index = torch.argmax(logits, dim=-1) # Shape: (Batch)
        
        # This is the discrete message sent over the channel.
        return discrete_token_index.long() 

