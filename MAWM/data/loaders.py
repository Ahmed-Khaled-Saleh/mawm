"""This module handles all communication-related functionalities, including message passing, event handling, and notifications."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/01c_data_loaders.ipynb.

# %% auto 0
__all__ = ['RolloutObservationDataset', 'RolloutSequenceDataset']

# %% ../../nbs/01c_data_loaders.ipynb 3
from fastcore import *
from fastcore.utils import *

# %% ../../nbs/01c_data_loaders.ipynb 4
from bisect import bisect
from os import listdir
from os.path import join, isdir
from tqdm import tqdm
import torch
import torch.utils.data
import numpy as np


# %% ../../nbs/01c_data_loaders.ipynb 13
class _RolloutDataset(torch.utils.data.Dataset): # pylint: disable=too-few-public-methods
    def __init__(self, agent, root, transform, buffer_size=200, train=True, obs_key = 'pov'): # pylint: disable=too-many-arguments
        
        self.agent = agent
        self._transform = transform
        self.obs_key = obs_key
        self._files = [join(root, sd) for sd in listdir(root)]

        def train_test_split(files, train):
            if train:
                return files[:-600]
            else:
                return files[-600:]

        self._files = train_test_split(self._files, train)
        self._cum_size = None
        self._buffer = None
        self._buffer_fnames = None
        self._buffer_index = 0
        self._buffer_size = buffer_size

    # def sample_sequence(self, rollout, agent, seq_len):

    #     episode_len = rollout["episode_len"].item()

    #     # sample start index uniformly in [0, episode_len)
    #     start = np.random.randint(0, episode_len)

    #     # clamp so the sequence fits
    #     start = min(start, episode_len - seq_len)
    #     print(f"Sampling sequence for {agent} from index {start} to {start + seq_len}")

    #     end = start + seq_len
    #     return start, end

    def load_next_buffer(self):
        """ Loads next buffer """
        self._buffer_fnames = self._files[self._buffer_index:self._buffer_index + self._buffer_size]
        self._buffer_index += self._buffer_size
        self._buffer_index = self._buffer_index % len(self._files)
        self._buffer = []
        self._cum_size = [0]

        # progress bar
        pbar = tqdm(total=len(self._buffer_fnames),
                    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} {postfix}')
        pbar.set_description("Loading file buffer ...")

        for f in self._buffer_fnames:
            with np.load(f, allow_pickle= True) as data:
                self._buffer += [{k: np.copy(v) for k, v in data.items()}] # list of dicts,each dict is an episode data
                self._cum_size += [self._cum_size[-1] + data['episode_len'].item()]
            pbar.update(1)
        pbar.close()

    def __len__(self):
        # to have a full sequence, you need self.seq_len + 1 elements, as
        # you must produce both an seq_len obs and seq_len next_obs sequences
        if not self._cum_size:
            self.load_next_buffer()
        return self._cum_size[-1]

    def __getitem__(self, i):
        # binary search through cum_size
        file_index = bisect(self._cum_size, i) - 1
        seq_index = i - self._cum_size[file_index]
        data = self._buffer[file_index] # list of a dict
        return self._get_data(data, seq_index)

    def _get_data(self, data, seq_index):
        raise NotImplementedError

    def _data_per_sequence(self, data_length):
        raise NotImplementedError

# %% ../../nbs/01c_data_loaders.ipynb 14
class RolloutObservationDataset(_RolloutDataset): # pylint: disable=too-few-public-methods
    """ Encapsulates rollouts.

    Rollouts should be stored in subdirs of the root directory, in the form of npz files,
    each containing a dictionary with the keys:
        - observations: (rollout_len, *obs_shape)
        - actions: (rollout_len, action_size)
        - rewards: (rollout_len,)
        - terminals: (rollout_len,), boolean

     As the dataset is too big to be entirely stored in rams, only chunks of it
     are stored, consisting of a constant number of files (determined by the
     buffer_size parameter).  Once built, buffers must be loaded with the
     load_next_buffer method.

    Data are then provided in the form of images

    :args root: root directory of data sequences
    :args seq_len: number of timesteps extracted from each rollout
    :args transform: transformation of the observations
    :args train: if True, train data, else test
    """
    def _data_per_sequence(self, data_length):
        return data_length

    def _get_data(self, data, seq_index):
        print(f"Getting data at index {seq_index}")
        done = data[f'{self.agent}_info'][seq_index]['done']
        obs = data[f'{self.agent}_obs'][seq_index][self.obs_key].astype(np.uint8)
        return self._transform(obs), done


# %% ../../nbs/01c_data_loaders.ipynb 22
class RolloutSequenceDataset(_RolloutDataset): # pylint: disable=too-few-public-methods
    """ Encapsulates rollouts.

    Rollouts should be stored in subdirs of the root directory, in the form of npz files,
    each containing a dictionary with the keys:
        - observations: (rollout_len, *obs_shape)
        - actions: (rovllout_len, action_size)
        - rewards: (rollout_len,)
        - terminals: (rollout_len,), boolean

     As the dataset is too big to be entirely stored in rams, only chunks of it
     are stored, consisting of a constant number of files (determined by the
     buffer_size parameter).  Once built, buffers must be loaded with the
     load_next_buffer method.

    Data are then provided in the form of tuples (obs, action, reward, terminal, next_obs):
    - obs: (seq_len, *obs_shape)
    - actions: (seq_len, action_size)
    - reward: (seq_len,)
    - terminal: (seq_len,) boolean
    - next_obs: (seq_len, *obs_shape)

    NOTE: seq_len < rollout_len in moste use cases

    :args root: root directory of data sequences
    :args seq_len: number of timesteps extracted from each rollout
    :args transform: transformation of the observations
    :args train: if True, train data, else test
    """
    def __init__(self, agent, root, seq_len, transform, buffer_size=200, train=True, obs_key='pov'): # pylint: disable=too-many-arguments
        super().__init__(agent, root, transform, buffer_size, train, obs_key)
        self._seq_len = seq_len
        self.agent = agent

    def _get_agent_data(self, data, seq_index):
        data_dict = {}

        obs_data  = data[f'{self.agent}_obs'][seq_index:seq_index + self._seq_len + 1]
        obs_data  = [self._transform(obs_data[i][self.obs_key].astype(np.uint8)) for i in range(len(obs_data))]
        
        obs, next_obs = obs_data[:-1], obs_data[1:]
        data_dict["obs"] = obs
        data_dict["next_obs"] = next_obs

        action = data[f'{self.agent}_act'][seq_index+1:seq_index + self._seq_len + 1]
        action = action.astype(np.float32)
        data_dict["act"] = action

        data_dict["rew"] = data[f'{self.agent}_rew'][seq_index+1:seq_index + self._seq_len + 1].astype(np.float32)
        
        data_dict["info"] = data[f'{self.agent}_info'][seq_index+1:seq_index + self._seq_len + 1]
        return data_dict

    def _get_data(self, data, seq_index):
        return self._get_agent_data(data, seq_index)

    def _data_per_sequence(self, data_length):
        return data_length - self._seq_len

