"""Collect random rollouts from the MeltingPot environments and save them to disk for later use in training and evaluation."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/01a_data_generation.ipynb.

# %% auto 0
__all__ = ['env', 'seq_len', 'rollouts', 'data_dir', 'noise_type', 'sample_continuous_policy', 'generate_data']

# %% ../../nbs/01a_data_generation.ipynb 3
from fastcore import *
from fastcore.utils import *

# %% ../../nbs/01a_data_generation.ipynb 4
import numpy as np
import math

def sample_continuous_policy(action_space, seq_len, dt):
    """ Sample a continuous policy.

    Atm, action_space is supposed to be a box environment. The policy is
    sampled as a brownian motion a_{t+1} = a_t + sqrt(dt) N(0, 1).

    :args action_space: gym action space
    :args seq_len: number of actions returned
    :args dt: temporal discretization

    :returns: sequence of seq_len actions
    """
    actions = [action_space.sample()]
    for _ in range(seq_len):
        daction_dt = np.random.randn(*actions[-1].shape)
        actions.append(
            np.clip(actions[-1] + math.sqrt(dt) * daction_dt,
                    action_space.low, action_space.high))
    return actions

# %% ../../nbs/01a_data_generation.ipynb 5
"""
Generating data from the CarRacing gym environment.
!!! DOES NOT WORK ON TITANIC, DO IT AT HOME, THEN SCP !!!
"""
import argparse
from os.path import join, exists
import gymnasium as gym
import numpy as np

def generate_data(rollouts= 50, data_dir= '.', noise_type='brown'): # pylint: disable=R0914
    """ Generates data """
    assert exists(data_dir), "The data directory does not exist..."

    env = gym.make("CarRacing-v3")
    seq_len = 1000

    for i in range(rollouts):
        env.reset()
        # env.env.viewer.window.dispatch_events()
        if noise_type == 'white':
            a_rollout = [env.action_space.sample() for _ in range(seq_len)]
        elif noise_type == 'brown':
            a_rollout = sample_continuous_policy(env.action_space, seq_len, 1. / 50)

        s_rollout = []
        r_rollout = []
        d_rollout = []

        t = 0
        while True:
            action = a_rollout[t]
            t += 1

            s, r, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            # env.env.viewer.window.dispatch_events()
            s_rollout += [s]
            r_rollout += [r]
            d_rollout += [done]
            if done:
                print("> End of rollout {}, {} frames...".format(i, len(s_rollout)))
                np.savez(join(data_dir, 'rollout_{}'.format(i)),
                         observations=np.array(s_rollout),
                         rewards=np.array(r_rollout),
                         actions=np.array(a_rollout),
                         terminals=np.array(d_rollout))
                break




# %% ../../nbs/01a_data_generation.ipynb 13
from shimmy import MeltingPotCompatibilityV0
from shimmy.utils.meltingpot import load_meltingpot
import numpy as np

env = load_meltingpot("collaborative_cooking__circuit")
env = MeltingPotCompatibilityV0(env, render_mode="rgb_array")

seq_len = 1000
rollouts = 10
data_dir = './data'
noise_type = 'brown'



for i in range(rollouts):
    # reset environment
    reset_out = env.reset()
    if isinstance(reset_out, tuple):  # unpack tuple (obs, info)
        observations, infos = reset_out
    else:
        observations = reset_out
        infos = None

    if noise_type == 'white':
        for agent in env.agents:
            a_rollout[agent] = [env.action_space(agent).sample() for _ in range(seq_len)]
    
    elif noise_type == 'brown':
        for agent in env.agents:
            a_rollout[agent] = sample_continuous_policy(env.action_space(agent), seq_len, 1. / 50)

    s_rollout = []
    r_rollout = []
    d_rollout = []

    t = 0
    while True:

        actions = {}

        action = a_rollout[t]
        t += 1

        for agent in env.agents:
            action = env.action_space(agent).sample()
            actions[agent] = action

            # access observation dict correctly
            obs = observations[agent]
            data[agent]["obs"].append(obs)
            data[agent]["actions"].append(action)

        s, r, terminated, truncated, info = env.step(action)
        done = terminated or truncated

        s_rollout += [s]
        r_rollout += [r]
        d_rollout += [done]

        if done:
            print("> End of rollout {}, {} frames...".format(i, len(s_rollout)))
            np.savez(join(data_dir, 'rollout_{}'.format(i)),
                        observations=np.array(s_rollout),
                        rewards=np.array(r_rollout),
                        actions=np.array(a_rollout),
                        terminals=np.array(d_rollout))
            break




        

        # perform step
        step_out = env.step(actions)
        if len(step_out) == 5:
            observations, rewards, terminations, truncations, infos = step_out
        else:
            observations, rewards, terminations, truncations = step_out
            infos = None

        # store rewards
        for agent, r in rewards.items():
            data[agent]["rewards"].append(r)

env.close()

# 5. Convert to numpy arrays
for agent, d in data.items():
    d["obs"] = np.array(d["obs"])
    d["actions"] = np.array(d["actions"])
    d["rewards"] = np.array(d["rewards"])

# 6. Print summary
for agent, d in data.items():
    print(f"{agent}: {len(d['obs'])} steps, avg reward = {np.mean(d['rewards']):.3f}")


# %% ../../nbs/01a_data_generation.ipynb 17
"""
Generating data from the CarRacing gym environment.
!!! DOES NOT WORK ON TITANIC, DO IT AT HOME, THEN SCP !!!
"""
import argparse
from os.path import join, exists
import gymnasium as gym
import numpy as np

def generate_data(rollouts= 50, data_dir= '.', noise_type='brown'): # pylint: disable=R0914
    """ Generates data """
    assert exists(data_dir), "The data directory does not exist..."

    env = gym.make("CarRacing-v3")
    seq_len = 1000

    for i in range(rollouts):
        env.reset()
        # env.env.viewer.window.dispatch_events()
        if noise_type == 'white':
            a_rollout = [env.action_space.sample() for _ in range(seq_len)]
        elif noise_type == 'brown':
            a_rollout = sample_continuous_policy(env.action_space, seq_len, 1. / 50)

        s_rollout = []
        r_rollout = []
        d_rollout = []

        t = 0
        while True:
            action = a_rollout[t]
            t += 1

            s, r, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            # env.env.viewer.window.dispatch_events()
            s_rollout += [s]
            r_rollout += [r]
            d_rollout += [done]
            if done:
                print("> End of rollout {}, {} frames...".format(i, len(s_rollout)))
                np.savez(join(data_dir, 'rollout_{}'.format(i)),
                         observations=np.array(s_rollout),
                         rewards=np.array(r_rollout),
                         actions=np.array(a_rollout),
                         terminals=np.array(d_rollout))
                break

# if __name__ == "__main__":
#     parser = argparse.ArgumentParser()
#     parser.add_argument('--rollouts', type=int, help="Number of rollouts")
#     parser.add_argument('--dir', type=str, help="Where to place rollouts")
#     parser.add_argument('--policy', type=str, choices=['white', 'brown'],
#                         help='Noise type used for action sampling.',
#                         default='brown')
#     args = parser.parse_args()
generate_data()

