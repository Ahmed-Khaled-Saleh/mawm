"""Misc utils for the FindGoal environment in MARLGrid."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../../../nbs/07m_envs.marlgrid_env_findgoal_cfg.ipynb.

# %% ../../../../../nbs/07m_envs.marlgrid_env_findgoal_cfg.ipynb 3
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

import os, sys
import datetime
import argparse

import json
from easydict import EasyDict as edict

from .grid_world_environment import get_env_name
from .utils import cprint




# %% auto 0
__all__ = ['env_cfg', 'config']

# %% ../../../../../nbs/07m_envs.marlgrid_env_findgoal_cfg.ipynb 4
from omegaconf import OmegaConf
env_cfg = OmegaConf.create()
env_cfg.seed = 1

env_cfg.env_type = 'c'

env_cfg.num_agents = 2
env_cfg.num_adversaries = 0

env_cfg.max_steps = 512
env_cfg.grid_size = 15
env_cfg.observation_style = 'dict'
env_cfg.observe_position = False
env_cfg.observe_self_position = True
env_cfg.observe_self_env_act = False
env_cfg.observe_t = False
env_cfg.observe_done = False
env_cfg.neutral_shape = True
env_cfg.can_overlap = False
env_cfg.active_after_done = False

env_cfg.discrete_position = True

env_cfg.view_size = 7
env_cfg.view_tile_size = 6
env_cfg.clutter_density = 0.15

# if `num_blind_agents` == b, the FIRST b agents do not get image obs
env_cfg.num_blind_agents = 0

# agent comm length
env_cfg.comm_len = 0

# if False, use continuous communication
env_cfg.discrete_comm = True

# team reward settings

env_cfg.team_reward_type = 'share'
env_cfg.team_reward_freq = 'none'
env_cfg.team_reward_multiplier = 1

env_cfg.info_gain_rew = False

# update env policy / comm policy using only env reward / team reward
env_cfg.separate_rew_more = False


# %% ../../../../../nbs/07m_envs.marlgrid_env_findgoal_cfg.ipynb 5
config = OmegaConf.create()

config.env_cfg = env_cfg    

config.run_dir = 'runs'
config.num_workers = 16
config.gpu = [int(g) for g in '0'.split(',')]

# the prefix to the log
config.id = ''

# async update steps
config.tmax = 20

# max total training iterations
config.train_iter = 300000

config.lr = 0.0001

# experiment id
config.resume_path = ''

# the policy head
config.policy = 'lstm'  # ['fc', 'lstm']
config.model = 'shared'  # ['shared']
config.share_critic = False
config.layer_norm = True
config.comm_rnn = True

# mask logits of unavailable actions
config.mask = True

# # update env policy using only env reward
# config.separate_rew = True

# training
config.anneal_comm_rew = False
config.ae_loss_k = 1.0
config.ae_std = 1.0
config.ae_pg = 0
config.ae_type = ''  # ['', 'fc', 'mlp', 'rfc', 'rmlp']
config.ae_agent_done = False
config.img_feat_dim = 64
config.comm_vf = False

# eval configs
config.video_save_freq = 20
config.ckpt_save_freq = 100
config.num_eval_episodes = 10
config.num_eval_videos = 10
config.eval_ae = False
config.env_cfg.env_name = get_env_name(config.env_cfg)

