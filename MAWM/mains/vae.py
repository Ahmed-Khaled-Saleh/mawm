"""Main file to train VAE model"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/010_main_vae.ipynb.

# %% auto 0
__all__ = ['parser', 'args', 'cuda', 'device', 'dataset_train', 'dataset_test', 'train_loader', 'val_loader', 'model_cls',
           'model', 'writer', 'trainer', 'df_res', 'criterion']

# %% ../../nbs/010_main_vae.ipynb 3
from fastcore import *
from fastcore.utils import *
import torch

# %% ../../nbs/010_main_vae.ipynb 4
import argparse
from os.path import join, exists
from os import mkdir

import torch
import torch.utils.data
from torch import optim
from torch.nn import functional as F
from torchvision import transforms
from omegaconf import OmegaConf
from dotenv import load_dotenv


# %% ../../nbs/010_main_vae.ipynb 5
from ..core import get_cls

from ..data.utils import transform_train, transform_test
from ..data.loaders import RolloutObservationDataset

from ..models.vae import VAE

from ..optimizer.utils import ReduceLROnPlateau, EarlyStopping
from ..trainers.trainer import VAETrainer
from ..writers.wandb_writer import WandbWriter

# %% ../../nbs/010_main_vae.ipynb 6
parser = argparse.ArgumentParser(description='VAE Training')
parser.add_argument('--config', type=str, help='Path to the YAML config file', required=True)
parser.add_argument('--timestamp', type=str, help='Time stamp', required=True)
parser.add_argument('--env_file', type=str, help='Path to the .env file', required=False)


parser.add_argument('--batch-size', type=int, default=32, metavar='N',
                    help='input batch size for training (default: 32)')
parser.add_argument('--epochs', type=int, default=1000, metavar='N',
                    help='number of epochs to train (default: 1000)')
parser.add_argument('--log_dir', type=str, help='Directory where results are logged')
parser.add_argument('--noreload', action='store_true',
                    help='Best model is not reloaded if specified')
parser.add_argument('--nosamples', action='store_true',
                    help='Does not save samples during training if specified')


args = parser.parse_args()



# %% ../../nbs/010_main_vae.ipynb 7
if args.env_file:
    load_dotenv(args.env_file)
    key = os.getenv("WANDB_API_KEY", None)
    hf_secret = os.getenv("HF_SECRET_CODE", None)

    if key:
        os.environ["WANDB_API_KEY"] = key
    if hf_secret:
        os.environ["HF_SECRET_CODE"] = hf_secret     

try:
    with open(args.config, 'r') as file:
        cfg = OmegaConf.load(file)
except:
    print("Invalid config file path")

# %% ../../nbs/010_main_vae.ipynb 8
cfg.now = args.timestamp 

cfg.optimizer.lr = float(args.lr) if args.lr else cfg.optimizer.lr
cfg.data.batch_size = int(args.batch_size) if args.batch_size else cfg.data.batch_size
cfg.optimizer.name = args.optimizer if args.optimizer else cfg.optimizer.name


# %% ../../nbs/010_main_vae.ipynb 9
cuda = torch.cuda.is_available()
torch.manual_seed(123)
# Fix numeric divergence due to bug in Cudnn
torch.backends.cudnn.benchmark = True
device = torch.device("cuda" if cuda else "cpu")


# %% ../../nbs/010_main_vae.ipynb 10
dataset_train = RolloutObservationDataset(cfg.data.data_root,
                                          transform_train, 
                                          train=True)

dataset_test = RolloutObservationDataset(cfg.data.data_root,
                                         transform_test,
                                         train=False)

train_loader = torch.utils.data.DataLoader(
    dataset_train, batch_size=args.batch_size, shuffle=True, num_workers=2)

val_loader = torch.utils.data.DataLoader(
    dataset_test, batch_size=args.batch_size, shuffle=True, num_workers=2)

model_cls = get_cls(f"MAWM.models.{cfg.model.name}", cfg.model.name.upper())
model = model_cls(cfg.data.channels, cfg.model.latent_size).to(device)


# %% ../../nbs/010_main_vae.ipynb 13
def criterion(recon_x, x, mu, logsigma):
    """ VAE loss function """
    BCE = F.mse_loss(recon_x, x, size_average=False)

    # see Appendix B from VAE paper:
    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014
    # https://arxiv.org/abs/1312.6114
    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
    KLD = -0.5 * torch.sum(1 + 2 * logsigma - mu.pow(2) - (2 * logsigma).exp())
    return BCE + KLD

# %% ../../nbs/010_main_vae.ipynb 14
writer = WandbWriter(cfg)
trainer = VAETrainer(cfg, model, train_loader, val_loader, criterion, 
                     optimizer, device, dataset_train, dataset_test,
                     earlystopping, scheduler, writer)

df_res = trainer.fit()

