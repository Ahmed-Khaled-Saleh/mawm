"""This module handles all aspects of the VAE, including encoding, decoding, and latent space representation."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/06a_beam_search.ipynb.

# %% auto 0
__all__ = ['SIGReg', 'HFDataset', 'VAETrainer']

# %% ../../nbs/06a_beam_search.ipynb 3
from fastcore import *
from fastcore.utils import *
from torchvision.utils import save_image
import torch
import os
from torch import nn
import pandas as pd

# %% ../../nbs/06a_beam_search.ipynb 4
import torch, torch.nn as nn, torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision.transforms import v2
import timm, wandb, hydra, tqdm
from omegaconf import DictConfig
from datasets import load_dataset
from torch.amp import GradScaler, autocast
from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR


class SIGReg(torch.nn.Module):
    def __init__(self, knots=17):
        super().__init__()
        t = torch.linspace(0, 3, knots, dtype=torch.float32)
        dt = 3 / (knots - 1)
        weights = torch.full((knots,), 2 * dt, dtype=torch.float32)
        weights[[0, -1]] = dt
        window = torch.exp(-t.square() / 2.0)
        self.register_buffer("t", t)
        self.register_buffer("phi", window)
        self.register_buffer("weights", weights * window)

    def forward(self, proj):
        A = torch.randn(proj.size(-1), 256, device="cuda")
        A = A.div_(A.norm(p=2, dim=0))
        x_t = (proj @ A).unsqueeze(-1) * self.t
        err = (x_t.cos().mean(-3) - self.phi).square() + x_t.sin().mean(-3).square()
        statistic = (err @ self.weights) * proj.size(-2)
        return statistic.mean()



class HFDataset(torch.utils.data.Dataset):
    def __init__(self, split, V=1):
        self.V = V
        self.ds = load_dataset("frgfm/imagenette", "160px", split=split)
        self.aug = v2.Compose(
            [
                v2.RandomResizedCrop(128, scale=(0.08, 1.0)),
                v2.RandomApply([v2.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8),
                v2.RandomGrayscale(p=0.2),
                v2.RandomApply([v2.GaussianBlur(kernel_size=7, sigma=(0.1, 2.0))]),
                v2.RandomApply([v2.RandomSolarize(threshold=128)], p=0.2),
                v2.RandomHorizontalFlip(),
                v2.ToImage(),
                v2.ToDtype(torch.float32, scale=True),
                v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ]
        )
        self.test = v2.Compose(
            [
                v2.Resize(128),
                v2.CenterCrop(128),
                v2.ToImage(),
                v2.ToDtype(torch.float32, scale=True),
                v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ]
        )

    def __getitem__(self, i):
        item = self.ds[i]
        img = item["image"].convert("RGB")
        transform = self.aug if self.V > 1 else self.test
        return torch.stack([transform(img) for _ in range(self.V)]), item["label"]

    def __len__(self):
        return len(self.ds)


# %% ../../nbs/06a_beam_search.ipynb 5
from ..trainers.trainer import Trainer
from ..models.utils import save_checkpoint

class VAETrainer(Trainer):
    def __init__(self, cfg, model, train_loader, val_loader=None, 
                 criterion=None, optimizer=None, device=None,
                 earlystopping=None, scheduler=None, writer= None):
        
        super().__init__(cfg, model, train_loader, val_loader, criterion, optimizer, device)
        self.earlystopping = earlystopping
        self.scheduler = scheduler
        self.writer = writer

        self.vae_dir = os.path.join(self.cfg.log_dir, 'vae_marlrid')
        if not os.path.exists(self.vae_dir):
            os.mkdir(self.vae_dir)
            os.mkdir(os.path.join(self.vae_dir, 'samples'))

    

# %% ../../nbs/06a_beam_search.ipynb 6
@patch
def reload(self: VAETrainer):

    reload_file = os.path.join(self.vae_dir, 'best.pth')
    if not self.cfg.noreload and os.path.exists(reload_file):
        state = torch.load(reload_file)
        print("Reloading model at epoch {}"
            ", with test error {}".format(
                state['epoch'],
                state['precision']))
        
        self.model.load_state_dict(state['state_dict'])
        self.optimizer.load_state_dict(state['optimizer'])
        self.scheduler.load_state_dict(state['scheduler'])
        self.earlystopping.load_state_dict(state['earlystopping'])



# %% ../../nbs/06a_beam_search.ipynb 7
@patch
def train_epoch(self: VAETrainer, epoch):
    self.model.train()
    self.train_loader.dataset.load_next_buffer()
    train_loss = 0
    LATENT_DIM = 20
    TOTAL_EPOCHS = self.cfg.epochs
    ANNEALING_EPOCHS = 50  # Beta reaches its maximum value after 50 epochs
    MAX_BETA = 1.0  
    for batch_idx, data in enumerate(self.train_loader):
        # import pdb; pdb.set_trace()
        beta = min(epoch / ANNEALING_EPOCHS, 1.0) * MAX_BETA
        obs, dones, agent_id = data
        mask = ~dones.bool()     # keep only where done is False

        if mask.sum() == 0:
            continue  # entire batch is terminals

        obs = obs[mask]          # filter observations
        obs = obs.to(self.device)

        self.optimizer.zero_grad()
        recon_batch, x, mu, logvar = self.model(obs)
        loss = self.criterion(recon_batch, obs, mu, logvar, beta)
        loss.backward()
        train_loss += loss.item()
        self.optimizer.step()
        if batch_idx % 20 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(obs), len(self.train_loader.dataset),
                100. * batch_idx / len(self.train_loader),
                loss.item() / len(obs)))

    print('====> Epoch: {} Average loss: {:.4f}'.format(
        epoch, train_loss / len(self.train_loader.dataset)))

    return train_loss / len(self.train_loader.dataset)
       

# %% ../../nbs/06a_beam_search.ipynb 8
@patch
def eval_epoch(self: VAETrainer):
    self.model.eval()
    self.val_loader.dataset.load_next_buffer()
    test_loss = 0
    with torch.no_grad():
        for data in self.val_loader:
            obs, dones, agent_id = data
            mask = ~dones.bool()     # keep only where done is False

            if mask.sum() == 0:
                continue  # entire batch is terminals

            obs = obs[mask]          # filter observations
            obs = obs.to(self.device)
            recon_batch, x, mu, logvar = self.model(obs)
            test_loss += self.criterion(recon_batch, obs, mu, logvar).item()

    test_loss /= len(self.val_loader.dataset)
    print('====> Test set loss: {:.4f}'.format(test_loss))
    return test_loss


# %% ../../nbs/06a_beam_search.ipynb 9
import wandb


@patch
def fit(self: VAETrainer):
    cur_best = None
    lst_dfs = []

    for epoch in range(1, self.cfg.epochs + 1):
        train_loss = self.train_epoch(epoch)
        test_loss = self.eval_epoch()
        self.scheduler.step(test_loss)

        # checkpointing
        best_filename = os.path.join(self.vae_dir, 'best.pth')
        filename = os.path.join(self.vae_dir, 'checkpoint.pth')

        is_best = not cur_best or test_loss < cur_best
        if is_best:
            cur_best = test_loss

        save_checkpoint({
            'epoch': epoch,
            'state_dict': self.model.state_dict(),
            'precision': test_loss,
            'optimizer': self.optimizer.state_dict(),
            'scheduler': self.scheduler.state_dict(),
            # 'earlystopping': self.earlystopping.state_dict()
        }, is_best, filename, best_filename)

        if self.earlystopping.early_stop(test_loss):             
            break
       

        to_log = {
            "train_loss": train_loss, 
            "test_loss": test_loss,
        }

        self.writer.write(to_log)
        df = pd.DataFrame.from_records([{"epoch": epoch ,"train_loss": train_loss, "test_loss":test_loss}], index= "epoch")
        lst_dfs.append(df)

    df_res = pd.concat(lst_dfs)
    df_reset = df_res.reset_index()
    self.writer.write({'Train-Val Loss Table': wandb.Table(dataframe= df_reset)})

    self.writer.finish()
    return df_reset
