"""This module handles all aspects of the VAE, including encoding, decoding, and latent space representation."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/06a_search.beam_search.ipynb.

# %% auto 0
__all__ = ['neural_guided_beam_search']

# %% ../../nbs/06a_search.beam_search.ipynb 3
from fastcore import *
from fastcore.utils import *
from torchvision.utils import save_image
import torch
import os
from torch import nn
import torch.nn.functional as F
import pandas as pd

# %% ../../nbs/06a_search.beam_search.ipynb 5
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from ..core import Program, PRIMITIVE_TEMPLATES
from ..models.program_embedder import batchify_programs
from ..models.program_encoder import ProgramEncoder
from ..models.program_synthizer import Proposer

@torch.no_grad()
def neural_guided_beam_search(
    z,
    program_embedder: nn.Module,
    proposer: nn.Module,
    program_encoder: nn.Module,
    score_fn,
    MAX_PARAMS=3,
    beam_width=5,
    topk=6,
    max_prog_len=5,
    grid_size=7,
    lambdas= (0.25, 0.5, 0.25),
    device="cuda"
):
    """
    neural-guided beam search.
    """

    # --- normalize z shape ---
    z = z.to(device)
    if z.dim() == 1:
        z = z.unsqueeze(0)
    lambda_1, lambda_2, lambda_3 = lambdas

    sos_idx = proposer.sos_idx
    EOS_IDX = len(PRIMITIVE_TEMPLATES)
    zero_params = torch.full((MAX_PARAMS,), -1, device=device)

    # initial program: [SOS]
    init_prog = Program(tokens=[(sos_idx, zero_params.tolist())])
    beam = [(init_prog, 0.0)]  # (program, score)

    best_program = init_prog
    best_score = -float("inf")

    # ---- start beam search ----
    for depth in range(1, max_prog_len + 1):
        alive = []
        finished = []

        for prog, score in beam:
            last_prim = prog.tokens[-1][0]
            if last_prim == EOS_IDX:
                finished.append((prog, score))
            else:
                alive.append((prog, score))

        if len(alive) == 0:
            break
        # ---- 1) build batch of prefixes ----
        prefix_programs = [p for (p, _) in alive]
        prev_idx_batch, prev_params_batch = batchify_programs(prefix_programs, padding_vals=[EOS_IDX, -1])

        B = prev_idx_batch.shape[0]

        p_vec = program_embedder(prev_idx_batch.to(device),
                                 prev_params_batch.to(device))

        # replicate z
        z_batch = z.repeat(B, 1)

        # proposer forward
        def get_proposer(seq_len):
            return Proposer(
                    obs_dim= z.shape[-1],
                    num_prims= len(PRIMITIVE_TEMPLATES),
                    max_params= 2,
                    seq_len= seq_len,
                    prog_emb_dim_x= 32,
                    prog_emb_dim_y= 32,
                    prog_emb_dim_prims= 32,
                )

        proposer = get_proposer(prev_idx_batch.shape[-1])
        prim_logits_batch, param_pred_batch = proposer.forward_step(z_batch, p_vec)

        # ---- 2) expand each beam entry ----
        expansions = []
        for b_i, (prog_parent, parent_score) in enumerate(alive):
            prim_logprobs = F.log_softmax(prim_logits_batch[b_i], dim=-1)
            top_vals, top_idx = torch.topk(prim_logprobs, k=topk)

            for k_i in range(topk):
                prim_idx = int(top_idx[k_i].item())
                prim_logp = float(top_vals[k_i].item())
                if prim_idx == EOS_IDX:
                    # add program as completed (no more expansions)
                    new_prog = prog_parent.extend(prim_idx, [])
                    expansions.append((new_prog, parent_score + prim_logp))
                    # print(f"Beam search found EOS at depth {depth} with program: {new_prog}")
                    continue

                arity = PRIMITIVE_TEMPLATES[prim_idx][1]              

                # ----- parameter instantiation -----
                instantiations = []
                if arity > 0:
                    pred_params = param_pred_batch[b_i][:arity].cpu().numpy()
                    for _ in range(3):
                        if np.random.rand() < 0.7:
                            inst = [
                                float(round(float(p) * (grid_size - 1)))
                                for p in pred_params
                            ]
                        else:
                            inst = [
                                float(np.random.randint(0, grid_size))
                                for _ in range(arity)
                            ]
                        instantiations.append(inst)
                else:
                    instantiations.append([])

                # ----- create new beam children -----
                for inst_params in instantiations:
                    new_prog = prog_parent.extend(prim_idx, inst_params)
                    expansions.append((new_prog, parent_score + prim_logp))

        if not expansions:
            break

        # ---- 3) Score all expanded programs ----
        cand_programs = [p for (p, _) in expansions]
        prim_ids_padded, params_padded = batchify_programs(cand_programs, padding_vals=[EOS_IDX, -1])

        def get_pencoder(seq_len):
            return ProgramEncoder(num_primitives= len(PRIMITIVE_TEMPLATES),
                    param_cardinalities= [grid_size, grid_size],
                    seq_len= seq_len,
                    max_params_per_primitive= 2)
        
        with torch.no_grad():
            program_encoder = get_pencoder(prim_ids_padded.shape[-1])
            prog_emb_batch = program_encoder(prim_ids_padded, params_padded)

        scores = score_fn(z, prog_emb_batch)

        # ---- 4) attach scores and prune ----
        candidates = []
        for idx_c, (prog, base_score) in enumerate(expansions):
            total = lambda_1 * base_score + lambda_2 * float(scores[idx_c].item()) - lambda_3 * len(prog.tokens) 
            candidates.append((prog, total))

        candidates.sort(key=lambda x: x[1], reverse=True)
        # best_program, best_score = max(candidates, key=lambda x: x[1])

        topk_from_alive = candidates#[:beam_width]
        beam = topk_from_alive + finished

        filtered_beam = []
        for p in beam:
            if len(p[0].tokens) == 1 and p[0].tokens[0][0] == -1:
                continue
            filtered_beam.append(p)
        if len(filtered_beam) == 0:
            break

        best_program, best_score = max(filtered_beam, key=lambda x: x[1])
        beam = sorted(beam, key=lambda x: x[1], reverse=True)[:beam_width]

        # update alive + finished splits
        alive  = [(p,s) for (p,s) in beam if not p.finished]
        finished = [(p,s) for (p,s) in beam if p.finished]

    return best_program, best_score

