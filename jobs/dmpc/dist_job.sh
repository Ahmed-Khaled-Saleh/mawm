#!/bin/bash
#SBATCH --account=project_2009050
#SBATCH --job-name=distributed_wm_training
#SBATCH --partition=gpusmall
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=32
#SBATCH --mem=100G
#SBATCH --time=2:00:00
#SBATCH --gres=gpu:a100:2
#SBATCH --output=./logs/out_%j_%x_%N.log  # includes time stamp (t), job ID(j), job name (x), and node name (N)
#SBATCH --error=./logs/err_%j_%x_%N.err


module --force purge
module load pytorch
source /projappl/project_2009050/mytorch/bin/activate
cd /projappl/project_2009050/mawm/mains/

export PYTHONPATH=$PYTHONPATH:/projappl/project_2009050/mytorch/lib/python3.11/site-packages
echo "Current PYTHONPATH: $PYTHONPATH"



ts=$(date +%Y%m%d_%H%M%S)
srun torchrun --standalone --nnodes=1 --nproc_per_node=2 train_wm.py ../cfgs/MPCJepa/mpc.yaml --env_file ../.env --timestamp ${ts}
# srun python vicreg_main.py --config ../cfgs/MPCJepa/mpc.yaml --env_file ../.env --timestamp ${ts}