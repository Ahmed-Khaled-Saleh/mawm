{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9980c183",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a1398",
   "metadata": {},
   "source": [
    "# Main VAE\n",
    "\n",
    "> Main file to train VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15226553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore import *\n",
    "from fastcore.utils import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82bff6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "from os.path import join, exists\n",
    "from os import mkdir\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from omegaconf import OmegaConf\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "835ae81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mawm.core import get_cls\n",
    "\n",
    "from mawm.data.utils import transform_train, transform_test\n",
    "from mawm.data.loaders import RolloutObservationDataset\n",
    "\n",
    "from mawm.optimizer.utils import ReduceLROnPlateau, EarlyStopping\n",
    "from mawm.trainers.vae_trainer import VAETrainer\n",
    "from mawm.writers.wandb_writer import WandbWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01247562",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(join(\"../cfgs\", \"vae\", \"cfg.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "221dabaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'project_name': 'vae_meltingpot', 'epochs': 1000, 'loss_fn': 'CrossEntropyLoss', 'writer': 'WandbWriter', 'noreload': False, 'root_dir': '/scratch/project_2009050/', 'save_dir': 'models', 'res_dir': 'results', 'log_dir': '../logs', 'state_dir': 'aggregated_model_', 'data': {'data_dir': '/scratch/project_2009050/datasets/meltingpot_data', 'batch_size': 32, 'name': 'meltingpot'}, 'model': {'name': 'VAE', 'channels': 3, 'img_size': 40, 'latent_size': 512, 'grad_norm_clip': 1.0}, 'optimizer': {'name': 'Adam', 'lr': 0.001}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d4bcfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.epochs = 2\n",
    "cfg.data.data_dir = \"../meltingpot_data/\"\n",
    "cfg.data.batch_size = 1\n",
    "cfg.model.latent_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85a078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='VAE Training')\n",
    "parser.add_argument('--config', type=str, help='Path to the YAML config file', required=True)\n",
    "parser.add_argument('--timestamp', type=str, help='Time stamp', required=True)\n",
    "parser.add_argument('--env_file', type=str, help='Path to the .env file', required=False)\n",
    "\n",
    "\n",
    "parser.add_argument('--batch-size', type=int, default=32, metavar='N',\n",
    "                    help='input batch size for training (default: 32)')\n",
    "parser.add_argument('--epochs', type=int, default=1000, metavar='N',\n",
    "                    help='number of epochs to train (default: 1000)')\n",
    "parser.add_argument('--log_dir', type=str, help='Directory where results are logged')\n",
    "parser.add_argument('--noreload', action='store_true',\n",
    "                    help='Best model is not reloaded if specified')\n",
    "parser.add_argument('--nosamples', action='store_true',\n",
    "                    help='Does not save samples during training if specified')\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9da8b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if args.env_file:\n",
    "    load_dotenv(args.env_file)\n",
    "    key = os.getenv(\"WANDB_API_KEY\", None)\n",
    "    hf_secret = os.getenv(\"HF_SECRET_CODE\", None)\n",
    "\n",
    "    if key:\n",
    "        os.environ[\"WANDB_API_KEY\"] = key\n",
    "    if hf_secret:\n",
    "        os.environ[\"HF_SECRET_CODE\"] = hf_secret     \n",
    "\n",
    "try:\n",
    "    cfg = OmegaConf.load(args.config)\n",
    "except:\n",
    "    print(\"Invalid config file path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb47a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "key = os.getenv(\"WANDB_API_KEY\", None)\n",
    "hf_secret = os.getenv(\"HF_SECRET_CODE\", None)\n",
    "\n",
    "if key:\n",
    "    os.environ[\"WANDB_API_KEY\"] = key\n",
    "if hf_secret:\n",
    "    os.environ[\"HF_SECRET_CODE\"] = hf_secret     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12178710",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg.now = args.timestamp \n",
    "\n",
    "cfg.optimizer.lr = float(args.lr) if args.lr else cfg.optimizer.lr\n",
    "cfg.data.batch_size = int(args.batch_size) if args.batch_size else cfg.data.batch_size\n",
    "cfg.optimizer.name = args.optimizer if args.optimizer else cfg.optimizer.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd598bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "# Fix numeric divergence due to bug in Cudnn\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae105865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../meltingpot_data/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.data.data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c99f9443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading file buffer ...: 100%|██████████| 200/200 \n",
      "Loading file buffer ...: 100%|██████████| 200/200 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_train = RolloutObservationDataset(cfg.data.data_dir,\n",
    "                                          transform_train, \n",
    "                                          train=True)\n",
    "\n",
    "dataset_test = RolloutObservationDataset(cfg.data.data_dir,\n",
    "                                         transform_test,\n",
    "                                         train=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=cfg.data.batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=cfg.data.batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "model_cls = get_cls(f\"MAWM.models.{cfg.model.name.lower()}\", cfg.model.name)\n",
    "model = model_cls(cfg.model.channels, cfg.model.latent_size).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bdd78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (fc_mu): Linear(in_features=1024, out_features=128, bias=True)\n",
       "    (fc_logsigma): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (fc1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (deconv1): ConvTranspose2d(1024, 128, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (deconv2): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (deconv3): ConvTranspose2d(64, 32, kernel_size=(6, 6), stride=(2, 2))\n",
       "    (deconv4): ConvTranspose2d(32, 3, kernel_size=(6, 6), stride=(2, 2))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "from mawm.core import get_cls\n",
    "cls = get_cls(f\"MAWM.models.{\"VAE\".lower()}\", \"VAE\")\n",
    "model = cls(3, 32)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b767f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.latent_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "236690eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "optimizer_cls = get_cls(\"torch.optim\", cfg.optimizer.name)\n",
    "optimizer = optimizer_cls(model.parameters(), lr=cfg.optimizer.lr)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5)\n",
    "earlystopping = EarlyStopping('min', patience=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97150d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def criterion(recon_x, x, mu, logsigma):\n",
    "    \"\"\" VAE loss function \"\"\"\n",
    "    BCE = F.mse_loss(recon_x, x, size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + 2 * logsigma - mu.pow(2) - (2 * logsigma).exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c8b31d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.log_dir = \"../logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13dc2e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'VAE', 'channels': 3, 'img_size': 40, 'latent_size': 32, 'grad_norm_clip': 1.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74a5c31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ahmed/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ahmed/Ahmed-home/1- Projects/Research/Journal 2/Code/MAWM/mains/wandb/run-20251114_144741-rg62d8vt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/edge-ai-team/vae_meltingpot/runs/rg62d8vt' target=\"_blank\">vae_meltingpot20251114-144725</a></strong> to <a href='https://wandb.ai/edge-ai-team/vae_meltingpot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/edge-ai-team/vae_meltingpot' target=\"_blank\">https://wandb.ai/edge-ai-team/vae_meltingpot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/edge-ai-team/vae_meltingpot/runs/rg62d8vt' target=\"_blank\">https://wandb.ai/edge-ai-team/vae_meltingpot/runs/rg62d8vt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading file buffer ...: 100%|██████████| 200/200 \n",
      "/home/ahmed/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/200000 (0%)]\tLoss: 540.554382\n",
      "Train Epoch: 1 [20/200000 (0%)]\tLoss: 178.176559\n",
      "Train Epoch: 1 [40/200000 (0%)]\tLoss: 135.531128\n",
      "Train Epoch: 1 [60/200000 (0%)]\tLoss: 85.845726\n",
      "Train Epoch: 1 [80/200000 (0%)]\tLoss: 102.556778\n",
      "Train Epoch: 1 [100/200000 (0%)]\tLoss: 145.663788\n",
      "Train Epoch: 1 [120/200000 (0%)]\tLoss: 127.409119\n",
      "Train Epoch: 1 [140/200000 (0%)]\tLoss: 71.151588\n",
      "Train Epoch: 1 [160/200000 (0%)]\tLoss: 95.671219\n",
      "Train Epoch: 1 [180/200000 (0%)]\tLoss: 82.016563\n",
      "Train Epoch: 1 [200/200000 (0%)]\tLoss: 85.815201\n",
      "Train Epoch: 1 [220/200000 (0%)]\tLoss: 96.676323\n",
      "Train Epoch: 1 [240/200000 (0%)]\tLoss: 45.676609\n",
      "Train Epoch: 1 [260/200000 (0%)]\tLoss: 92.091774\n",
      "Train Epoch: 1 [280/200000 (0%)]\tLoss: 95.355728\n",
      "Train Epoch: 1 [300/200000 (0%)]\tLoss: 148.989197\n",
      "Train Epoch: 1 [320/200000 (0%)]\tLoss: 58.964054\n",
      "Train Epoch: 1 [340/200000 (0%)]\tLoss: 38.238411\n",
      "Train Epoch: 1 [360/200000 (0%)]\tLoss: 91.555275\n",
      "Train Epoch: 1 [380/200000 (0%)]\tLoss: 102.124641\n",
      "Train Epoch: 1 [400/200000 (0%)]\tLoss: 73.699081\n",
      "Train Epoch: 1 [420/200000 (0%)]\tLoss: 74.998131\n",
      "Train Epoch: 1 [440/200000 (0%)]\tLoss: 60.441265\n",
      "Train Epoch: 1 [460/200000 (0%)]\tLoss: 69.106239\n",
      "Train Epoch: 1 [480/200000 (0%)]\tLoss: 75.542381\n",
      "Train Epoch: 1 [500/200000 (0%)]\tLoss: 39.940639\n",
      "Train Epoch: 1 [520/200000 (0%)]\tLoss: 77.317360\n",
      "Train Epoch: 1 [540/200000 (0%)]\tLoss: 59.376751\n",
      "Train Epoch: 1 [560/200000 (0%)]\tLoss: 66.413757\n",
      "Train Epoch: 1 [580/200000 (0%)]\tLoss: 68.493477\n",
      "Train Epoch: 1 [600/200000 (0%)]\tLoss: 56.334831\n",
      "Train Epoch: 1 [620/200000 (0%)]\tLoss: 60.789104\n",
      "Train Epoch: 1 [640/200000 (0%)]\tLoss: 53.756844\n",
      "Train Epoch: 1 [660/200000 (0%)]\tLoss: 30.421551\n",
      "Train Epoch: 1 [680/200000 (0%)]\tLoss: 36.994469\n",
      "Train Epoch: 1 [700/200000 (0%)]\tLoss: 49.831409\n",
      "Train Epoch: 1 [720/200000 (0%)]\tLoss: 54.848206\n",
      "Train Epoch: 1 [740/200000 (0%)]\tLoss: 62.177441\n",
      "Train Epoch: 1 [760/200000 (0%)]\tLoss: 72.326439\n",
      "Train Epoch: 1 [780/200000 (0%)]\tLoss: 33.316380\n",
      "Train Epoch: 1 [800/200000 (0%)]\tLoss: 41.249813\n",
      "Train Epoch: 1 [820/200000 (0%)]\tLoss: 66.957169\n",
      "Train Epoch: 1 [840/200000 (0%)]\tLoss: 106.672279\n",
      "Train Epoch: 1 [860/200000 (0%)]\tLoss: 141.474884\n",
      "Train Epoch: 1 [880/200000 (0%)]\tLoss: 95.037048\n",
      "Train Epoch: 1 [900/200000 (0%)]\tLoss: 62.786350\n",
      "Train Epoch: 1 [920/200000 (0%)]\tLoss: 54.186146\n",
      "Train Epoch: 1 [940/200000 (0%)]\tLoss: 91.577049\n",
      "Train Epoch: 1 [960/200000 (0%)]\tLoss: 73.631424\n",
      "Train Epoch: 1 [980/200000 (0%)]\tLoss: 27.509878\n",
      "Train Epoch: 1 [1000/200000 (0%)]\tLoss: 16.253345\n",
      "Train Epoch: 1 [1020/200000 (1%)]\tLoss: 60.212631\n",
      "Train Epoch: 1 [1040/200000 (1%)]\tLoss: 49.727982\n",
      "Train Epoch: 1 [1060/200000 (1%)]\tLoss: 26.932547\n",
      "Train Epoch: 1 [1080/200000 (1%)]\tLoss: 25.659550\n",
      "Train Epoch: 1 [1100/200000 (1%)]\tLoss: 34.058701\n",
      "Train Epoch: 1 [1120/200000 (1%)]\tLoss: 18.841066\n",
      "Train Epoch: 1 [1140/200000 (1%)]\tLoss: 13.053649\n",
      "Train Epoch: 1 [1160/200000 (1%)]\tLoss: 33.049965\n",
      "Train Epoch: 1 [1180/200000 (1%)]\tLoss: 26.439875\n",
      "Train Epoch: 1 [1200/200000 (1%)]\tLoss: 36.233467\n",
      "Train Epoch: 1 [1220/200000 (1%)]\tLoss: 46.574310\n",
      "Train Epoch: 1 [1240/200000 (1%)]\tLoss: 40.438538\n",
      "Train Epoch: 1 [1260/200000 (1%)]\tLoss: 27.979626\n",
      "Train Epoch: 1 [1280/200000 (1%)]\tLoss: 85.543571\n",
      "Train Epoch: 1 [1300/200000 (1%)]\tLoss: 45.683220\n",
      "Train Epoch: 1 [1320/200000 (1%)]\tLoss: 25.893471\n",
      "Train Epoch: 1 [1340/200000 (1%)]\tLoss: 39.501293\n",
      "Train Epoch: 1 [1360/200000 (1%)]\tLoss: 79.820580\n",
      "Train Epoch: 1 [1380/200000 (1%)]\tLoss: 20.077234\n",
      "Train Epoch: 1 [1400/200000 (1%)]\tLoss: 81.121017\n",
      "Train Epoch: 1 [1420/200000 (1%)]\tLoss: 69.374138\n",
      "Train Epoch: 1 [1440/200000 (1%)]\tLoss: 24.757294\n",
      "Train Epoch: 1 [1460/200000 (1%)]\tLoss: 56.844757\n",
      "Train Epoch: 1 [1480/200000 (1%)]\tLoss: 48.576168\n",
      "Train Epoch: 1 [1500/200000 (1%)]\tLoss: 55.458908\n",
      "Train Epoch: 1 [1520/200000 (1%)]\tLoss: 76.689041\n",
      "Train Epoch: 1 [1540/200000 (1%)]\tLoss: 53.509773\n",
      "Train Epoch: 1 [1560/200000 (1%)]\tLoss: 85.818481\n",
      "Train Epoch: 1 [1580/200000 (1%)]\tLoss: 40.859055\n",
      "Train Epoch: 1 [1600/200000 (1%)]\tLoss: 48.431847\n",
      "Train Epoch: 1 [1620/200000 (1%)]\tLoss: 25.254261\n",
      "Train Epoch: 1 [1640/200000 (1%)]\tLoss: 33.329956\n",
      "Train Epoch: 1 [1660/200000 (1%)]\tLoss: 66.407387\n",
      "Train Epoch: 1 [1680/200000 (1%)]\tLoss: 55.677498\n",
      "Train Epoch: 1 [1700/200000 (1%)]\tLoss: 74.943207\n",
      "Train Epoch: 1 [1720/200000 (1%)]\tLoss: 16.909492\n",
      "Train Epoch: 1 [1740/200000 (1%)]\tLoss: 42.112846\n",
      "Train Epoch: 1 [1760/200000 (1%)]\tLoss: 54.280201\n",
      "Train Epoch: 1 [1780/200000 (1%)]\tLoss: 59.796982\n",
      "Train Epoch: 1 [1800/200000 (1%)]\tLoss: 44.930317\n",
      "Train Epoch: 1 [1820/200000 (1%)]\tLoss: 22.975479\n",
      "Train Epoch: 1 [1840/200000 (1%)]\tLoss: 23.476618\n",
      "Train Epoch: 1 [1860/200000 (1%)]\tLoss: 12.901333\n",
      "Train Epoch: 1 [1880/200000 (1%)]\tLoss: 45.229530\n",
      "Train Epoch: 1 [1900/200000 (1%)]\tLoss: 50.899902\n",
      "Train Epoch: 1 [1920/200000 (1%)]\tLoss: 48.527866\n",
      "Train Epoch: 1 [1940/200000 (1%)]\tLoss: 26.094002\n",
      "Train Epoch: 1 [1960/200000 (1%)]\tLoss: 38.809525\n",
      "Train Epoch: 1 [1980/200000 (1%)]\tLoss: 52.175373\n",
      "Train Epoch: 1 [2000/200000 (1%)]\tLoss: 41.606998\n",
      "Train Epoch: 1 [2020/200000 (1%)]\tLoss: 103.285027\n",
      "Train Epoch: 1 [2040/200000 (1%)]\tLoss: 47.050358\n",
      "Train Epoch: 1 [2060/200000 (1%)]\tLoss: 31.901871\n",
      "Train Epoch: 1 [2080/200000 (1%)]\tLoss: 18.074789\n",
      "Train Epoch: 1 [2100/200000 (1%)]\tLoss: 37.113808\n",
      "Train Epoch: 1 [2120/200000 (1%)]\tLoss: 45.320145\n",
      "Train Epoch: 1 [2140/200000 (1%)]\tLoss: 24.657175\n",
      "Train Epoch: 1 [2160/200000 (1%)]\tLoss: 67.734695\n",
      "Train Epoch: 1 [2180/200000 (1%)]\tLoss: 42.209106\n",
      "Train Epoch: 1 [2200/200000 (1%)]\tLoss: 37.770321\n",
      "Train Epoch: 1 [2220/200000 (1%)]\tLoss: 20.408667\n",
      "Train Epoch: 1 [2240/200000 (1%)]\tLoss: 20.395039\n",
      "Train Epoch: 1 [2260/200000 (1%)]\tLoss: 54.288166\n",
      "Train Epoch: 1 [2280/200000 (1%)]\tLoss: 45.530556\n",
      "Train Epoch: 1 [2300/200000 (1%)]\tLoss: 38.838078\n",
      "Train Epoch: 1 [2320/200000 (1%)]\tLoss: 26.530512\n",
      "Train Epoch: 1 [2340/200000 (1%)]\tLoss: 34.385815\n",
      "Train Epoch: 1 [2360/200000 (1%)]\tLoss: 49.430908\n",
      "Train Epoch: 1 [2380/200000 (1%)]\tLoss: 17.823671\n",
      "Train Epoch: 1 [2400/200000 (1%)]\tLoss: 50.894302\n",
      "Train Epoch: 1 [2420/200000 (1%)]\tLoss: 50.045231\n",
      "Train Epoch: 1 [2440/200000 (1%)]\tLoss: 33.322575\n",
      "Train Epoch: 1 [2460/200000 (1%)]\tLoss: 57.263813\n",
      "Train Epoch: 1 [2480/200000 (1%)]\tLoss: 34.490280\n",
      "Train Epoch: 1 [2500/200000 (1%)]\tLoss: 27.933407\n",
      "Train Epoch: 1 [2520/200000 (1%)]\tLoss: 31.552803\n",
      "Train Epoch: 1 [2540/200000 (1%)]\tLoss: 27.959496\n",
      "Train Epoch: 1 [2560/200000 (1%)]\tLoss: 19.174002\n",
      "Train Epoch: 1 [2580/200000 (1%)]\tLoss: 53.741890\n",
      "Train Epoch: 1 [2600/200000 (1%)]\tLoss: 56.135559\n",
      "Train Epoch: 1 [2620/200000 (1%)]\tLoss: 17.057528\n",
      "Train Epoch: 1 [2640/200000 (1%)]\tLoss: 51.662384\n",
      "Train Epoch: 1 [2660/200000 (1%)]\tLoss: 44.462837\n",
      "Train Epoch: 1 [2680/200000 (1%)]\tLoss: 45.309345\n",
      "Train Epoch: 1 [2700/200000 (1%)]\tLoss: 40.553005\n",
      "Train Epoch: 1 [2720/200000 (1%)]\tLoss: 52.202721\n",
      "Train Epoch: 1 [2740/200000 (1%)]\tLoss: 18.011187\n",
      "Train Epoch: 1 [2760/200000 (1%)]\tLoss: 49.817303\n",
      "Train Epoch: 1 [2780/200000 (1%)]\tLoss: 25.506260\n",
      "Train Epoch: 1 [2800/200000 (1%)]\tLoss: 18.611349\n",
      "Train Epoch: 1 [2820/200000 (1%)]\tLoss: 79.708115\n",
      "Train Epoch: 1 [2840/200000 (1%)]\tLoss: 58.942131\n",
      "Train Epoch: 1 [2860/200000 (1%)]\tLoss: 47.682747\n",
      "Train Epoch: 1 [2880/200000 (1%)]\tLoss: 55.271397\n",
      "Train Epoch: 1 [2900/200000 (1%)]\tLoss: 57.720112\n",
      "Train Epoch: 1 [2920/200000 (1%)]\tLoss: 39.687580\n",
      "Train Epoch: 1 [2940/200000 (1%)]\tLoss: 54.493275\n",
      "Train Epoch: 1 [2960/200000 (1%)]\tLoss: 51.429703\n",
      "Train Epoch: 1 [2980/200000 (1%)]\tLoss: 55.267452\n",
      "Train Epoch: 1 [3000/200000 (2%)]\tLoss: 58.485832\n",
      "Train Epoch: 1 [3020/200000 (2%)]\tLoss: 38.209785\n",
      "Train Epoch: 1 [3040/200000 (2%)]\tLoss: 24.691362\n",
      "Train Epoch: 1 [3060/200000 (2%)]\tLoss: 22.289928\n",
      "Train Epoch: 1 [3080/200000 (2%)]\tLoss: 100.481537\n",
      "Train Epoch: 1 [3100/200000 (2%)]\tLoss: 17.521721\n",
      "Train Epoch: 1 [3120/200000 (2%)]\tLoss: 25.397701\n",
      "Train Epoch: 1 [3140/200000 (2%)]\tLoss: 23.744518\n",
      "Train Epoch: 1 [3160/200000 (2%)]\tLoss: 45.911129\n",
      "Train Epoch: 1 [3180/200000 (2%)]\tLoss: 16.915529\n",
      "Train Epoch: 1 [3200/200000 (2%)]\tLoss: 12.478151\n",
      "Train Epoch: 1 [3220/200000 (2%)]\tLoss: 44.719189\n",
      "Train Epoch: 1 [3240/200000 (2%)]\tLoss: 32.634850\n",
      "Train Epoch: 1 [3260/200000 (2%)]\tLoss: 51.974815\n",
      "Train Epoch: 1 [3280/200000 (2%)]\tLoss: 34.346100\n",
      "Train Epoch: 1 [3300/200000 (2%)]\tLoss: 19.469482\n",
      "Train Epoch: 1 [3320/200000 (2%)]\tLoss: 19.923635\n",
      "Train Epoch: 1 [3340/200000 (2%)]\tLoss: 28.204201\n",
      "Train Epoch: 1 [3360/200000 (2%)]\tLoss: 33.149349\n",
      "Train Epoch: 1 [3380/200000 (2%)]\tLoss: 55.059963\n",
      "Train Epoch: 1 [3400/200000 (2%)]\tLoss: 49.867764\n",
      "Train Epoch: 1 [3420/200000 (2%)]\tLoss: 34.641182\n",
      "Train Epoch: 1 [3440/200000 (2%)]\tLoss: 36.127903\n",
      "Train Epoch: 1 [3460/200000 (2%)]\tLoss: 17.743557\n",
      "Train Epoch: 1 [3480/200000 (2%)]\tLoss: 18.221737\n",
      "Train Epoch: 1 [3500/200000 (2%)]\tLoss: 34.022892\n",
      "Train Epoch: 1 [3520/200000 (2%)]\tLoss: 12.979944\n",
      "Train Epoch: 1 [3540/200000 (2%)]\tLoss: 28.728014\n",
      "Train Epoch: 1 [3560/200000 (2%)]\tLoss: 56.531410\n",
      "Train Epoch: 1 [3580/200000 (2%)]\tLoss: 36.069710\n",
      "Train Epoch: 1 [3600/200000 (2%)]\tLoss: 29.972050\n",
      "Train Epoch: 1 [3620/200000 (2%)]\tLoss: 46.407425\n",
      "Train Epoch: 1 [3640/200000 (2%)]\tLoss: 15.019685\n",
      "Train Epoch: 1 [3660/200000 (2%)]\tLoss: 31.617437\n",
      "Train Epoch: 1 [3680/200000 (2%)]\tLoss: 33.914482\n",
      "Train Epoch: 1 [3700/200000 (2%)]\tLoss: 81.855537\n",
      "Train Epoch: 1 [3720/200000 (2%)]\tLoss: 42.016754\n",
      "Train Epoch: 1 [3740/200000 (2%)]\tLoss: 17.597591\n",
      "Train Epoch: 1 [3760/200000 (2%)]\tLoss: 20.712490\n",
      "Train Epoch: 1 [3780/200000 (2%)]\tLoss: 24.077429\n",
      "Train Epoch: 1 [3800/200000 (2%)]\tLoss: 32.668713\n",
      "Train Epoch: 1 [3820/200000 (2%)]\tLoss: 61.654377\n",
      "Train Epoch: 1 [3840/200000 (2%)]\tLoss: 32.140308\n",
      "Train Epoch: 1 [3860/200000 (2%)]\tLoss: 19.261353\n",
      "Train Epoch: 1 [3880/200000 (2%)]\tLoss: 38.318829\n",
      "Train Epoch: 1 [3900/200000 (2%)]\tLoss: 86.437263\n",
      "Train Epoch: 1 [3920/200000 (2%)]\tLoss: 31.762730\n",
      "Train Epoch: 1 [3940/200000 (2%)]\tLoss: 17.267817\n",
      "Train Epoch: 1 [3960/200000 (2%)]\tLoss: 42.213104\n",
      "Train Epoch: 1 [3980/200000 (2%)]\tLoss: 63.872913\n",
      "Train Epoch: 1 [4000/200000 (2%)]\tLoss: 20.056265\n",
      "Train Epoch: 1 [4020/200000 (2%)]\tLoss: 22.438709\n",
      "Train Epoch: 1 [4040/200000 (2%)]\tLoss: 25.773960\n",
      "Train Epoch: 1 [4060/200000 (2%)]\tLoss: 30.512836\n",
      "Train Epoch: 1 [4080/200000 (2%)]\tLoss: 31.104769\n",
      "Train Epoch: 1 [4100/200000 (2%)]\tLoss: 44.692089\n",
      "Train Epoch: 1 [4120/200000 (2%)]\tLoss: 19.281452\n",
      "Train Epoch: 1 [4140/200000 (2%)]\tLoss: 38.090157\n",
      "Train Epoch: 1 [4160/200000 (2%)]\tLoss: 34.373863\n",
      "Train Epoch: 1 [4180/200000 (2%)]\tLoss: 47.138069\n",
      "Train Epoch: 1 [4200/200000 (2%)]\tLoss: 53.056480\n",
      "Train Epoch: 1 [4220/200000 (2%)]\tLoss: 30.287106\n",
      "Train Epoch: 1 [4240/200000 (2%)]\tLoss: 8.920777\n",
      "Train Epoch: 1 [4260/200000 (2%)]\tLoss: 34.021317\n",
      "Train Epoch: 1 [4280/200000 (2%)]\tLoss: 21.999096\n",
      "Train Epoch: 1 [4300/200000 (2%)]\tLoss: 36.890877\n",
      "Train Epoch: 1 [4320/200000 (2%)]\tLoss: 27.274836\n",
      "Train Epoch: 1 [4340/200000 (2%)]\tLoss: 31.811085\n",
      "Train Epoch: 1 [4360/200000 (2%)]\tLoss: 32.329868\n",
      "Train Epoch: 1 [4380/200000 (2%)]\tLoss: 14.259249\n",
      "Train Epoch: 1 [4400/200000 (2%)]\tLoss: 19.357471\n",
      "Train Epoch: 1 [4420/200000 (2%)]\tLoss: 42.510178\n",
      "Train Epoch: 1 [4440/200000 (2%)]\tLoss: 30.406521\n",
      "Train Epoch: 1 [4460/200000 (2%)]\tLoss: 31.087740\n",
      "Train Epoch: 1 [4480/200000 (2%)]\tLoss: 38.244194\n",
      "Train Epoch: 1 [4500/200000 (2%)]\tLoss: 44.924564\n",
      "Train Epoch: 1 [4520/200000 (2%)]\tLoss: 28.644938\n",
      "Train Epoch: 1 [4540/200000 (2%)]\tLoss: 36.859020\n",
      "Train Epoch: 1 [4560/200000 (2%)]\tLoss: 17.463915\n",
      "Train Epoch: 1 [4580/200000 (2%)]\tLoss: 41.500347\n",
      "Train Epoch: 1 [4600/200000 (2%)]\tLoss: 21.098782\n",
      "Train Epoch: 1 [4620/200000 (2%)]\tLoss: 46.992096\n",
      "Train Epoch: 1 [4640/200000 (2%)]\tLoss: 16.811224\n",
      "Train Epoch: 1 [4660/200000 (2%)]\tLoss: 86.853043\n",
      "Train Epoch: 1 [4680/200000 (2%)]\tLoss: 28.877777\n",
      "Train Epoch: 1 [4700/200000 (2%)]\tLoss: 75.823990\n",
      "Train Epoch: 1 [4720/200000 (2%)]\tLoss: 26.162102\n",
      "Train Epoch: 1 [4740/200000 (2%)]\tLoss: 38.079151\n",
      "Train Epoch: 1 [4760/200000 (2%)]\tLoss: 16.625050\n",
      "Train Epoch: 1 [4780/200000 (2%)]\tLoss: 43.529854\n",
      "Train Epoch: 1 [4800/200000 (2%)]\tLoss: 41.698086\n",
      "Train Epoch: 1 [4820/200000 (2%)]\tLoss: 17.459282\n",
      "Train Epoch: 1 [4840/200000 (2%)]\tLoss: 31.998924\n",
      "Train Epoch: 1 [4860/200000 (2%)]\tLoss: 36.589066\n",
      "Train Epoch: 1 [4880/200000 (2%)]\tLoss: 27.040905\n",
      "Train Epoch: 1 [4900/200000 (2%)]\tLoss: 35.345268\n",
      "Train Epoch: 1 [4920/200000 (2%)]\tLoss: 15.046732\n",
      "Train Epoch: 1 [4940/200000 (2%)]\tLoss: 50.628189\n",
      "Train Epoch: 1 [4960/200000 (2%)]\tLoss: 38.084190\n",
      "Train Epoch: 1 [4980/200000 (2%)]\tLoss: 32.461166\n",
      "Train Epoch: 1 [5000/200000 (2%)]\tLoss: 59.253872\n",
      "Train Epoch: 1 [5020/200000 (3%)]\tLoss: 13.012487\n",
      "Train Epoch: 1 [5040/200000 (3%)]\tLoss: 15.799528\n",
      "Train Epoch: 1 [5060/200000 (3%)]\tLoss: 10.843929\n",
      "Train Epoch: 1 [5080/200000 (3%)]\tLoss: 42.609791\n",
      "Train Epoch: 1 [5100/200000 (3%)]\tLoss: 30.743015\n",
      "Train Epoch: 1 [5120/200000 (3%)]\tLoss: 12.158533\n",
      "Train Epoch: 1 [5140/200000 (3%)]\tLoss: 49.453972\n",
      "Train Epoch: 1 [5160/200000 (3%)]\tLoss: 17.419373\n",
      "Train Epoch: 1 [5180/200000 (3%)]\tLoss: 50.360466\n",
      "Train Epoch: 1 [5200/200000 (3%)]\tLoss: 54.769249\n",
      "Train Epoch: 1 [5220/200000 (3%)]\tLoss: 37.184288\n",
      "Train Epoch: 1 [5240/200000 (3%)]\tLoss: 32.743763\n",
      "Train Epoch: 1 [5260/200000 (3%)]\tLoss: 24.405655\n",
      "Train Epoch: 1 [5280/200000 (3%)]\tLoss: 29.419313\n",
      "Train Epoch: 1 [5300/200000 (3%)]\tLoss: 29.058941\n",
      "Train Epoch: 1 [5320/200000 (3%)]\tLoss: 39.417435\n",
      "Train Epoch: 1 [5340/200000 (3%)]\tLoss: 28.541718\n",
      "Train Epoch: 1 [5360/200000 (3%)]\tLoss: 36.165680\n",
      "Train Epoch: 1 [5380/200000 (3%)]\tLoss: 25.939388\n",
      "Train Epoch: 1 [5400/200000 (3%)]\tLoss: 34.015842\n",
      "Train Epoch: 1 [5420/200000 (3%)]\tLoss: 25.648792\n",
      "Train Epoch: 1 [5440/200000 (3%)]\tLoss: 61.981682\n",
      "Train Epoch: 1 [5460/200000 (3%)]\tLoss: 36.859222\n",
      "Train Epoch: 1 [5480/200000 (3%)]\tLoss: 15.268148\n",
      "Train Epoch: 1 [5500/200000 (3%)]\tLoss: 27.112654\n",
      "Train Epoch: 1 [5520/200000 (3%)]\tLoss: 15.813852\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      4\u001b[39m writer = WandbWriter(cfg)\n\u001b[32m      5\u001b[39m trainer = VAETrainer(cfg, model, train_loader, val_loader, criterion, \n\u001b[32m      6\u001b[39m                      optimizer, device, dataset_train, dataset_test,\n\u001b[32m      7\u001b[39m                      earlystopping, scheduler, writer)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m df_res = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Ahmed-home/1- Projects/Research/Journal 2/Code/MAWM/MAWM/trainers/vae_trainer.py:112\u001b[39m, in \u001b[36mfit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    109\u001b[39m lst_dfs = []\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.cfg.epochs + \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     train_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m     test_loss = \u001b[38;5;28mself\u001b[39m.eval_epoch()\n\u001b[32m    114\u001b[39m     \u001b[38;5;28mself\u001b[39m.scheduler.step(test_loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Ahmed-home/1- Projects/Research/Journal 2/Code/MAWM/MAWM/trainers/vae_trainer.py:73\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(self, epoch)\u001b[39m\n\u001b[32m     71\u001b[39m recon_batch, mu, logvar = \u001b[38;5;28mself\u001b[39m.model(observation)\n\u001b[32m     72\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(recon_batch, observation, mu, logvar)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m train_loss += loss.item()\n\u001b[32m     75\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "now = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "cfg.now = now\n",
    "writer = WandbWriter(cfg)\n",
    "trainer = VAETrainer(cfg, model, train_loader, val_loader, criterion, \n",
    "                     optimizer, device, dataset_train, dataset_test,\n",
    "                     earlystopping, scheduler, writer)\n",
    "\n",
    "df_res = trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9c234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
